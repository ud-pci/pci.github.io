{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to pCI's documentation pCI is a high-precision relativistic atomic calculation package based on the configuration interaction (CI) method and methods combining CI with many-body perturbation theory and/or the all-order coupled-cluster method. The serial version of the CI+MBPT code package was modified for public use and published in Computer Physics Communications in 2015 by M. Kozlov et al.","title":"Introduction"},{"location":"#welcome-to-pcis-documentation","text":"pCI is a high-precision relativistic atomic calculation package based on the configuration interaction (CI) method and methods combining CI with many-body perturbation theory and/or the all-order coupled-cluster method. The serial version of the CI+MBPT code package was modified for public use and published in Computer Physics Communications in 2015 by M. Kozlov et al.","title":"Welcome to pCI's documentation"},{"location":"about/","text":"About Us pCI Development Team Marianna Safronova Department of Physics and Astronomy, University of Delaware Website : http://www.physics.udel.edu/~msafrono/ Email : msafrono@udel.edu Charles Cheung Department of Physics and Astronomy, University of Delaware Email : ccheung@udel.edu Sergey Porsev Department of Physics and Astronomy, University of Delaware Petersburg Nuclear Physics Institute of NRC \"Kurchatov Institute\", Russia Mikhail Kozlov Petersburg Nuclear Physics Institute of NRC \"Kurchatov Institute\", Russia St. Petersburg Electrotechnical University \u201cLETI\u201d, Russia Ilya Tupitsyn Department of Physics, St. Petersburg State University, Russia Andrey Bondarev Peter the Great St. Petersburg Polytechnic University, Russia","title":"About the Team"},{"location":"about/#about-us","text":"","title":"About Us"},{"location":"about/#pci-development-team","text":"Marianna Safronova Department of Physics and Astronomy, University of Delaware Website : http://www.physics.udel.edu/~msafrono/ Email : msafrono@udel.edu Charles Cheung Department of Physics and Astronomy, University of Delaware Email : ccheung@udel.edu Sergey Porsev Department of Physics and Astronomy, University of Delaware Petersburg Nuclear Physics Institute of NRC \"Kurchatov Institute\", Russia Mikhail Kozlov Petersburg Nuclear Physics Institute of NRC \"Kurchatov Institute\", Russia St. Petersburg Electrotechnical University \u201cLETI\u201d, Russia Ilya Tupitsyn Department of Physics, St. Petersburg State University, Russia Andrey Bondarev Peter the Great St. Petersburg Polytechnic University, Russia","title":"pCI Development Team"},{"location":"all-order/","text":"All-order/MBPT code package All-order package The all-order part of the package calculates corrections to the bare Hamiltonian due to the core shells for the CI code conf . The all-order label refers to inclusion of the large number of terms (second-, third-, fourth-order, etc.) in order-by-order many-body perturbation theory expansion using the iterative solutions until the sufficient numerical convergence is achieved. This code version implements a variant of the linearized single double coupled-cluster (CC) method. This CC version has been developed specifically for atoms fully utilizing atomic symmetries and capable of being efficiently ran with very large basis sets (over 1000 orbitals), reaching negligible numerical uncertainty associated with the choice of basis set. The all-order package consists of three codes: allcore-ci , valsd-ci , and sdvw-ci , which calculates core, core-valence and valence-valence excitations, respectively. These codes store resulting data in SGC.CON (small file) and SCRC.CON (up to a few GB file). The all-order package can be omitted if high precision is not required, leading to a method referred to as CI+MBPT. The following code executions read the files hfspl.1 and hfspl.2 and generate the files SGC.CON and SCRC.CON . It also takes in the input file inf.aov and writes the results to the respective out. files. ./allcore-rle-ci <inf.aov >out.core # computes Sigma_ma, Sigma_mnab -> writes pair.3 file ./valsd-rle-cis <inf.aov >out.val # computes Sigma_mv (and thus Sigma_1, the one-body correcton to the Hamiltonian), Sigma_mnva -> writes val2 and sigma files ./sdvw-rle-cis <inf.aov >out.sdvw # computes Sigma_mnvw (Sigma_2, the two-body correction to the Hamiltonian) -> writes pair.vw and sigma1 files Click here to see a description of inf.aov 12 # ncore - number of core shells 1 -1 # n and kappa of the core shells 2 -1 2 1 2 -2 3 -1 3 1 3 -2 3 2 3 -3 4 -1 4 1 4 -2 35 5 # nmax and lmax in correlation diagram summations 0 0 # internal parameters, do not change 30 # max number of iterations for core 2 7 1 # Stabilizer code parameters, see below 0.d0 # damping factor, not active not zero 1 # kval (key_en) - key for energies, see explanation below 24 # nval - number of valence orbitals in list to follow 5 -1 30 # n and kappa of the valence orbitals, max number of iteration 6 -1 30 7 -1 30 8 -1 30 5 1 30 6 1 30 7 1 30 8 1 30 5 -2 30 6 -2 30 7 -2 30 8 -2 30 4 2 30 5 2 30 6 2 30 7 2 30 4 -3 30 5 -3 30 6 -3 30 7 -3 30 4 3 30 5 3 30 4 -4 30 5 -4 30 ============================================================== Additional explanations: 2 7 1 # Stabilizer code parameters 2 - DIIS method (change to 1 for RLE method) 7 \u2013 number in iterations before stabilizer code runs 1 - do not change (controls the type of linear algebra code) Note It is important to always check all output files. In out.core and out.val , check that the core and valence orbitals have converged, respectively. If any cases completed 30 iterations (max) with values slowly drifting up, you can reduce the number of interactions to 3 or 5, and rerun the respective codes. Here, it's important to look at how much energies fluctuated during the first few iterations. In cases of severe divergence, check all inputs for errors. If nothing is found, and the atom/ion with closed d shell, but not p shell, set kval=2 . Note kval controls the values of \\tilde{\\epsilon}_v in denominators (see Phys. Rev. A 80, 012516 (2009) ) for formulas). kval=1 is the default choice, where \\tilde{\\epsilon}_v for all nl_j is set to the DHF energy of the lowest valence n for the particular partial wave. For example for Sr, \\tilde{\\epsilon}_v for v=ns is set with the DHF energy of the 5s state, v=np_{1/2} is set with the DHF energy of the 5p_{1/2} state, v=nd_{3/2} is set with the DHF energy of the 5d_{3/2} state, and so on. kval=2 is only used when the all-order valence energies are severely divergent. So far, this was observed with highly-charged ions with filling p -shell (e.g. Sn-like). By \"severe\", we mean that the energies begin to diverge after the first or second iteration, immediately driving the correlation energy to be very large. Such a divergence cannot be fixed by a stabilizer. In this case, we have to identify which partial waves diverge and manually change the energies for these orbitals - we set them to the lowest DHF energy of the partial wave for which the all-order converged. For instance, if s diverges, but p does not, then set the ns energies to the lowest np_{1/2} DHF energies. The rest are left as DHF as in kval=1 . The format would be as follows: 2 # kval 3 # lmax for the input to follow 0 -0.28000 # l=0 energies 1 -0.22000 -0.22000 # l=1 energies p1/2 p3/2 2 -0.31000 -0.31000 # l=2 energies d3/2 d5/2 3 -0.13000 -0.13000 # l=3 energies f5/2 f7/2 MBPT package The MBPT part of the package calculates corrections to the bare Hamiltonian due to the core shells using second order MBPT for the CI code conf , but for a much larger part of the Hamiltonian than the all-order code since high accuracy is not required for corrections associated with higher orbitals. The MBPT package consists of a single code second-cis , which can be omitted, but then conf will not include electronic correlations associated with any of the core shells. If the all-order calculation was carried out, it will overwrite the second-order results with the all-order results where available. Such overlay of the MBPT and the all-order parts drastically improves the efficiency of the method. The following code execution reads the files hfspl.1 , hfspl.2 and HFD.DAT (to read the list of orbitals), and writes the files SGC.CON and SCRC.CON used by conf . It also takes in the input file inf.vw and writes the results to the respective out.sdvw files. ./second-cis <inf.vw >out.second.vw # writes SGC.CON and SCRC.CON files Click here to see a description of inf.vw 12 # ncore - number of core shells 1 -1 # n and kappa of the core shells 2 -1 2 1 2 -2 3 -1 3 1 3 -2 3 2 3 -3 4 -1 4 1 4 -2 35 5 # nmax and lmax in correlation diagram summations 195 # Nmax (max_orb) # of orbitals (from BASS.INPT) for how many Sigmas to calculate 3 # Lmax (lvmax) for how many Sigmas to calculate, supersedes Nmax. Maybe best to set to 4 when 4f is important 9 # Kmax (kvmax) for higest partial wave for Sigma2 S_Kmax(ijkl). May increase for f shell cases 195 100 # nav1 nav - maximum average of orbitals to calculate Sigma for nav1=(i+j)/2 for Sigma(ij) and nav=(i+j+k+l)/4 for Sigma_K(ijkl) 0 # if just second order, 1 to read all-order input from PAIR.VW and sigma1 1 # kval, set the same as in the all-order, see explanations of inf.aov ============================================================== Additional explanations about Sigma_1 and Sigma_2 restrictions: (1) Put the first number (Sigma 1) to be the last orbital you plan to list in ADD.INP. Look up the number in BASS.INP and add the number of core shells. Here, the last orbital to be included in conf is 21d5/2, so the number is 195. Example: Last orbital in BASS.INP: 183 2.1201 3 2.1201 # Here, the last orbital to be included in conf is 21d5/2, so the number is 183+12 = 195 Note: this is inputted twice, keep it the same. (2) Set 100 as the second value. This was tested a while ago, can increase.","title":"All-order/MBPT code package"},{"location":"all-order/#all-ordermbpt-code-package","text":"","title":"All-order/MBPT code package"},{"location":"all-order/#all-order-package","text":"The all-order part of the package calculates corrections to the bare Hamiltonian due to the core shells for the CI code conf . The all-order label refers to inclusion of the large number of terms (second-, third-, fourth-order, etc.) in order-by-order many-body perturbation theory expansion using the iterative solutions until the sufficient numerical convergence is achieved. This code version implements a variant of the linearized single double coupled-cluster (CC) method. This CC version has been developed specifically for atoms fully utilizing atomic symmetries and capable of being efficiently ran with very large basis sets (over 1000 orbitals), reaching negligible numerical uncertainty associated with the choice of basis set. The all-order package consists of three codes: allcore-ci , valsd-ci , and sdvw-ci , which calculates core, core-valence and valence-valence excitations, respectively. These codes store resulting data in SGC.CON (small file) and SCRC.CON (up to a few GB file). The all-order package can be omitted if high precision is not required, leading to a method referred to as CI+MBPT. The following code executions read the files hfspl.1 and hfspl.2 and generate the files SGC.CON and SCRC.CON . It also takes in the input file inf.aov and writes the results to the respective out. files. ./allcore-rle-ci <inf.aov >out.core # computes Sigma_ma, Sigma_mnab -> writes pair.3 file ./valsd-rle-cis <inf.aov >out.val # computes Sigma_mv (and thus Sigma_1, the one-body correcton to the Hamiltonian), Sigma_mnva -> writes val2 and sigma files ./sdvw-rle-cis <inf.aov >out.sdvw # computes Sigma_mnvw (Sigma_2, the two-body correction to the Hamiltonian) -> writes pair.vw and sigma1 files Click here to see a description of inf.aov 12 # ncore - number of core shells 1 -1 # n and kappa of the core shells 2 -1 2 1 2 -2 3 -1 3 1 3 -2 3 2 3 -3 4 -1 4 1 4 -2 35 5 # nmax and lmax in correlation diagram summations 0 0 # internal parameters, do not change 30 # max number of iterations for core 2 7 1 # Stabilizer code parameters, see below 0.d0 # damping factor, not active not zero 1 # kval (key_en) - key for energies, see explanation below 24 # nval - number of valence orbitals in list to follow 5 -1 30 # n and kappa of the valence orbitals, max number of iteration 6 -1 30 7 -1 30 8 -1 30 5 1 30 6 1 30 7 1 30 8 1 30 5 -2 30 6 -2 30 7 -2 30 8 -2 30 4 2 30 5 2 30 6 2 30 7 2 30 4 -3 30 5 -3 30 6 -3 30 7 -3 30 4 3 30 5 3 30 4 -4 30 5 -4 30 ============================================================== Additional explanations: 2 7 1 # Stabilizer code parameters 2 - DIIS method (change to 1 for RLE method) 7 \u2013 number in iterations before stabilizer code runs 1 - do not change (controls the type of linear algebra code) Note It is important to always check all output files. In out.core and out.val , check that the core and valence orbitals have converged, respectively. If any cases completed 30 iterations (max) with values slowly drifting up, you can reduce the number of interactions to 3 or 5, and rerun the respective codes. Here, it's important to look at how much energies fluctuated during the first few iterations. In cases of severe divergence, check all inputs for errors. If nothing is found, and the atom/ion with closed d shell, but not p shell, set kval=2 . Note kval controls the values of \\tilde{\\epsilon}_v in denominators (see Phys. Rev. A 80, 012516 (2009) ) for formulas). kval=1 is the default choice, where \\tilde{\\epsilon}_v for all nl_j is set to the DHF energy of the lowest valence n for the particular partial wave. For example for Sr, \\tilde{\\epsilon}_v for v=ns is set with the DHF energy of the 5s state, v=np_{1/2} is set with the DHF energy of the 5p_{1/2} state, v=nd_{3/2} is set with the DHF energy of the 5d_{3/2} state, and so on. kval=2 is only used when the all-order valence energies are severely divergent. So far, this was observed with highly-charged ions with filling p -shell (e.g. Sn-like). By \"severe\", we mean that the energies begin to diverge after the first or second iteration, immediately driving the correlation energy to be very large. Such a divergence cannot be fixed by a stabilizer. In this case, we have to identify which partial waves diverge and manually change the energies for these orbitals - we set them to the lowest DHF energy of the partial wave for which the all-order converged. For instance, if s diverges, but p does not, then set the ns energies to the lowest np_{1/2} DHF energies. The rest are left as DHF as in kval=1 . The format would be as follows: 2 # kval 3 # lmax for the input to follow 0 -0.28000 # l=0 energies 1 -0.22000 -0.22000 # l=1 energies p1/2 p3/2 2 -0.31000 -0.31000 # l=2 energies d3/2 d5/2 3 -0.13000 -0.13000 # l=3 energies f5/2 f7/2","title":"All-order package"},{"location":"all-order/#mbpt-package","text":"The MBPT part of the package calculates corrections to the bare Hamiltonian due to the core shells using second order MBPT for the CI code conf , but for a much larger part of the Hamiltonian than the all-order code since high accuracy is not required for corrections associated with higher orbitals. The MBPT package consists of a single code second-cis , which can be omitted, but then conf will not include electronic correlations associated with any of the core shells. If the all-order calculation was carried out, it will overwrite the second-order results with the all-order results where available. Such overlay of the MBPT and the all-order parts drastically improves the efficiency of the method. The following code execution reads the files hfspl.1 , hfspl.2 and HFD.DAT (to read the list of orbitals), and writes the files SGC.CON and SCRC.CON used by conf . It also takes in the input file inf.vw and writes the results to the respective out.sdvw files. ./second-cis <inf.vw >out.second.vw # writes SGC.CON and SCRC.CON files Click here to see a description of inf.vw 12 # ncore - number of core shells 1 -1 # n and kappa of the core shells 2 -1 2 1 2 -2 3 -1 3 1 3 -2 3 2 3 -3 4 -1 4 1 4 -2 35 5 # nmax and lmax in correlation diagram summations 195 # Nmax (max_orb) # of orbitals (from BASS.INPT) for how many Sigmas to calculate 3 # Lmax (lvmax) for how many Sigmas to calculate, supersedes Nmax. Maybe best to set to 4 when 4f is important 9 # Kmax (kvmax) for higest partial wave for Sigma2 S_Kmax(ijkl). May increase for f shell cases 195 100 # nav1 nav - maximum average of orbitals to calculate Sigma for nav1=(i+j)/2 for Sigma(ij) and nav=(i+j+k+l)/4 for Sigma_K(ijkl) 0 # if just second order, 1 to read all-order input from PAIR.VW and sigma1 1 # kval, set the same as in the all-order, see explanations of inf.aov ============================================================== Additional explanations about Sigma_1 and Sigma_2 restrictions: (1) Put the first number (Sigma 1) to be the last orbital you plan to list in ADD.INP. Look up the number in BASS.INP and add the number of core shells. Here, the last orbital to be included in conf is 21d5/2, so the number is 195. Example: Last orbital in BASS.INP: 183 2.1201 3 2.1201 # Here, the last orbital to be included in conf is 21d5/2, so the number is 183+12 = 195 Note: this is inputted twice, keep it the same. (2) Set 100 as the second value. This was tested a while ago, can increase.","title":"MBPT package"},{"location":"auxiliary/","text":"Auxiliary programs It is sometimes useful to combine two configuration lists. Here, we describe an auxiliary codes con_cut which truncates a configuration list, and merge_ci which merges two configuration lists. bdhfA - Dirac-Hartree-Fock code bdhfA can be used in place of bdhf to make a B-spline basis set. This program reads in an input file xx.dat with a much simpler input than for bdhf , and will write bdhf.in , which can be renamed to bas_wj.in for subsequent use in all-order codes. Click here to see a description of a minimum input file for Cs . 1 1 Cs 1s 2s 2p 3s 3p 3d 4s 4p 4d 5s 5p ========== 7 70 9 220.0 Click here to see a description of a complete input file for Cs . 1 1 Cs 137 0.0001 1s 2s 2p 3s 3p 3d 4s 4p 4d 5s 5p 6 9 s1/2 6 9 p1/2 5d3/2 5d5/2 ========== 7 70 9 220.0 Click here to see a description of a complete input file for Yb . 1 1 Yb 176 0.0001 1s 2s 2p 3s 3p 3d 4s 4p 4d 5s 5p 4f 6s1/2 5d3/2 5d5/2 ========== 7 50 9 60.0 The input files have the following format: 1 # 0 for old input, 1 for new input 1 # convergence parameter: 1 - most cases, 5 - in case of convergence problems Cs # name of element - e.g. Cs, Yb, etc. nuclear charge Z is set using this name # Isotope can also be entered. If no isotope is entered, most abundant or # most stable isotope will be used. # First grid point can be entered if finer grid is used inside the nuclei. # The default is set to 0.0005, which will produce a few points inside the nucleus. # Use 0.0001, for example, for more points (~25). # Examples (either will work): # Cs # Cs 137 # Cs 0.0001 1s 2s ... 5p # List of filled core shells. This code has to use fully filled subshells for core potential. # Valence electron lines can be omitted if the code is used for bspl. # To include valence electrons, input nmin nmax l j, or nlj: # 6 8 s1/2 # for 6, 7, 8 s1/2 # 6 s1/2 # for just 6s1/2 ========== # required line for formatting 7 70 9 220.0 # highest partial wave, number of splines, order of splines, and cavity radius in a.u. thorA - dynamic polarizability code thorA is used to make a batch script along with all input files for a range of wavelengths to aid in the submission of dynamic polarizability calculations. Click here to see a description of the input file . C # C for Caviness, D for Darwin 2 # inout on the second line for ine_dyn_E28 input 3P2. # state names for file names 800 980 2 # initial wavelength, final wavelength, and step in nm con_cut - truncating configuration lists con_cut is used to truncate a configuration list to only hold all configurations with weights above a user-specified cutoff. This program outputs the file CONF_CUT.RES , which can be used as a new CONF.INP file. The configurations in CONF_CUT.RES are also listed in order of descending weights, so the most important configurations are at the top of the list. You can copy the top configurations to your ADD.INP file and reconstruct a new configuration list with the top configurations as basic configurations, or use merge_ci to put the top configurations in another CONF.INP file. merge_ci - merging configuration lists merge_ci takes in two CONF.INP files named C_A.INP and CONF.INP and outputs the file C_M.INP . This output file can be renamed CONF.INP again to be used in conf . In summary: Run con_cut , inputting a cutoff threshold for the weight, to obtain CON_CUT.RES . Rename CON_CUT.RES to C_A.INP . Replace the basic configurations in ADD.INP with the top configurations from C_A.INP and run add . Run merge_ci , combining the configurations in C_A.INP and CONF.INP to obtain C_M.INP . Rename CONF.INP to C_B.INP , and C_M.INP to CONF.INP . Run conf . conf_pt - valence perturbation theory When running CI calculations with a large number of configurations relative to the number of work resources, it is often times necessary to determine the most important configurations in the CI space, and truncate the list of configurations to make successive calculations feasible. The conf_pt program begins the same way conf begins, reading in several input parameters and the list of configurations from the file CONF.INP , the basis set from the files HFD.DAT and CONF.DAT , and the radial integrals from CONF.INT and CONF.GNT . In addition, it also reads the CI eigenvectors from the file CONF.XIJ . qed_pot_conf - The pCI package includes 5 variants ( kvar ) of QED potentials: QEDMOD Flambaum local potential + QEDMOD non-local correction Flambaum local potential QEDPOT Semi-empirical approach sort - converts parallel matrix element files to serial format sort.py is a python program that sorts the matrix elements of the operator J^2 in order of ascending index (as done in the serial version of the conf program). This program takes in the parallel file CONF.JJJ or CONF.HIJ and returns a serial version of the inputted file. However, there is one change made to the file. There is an additional integer at the start of the file that stores the total number of matrix elements.","title":"Auxiliary programs"},{"location":"auxiliary/#auxiliary-programs","text":"It is sometimes useful to combine two configuration lists. Here, we describe an auxiliary codes con_cut which truncates a configuration list, and merge_ci which merges two configuration lists.","title":"Auxiliary programs"},{"location":"auxiliary/#bdhfa-dirac-hartree-fock-code","text":"bdhfA can be used in place of bdhf to make a B-spline basis set. This program reads in an input file xx.dat with a much simpler input than for bdhf , and will write bdhf.in , which can be renamed to bas_wj.in for subsequent use in all-order codes. Click here to see a description of a minimum input file for Cs . 1 1 Cs 1s 2s 2p 3s 3p 3d 4s 4p 4d 5s 5p ========== 7 70 9 220.0 Click here to see a description of a complete input file for Cs . 1 1 Cs 137 0.0001 1s 2s 2p 3s 3p 3d 4s 4p 4d 5s 5p 6 9 s1/2 6 9 p1/2 5d3/2 5d5/2 ========== 7 70 9 220.0 Click here to see a description of a complete input file for Yb . 1 1 Yb 176 0.0001 1s 2s 2p 3s 3p 3d 4s 4p 4d 5s 5p 4f 6s1/2 5d3/2 5d5/2 ========== 7 50 9 60.0 The input files have the following format: 1 # 0 for old input, 1 for new input 1 # convergence parameter: 1 - most cases, 5 - in case of convergence problems Cs # name of element - e.g. Cs, Yb, etc. nuclear charge Z is set using this name # Isotope can also be entered. If no isotope is entered, most abundant or # most stable isotope will be used. # First grid point can be entered if finer grid is used inside the nuclei. # The default is set to 0.0005, which will produce a few points inside the nucleus. # Use 0.0001, for example, for more points (~25). # Examples (either will work): # Cs # Cs 137 # Cs 0.0001 1s 2s ... 5p # List of filled core shells. This code has to use fully filled subshells for core potential. # Valence electron lines can be omitted if the code is used for bspl. # To include valence electrons, input nmin nmax l j, or nlj: # 6 8 s1/2 # for 6, 7, 8 s1/2 # 6 s1/2 # for just 6s1/2 ========== # required line for formatting 7 70 9 220.0 # highest partial wave, number of splines, order of splines, and cavity radius in a.u.","title":"bdhfA - Dirac-Hartree-Fock code"},{"location":"auxiliary/#thora-dynamic-polarizability-code","text":"thorA is used to make a batch script along with all input files for a range of wavelengths to aid in the submission of dynamic polarizability calculations. Click here to see a description of the input file . C # C for Caviness, D for Darwin 2 # inout on the second line for ine_dyn_E28 input 3P2. # state names for file names 800 980 2 # initial wavelength, final wavelength, and step in nm","title":"thorA - dynamic polarizability code"},{"location":"auxiliary/#con_cut-truncating-configuration-lists","text":"con_cut is used to truncate a configuration list to only hold all configurations with weights above a user-specified cutoff. This program outputs the file CONF_CUT.RES , which can be used as a new CONF.INP file. The configurations in CONF_CUT.RES are also listed in order of descending weights, so the most important configurations are at the top of the list. You can copy the top configurations to your ADD.INP file and reconstruct a new configuration list with the top configurations as basic configurations, or use merge_ci to put the top configurations in another CONF.INP file.","title":"con_cut - truncating configuration lists"},{"location":"auxiliary/#merge_ci-merging-configuration-lists","text":"merge_ci takes in two CONF.INP files named C_A.INP and CONF.INP and outputs the file C_M.INP . This output file can be renamed CONF.INP again to be used in conf . In summary: Run con_cut , inputting a cutoff threshold for the weight, to obtain CON_CUT.RES . Rename CON_CUT.RES to C_A.INP . Replace the basic configurations in ADD.INP with the top configurations from C_A.INP and run add . Run merge_ci , combining the configurations in C_A.INP and CONF.INP to obtain C_M.INP . Rename CONF.INP to C_B.INP , and C_M.INP to CONF.INP . Run conf .","title":"merge_ci - merging configuration lists"},{"location":"auxiliary/#conf_pt-valence-perturbation-theory","text":"When running CI calculations with a large number of configurations relative to the number of work resources, it is often times necessary to determine the most important configurations in the CI space, and truncate the list of configurations to make successive calculations feasible. The conf_pt program begins the same way conf begins, reading in several input parameters and the list of configurations from the file CONF.INP , the basis set from the files HFD.DAT and CONF.DAT , and the radial integrals from CONF.INT and CONF.GNT . In addition, it also reads the CI eigenvectors from the file CONF.XIJ .","title":"conf_pt - valence perturbation theory"},{"location":"auxiliary/#qed_pot_conf-","text":"The pCI package includes 5 variants ( kvar ) of QED potentials: QEDMOD Flambaum local potential + QEDMOD non-local correction Flambaum local potential QEDPOT Semi-empirical approach","title":"qed_pot_conf -"},{"location":"auxiliary/#sort-converts-parallel-matrix-element-files-to-serial-format","text":"sort.py is a python program that sorts the matrix elements of the operator J^2 in order of ascending index (as done in the serial version of the conf program). This program takes in the parallel file CONF.JJJ or CONF.HIJ and returns a serial version of the inputted file. However, there is one change made to the file. There is an additional integer at the start of the file that stores the total number of matrix elements.","title":"sort - converts parallel matrix element files to serial format"},{"location":"basis_complex/","text":"Example: Fe XVII and Ni XIX The following instructions assume familiarity with the main programs of the pCI package . In this section, we describe a method used to construct basis sets for the cases of Fe XVII and Ni XIX. In this example, we utilize multiple HFD.INP to construct the orbitals. The following is a list of the input files used in this example. h_m_1.inp - Here we construct the 1s, 2s, 2p, 3s, 3p, 3d orbitals with the 2s^2 2p^5 3d configuration. QQ is 0 for 3s and 3p, but the orbital is still formed. DF is solved with 0 occ. num., but they won't be very good orbitals. You can think of 3s0 and 3p0 as placeholders to keep the order of orbitals, and add them in the next step. h_m_2.inp - Here we freeze the 1s, 2s, 2p, 3d orbitals and re-construct the 3s, 3p orbitals from the 2s^2 2p^5 3s and 2s^2 2p^5 3p configurations. The following code block shows h_m_1.inp on the left and h_m_2.inp on the right of the partition. The head of both input files are identical. Fe XVII & Ni XIX KL = 0 # (0 - new calculation, 1 - continue) NS = 9 # number of orbitals NSO= 2 # number of closed orbitals (in this case only 1s2, 2s2) Z = 26.0 # atomic number AM = 56.000 # atomic mass JM = -2.0 # R2 = 20.0 # radius of cavity kbr= 2 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) NL J QQ KP NC | NL J QQ KP NC | 1 1S (1/2) 2.0000 0 0 | 1 1S (1/2) 2.0000 1 0 2 2S (1/2) 2.0000 0 0 | 2 2S (1/2) 2.0000 1 0 3 2P (1/2) 2.0000 0 0 | 3 2P (1/2) 2.0000 1 0 4 2P (3/2) 3.0000 0 0 | 4 2P (3/2) 3.0000 1 0 5 3S (1/2) 0.0000 0 0 | 5 3S (1/2) 1.0000 0 1 6 3P (1/2) 0.0000 0 0 | 6 3P (1/2) 1.0000 0 2 7 3P (3/2) 0.0000 0 0 | 7 3P (3/2) 0.0000 0 2 8 3D (3/2) 1.0000 0 0 | 8 3D (3/2) 0.0000 1 3 9 3D (5/2) 0.0000 0 0 | 9 3D (5/2) 0.0000 1 3 h_m_3.inp - Here we freeze the 1s, 2s, 2p, 3s, 3p, 3d orbitals then construct the 4s, 4p, 4d, 4f, 5g orbitals from the 2s^2 2p^5 4s, 2s^2 2p^5 4p, \\dots, 2s^2 2p^5 5g configurations. Note that the number of orbitals Ns has changed from 9 to 13. Fe XVII & Ni XIX KL = 0 # NS = 13 # number of orbitals NSO= 2 # number of closed orbitals (in this case only 1s2, 2s2) Z = 26.0 # atomic number AM = 56.000 # atomic mass JM = -2.0 # R2 = 20.0 # radius of cavity kbr= 2 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) NL J QQ KP NC 1 1S (1/2) 2.0000 1 0 2 2S (1/2) 2.0000 1 0 3 2P (1/2) 2.0000 1 0 4 2P (3/2) 3.0000 1 0 5 3S (1/2) 0.0000 1 0 6 3P (1/2) 0.0000 1 0 7 3P (3/2) 0.0000 1 0 8 3D (3/2) 0.0000 1 0 9 3D (5/2) 0.0000 1 0 10 4F (5/2) 1.0000 0 1 11 4F (7/2) 0.0000 0 1 12 5G (7/2) 1.0000 0 2 13 5G (9/2) 0.0000 0 2 b_m_2.inp - Here we update HFD.DAT by including virtual orbitals to account for correlations. Fe XVII & Ni XIX Z = 26.0 Am = 52.0 Nso= 4 # number of core orbitals (defines DF operator) Nv = 84 # number of valence & virtual orbitals Ksg= 1 # defines Hamiltonian: 1-DF, 3-DF+Breit Kdg= 0 # diagonalization of Hamiltonian (0=no,1,2=yes) orb= 4s 1 # first orbital for diagonalization Kkin 1 # kinetic balance (0,1,or 2) orb= 5s 1 # first orbital to apply kin.bal. orb= 2p 3 # last frozen orbital orb= 0p 3 # last orbital in basis set kout= 0 # detail rate in the output kbrt= 2 # 0,1,2 - Coulomb, Gaunt, Breit ---------------------------------------------------------- 0.1002 0.2002 -0.2102 0.2104 1 0.3001 # 2 -0.3101 # These orbitals are in HFD.DAT already run by hfd 3 0.3101 # 4 -0.3201 # 5 0.3201 # 6 0.4001 3 0.4001 # reading 4s from 4s from HFD.DAT 7 -0.4101 3 -0.4101 # key '3' means 'read in from HFD.DAT' 8 0.4101 3 0.4101 # HFD.DAT is h_m_3.inp in this case 9 -0.4201 3 -0.4201 10 0.4201 3 0.4201 11 -0.4301 3 -0.4301 12 0.4301 3 0.4301 13 0.5001 # key '0' or ' ' means 'build nl from (n-1)l' 14 -0.5101 # e.g. 5s is built from 4s, 5p from 4p 15 0.5101 # 5d from 4d, ... 16 -0.5201 17 0.5201 18 -0.5301 19 0.5301 20 -0.5401 3 -0.5401 # since key '3' is present, 5f is read in from HFD.DAT 21 0.5401 3 0.5401 22 -0.6401 23 0.6401 : : : 84 1.2401 The following bash script utilizes the above input files and forms the basis set for Fe XVII and Ni XIX. #! /bin/bash ##################################################################### # script to form basis set for Fe 16+ and Ni 18+ cp h_m_1.inp HFD.INP ./hfd cp h_m_2.inp HFD.INP ./hfd cp HFD.DAT h0.dat cp h_m_3.inp HFD.INP ./hfd mv HFD.DAT h_m.dat mv h0.dat HFD.DAT cp b_m_2.inp BASS.INP ./bass <b.in ./bass echo \" End of script\"","title":"Example 2 - another method of creating basis"},{"location":"basis_complex/#example-fe-xvii-and-ni-xix","text":"The following instructions assume familiarity with the main programs of the pCI package . In this section, we describe a method used to construct basis sets for the cases of Fe XVII and Ni XIX. In this example, we utilize multiple HFD.INP to construct the orbitals. The following is a list of the input files used in this example. h_m_1.inp - Here we construct the 1s, 2s, 2p, 3s, 3p, 3d orbitals with the 2s^2 2p^5 3d configuration. QQ is 0 for 3s and 3p, but the orbital is still formed. DF is solved with 0 occ. num., but they won't be very good orbitals. You can think of 3s0 and 3p0 as placeholders to keep the order of orbitals, and add them in the next step. h_m_2.inp - Here we freeze the 1s, 2s, 2p, 3d orbitals and re-construct the 3s, 3p orbitals from the 2s^2 2p^5 3s and 2s^2 2p^5 3p configurations. The following code block shows h_m_1.inp on the left and h_m_2.inp on the right of the partition. The head of both input files are identical. Fe XVII & Ni XIX KL = 0 # (0 - new calculation, 1 - continue) NS = 9 # number of orbitals NSO= 2 # number of closed orbitals (in this case only 1s2, 2s2) Z = 26.0 # atomic number AM = 56.000 # atomic mass JM = -2.0 # R2 = 20.0 # radius of cavity kbr= 2 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) NL J QQ KP NC | NL J QQ KP NC | 1 1S (1/2) 2.0000 0 0 | 1 1S (1/2) 2.0000 1 0 2 2S (1/2) 2.0000 0 0 | 2 2S (1/2) 2.0000 1 0 3 2P (1/2) 2.0000 0 0 | 3 2P (1/2) 2.0000 1 0 4 2P (3/2) 3.0000 0 0 | 4 2P (3/2) 3.0000 1 0 5 3S (1/2) 0.0000 0 0 | 5 3S (1/2) 1.0000 0 1 6 3P (1/2) 0.0000 0 0 | 6 3P (1/2) 1.0000 0 2 7 3P (3/2) 0.0000 0 0 | 7 3P (3/2) 0.0000 0 2 8 3D (3/2) 1.0000 0 0 | 8 3D (3/2) 0.0000 1 3 9 3D (5/2) 0.0000 0 0 | 9 3D (5/2) 0.0000 1 3 h_m_3.inp - Here we freeze the 1s, 2s, 2p, 3s, 3p, 3d orbitals then construct the 4s, 4p, 4d, 4f, 5g orbitals from the 2s^2 2p^5 4s, 2s^2 2p^5 4p, \\dots, 2s^2 2p^5 5g configurations. Note that the number of orbitals Ns has changed from 9 to 13. Fe XVII & Ni XIX KL = 0 # NS = 13 # number of orbitals NSO= 2 # number of closed orbitals (in this case only 1s2, 2s2) Z = 26.0 # atomic number AM = 56.000 # atomic mass JM = -2.0 # R2 = 20.0 # radius of cavity kbr= 2 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) NL J QQ KP NC 1 1S (1/2) 2.0000 1 0 2 2S (1/2) 2.0000 1 0 3 2P (1/2) 2.0000 1 0 4 2P (3/2) 3.0000 1 0 5 3S (1/2) 0.0000 1 0 6 3P (1/2) 0.0000 1 0 7 3P (3/2) 0.0000 1 0 8 3D (3/2) 0.0000 1 0 9 3D (5/2) 0.0000 1 0 10 4F (5/2) 1.0000 0 1 11 4F (7/2) 0.0000 0 1 12 5G (7/2) 1.0000 0 2 13 5G (9/2) 0.0000 0 2 b_m_2.inp - Here we update HFD.DAT by including virtual orbitals to account for correlations. Fe XVII & Ni XIX Z = 26.0 Am = 52.0 Nso= 4 # number of core orbitals (defines DF operator) Nv = 84 # number of valence & virtual orbitals Ksg= 1 # defines Hamiltonian: 1-DF, 3-DF+Breit Kdg= 0 # diagonalization of Hamiltonian (0=no,1,2=yes) orb= 4s 1 # first orbital for diagonalization Kkin 1 # kinetic balance (0,1,or 2) orb= 5s 1 # first orbital to apply kin.bal. orb= 2p 3 # last frozen orbital orb= 0p 3 # last orbital in basis set kout= 0 # detail rate in the output kbrt= 2 # 0,1,2 - Coulomb, Gaunt, Breit ---------------------------------------------------------- 0.1002 0.2002 -0.2102 0.2104 1 0.3001 # 2 -0.3101 # These orbitals are in HFD.DAT already run by hfd 3 0.3101 # 4 -0.3201 # 5 0.3201 # 6 0.4001 3 0.4001 # reading 4s from 4s from HFD.DAT 7 -0.4101 3 -0.4101 # key '3' means 'read in from HFD.DAT' 8 0.4101 3 0.4101 # HFD.DAT is h_m_3.inp in this case 9 -0.4201 3 -0.4201 10 0.4201 3 0.4201 11 -0.4301 3 -0.4301 12 0.4301 3 0.4301 13 0.5001 # key '0' or ' ' means 'build nl from (n-1)l' 14 -0.5101 # e.g. 5s is built from 4s, 5p from 4p 15 0.5101 # 5d from 4d, ... 16 -0.5201 17 0.5201 18 -0.5301 19 0.5301 20 -0.5401 3 -0.5401 # since key '3' is present, 5f is read in from HFD.DAT 21 0.5401 3 0.5401 22 -0.6401 23 0.6401 : : : 84 1.2401 The following bash script utilizes the above input files and forms the basis set for Fe XVII and Ni XIX. #! /bin/bash ##################################################################### # script to form basis set for Fe 16+ and Ni 18+ cp h_m_1.inp HFD.INP ./hfd cp h_m_2.inp HFD.INP ./hfd cp HFD.DAT h0.dat cp h_m_3.inp HFD.INP ./hfd mv HFD.DAT h_m.dat mv h0.dat HFD.DAT cp b_m_2.inp BASS.INP ./bass <b.in ./bass echo \" End of script\"","title":"Example: Fe XVII and Ni XIX"},{"location":"basis_neutral/","text":"Examples: Neutral atoms The following instructions assume familiarity with the main programs of the pCI package . In this section, we describe a method used to construct basis sets for the case of neutral atoms. In these example, we utilize a single HFD.INP to construct the orbitals. Ac Ac KL = 0 # (0 - new calculation, 1 - continue) NS = 36 # number of orbitals NSO= 24 # number of closed orbitals (in this case only 1s2, 2s2) Z = 89.0 # atomic number AM = 227.00 # atomic mass JM = -2.0 # (-2 - average for non-relativistic configuration) R2 = 60.0 # radius of cavity kbr= 2 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) rnuc= 5.7350 # (optional) rms nuclear radius (https://www-nds.iaea.org/radii/) NL J QQ KP NC 1 1S (1/2) 2.0000 0 0 2 2S (1/2) 2.0000 0 0 3 2P (1/2) 2.0000 0 0 4 2P (3/2) 4.0000 0 0 5 3S (1/2) 2.0000 0 0 6 3P (1/2) 2.0000 0 0 7 3P (3/2) 4.0000 0 0 8 3D (3/2) 4.0000 0 0 9 3D (5/2) 6.0000 0 0 10 4S (1/2) 2.0000 0 0 11 4P (1/2) 2.0000 0 0 12 4P (3/2) 4.0000 0 0 13 4D (3/2) 4.0000 0 0 14 4D (5/2) 6.0000 0 0 15 4F (5/2) 6.0000 0 0 16 4F (7/2) 8.0000 0 0 17 5S (1/2) 2.0000 0 0 18 5P (1/2) 2.0000 0 0 19 5P (3/2) 4.0000 0 0 20 5D (3/2) 4.0000 0 0 21 5D (5/2) 6.0000 0 0 22 6S (1/2) 2.0000 0 0 23 6P (1/2) 2.0000 0 0 24 6P (3/2) 4.0000 0 0 25 7S (1/2) 1.0000 0 1 26 6D (3/2) 1.0000 0 2 27 6D (5/2) 0.0000 0 2 28 7P (1/2) 1.0000 0 3 29 7P (3/2) 0.0000 0 3 30 5F (5/2) 1.0000 0 4 31 5F (7/2) 0.0000 0 4 32 8S (1/2) 1.0000 0 5 33 7D (3/2) 1.0000 0 6 34 7D (5/2) 0.0000 0 6 35 8P (1/2) 1.0000 0 7 36 8P (3/2) 0.0000 0 7 Sr Sr III KL = 0 # (0 - new calculation, 1 - continue) NS = 20 # number of orbitals NSO= 12 # number of closed orbitals (in this case only 1s2, 2s2) Z = 38.0 # atomic number AM = 90.000 # atomic mass JM = -2.0 # (-2 - average for non-relativistic configuration) R2 = 60.0 # radius of cavity kbr= 0 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) NL J QQ KP NC 1 1S (1/2) 2.0000 0 0 2 2S (1/2) 2.0000 0 0 3 2P (1/2) 2.0000 0 0 4 2P (3/2) 4.0000 0 0 5 3S (1/2) 2.0000 0 0 6 3P (1/2) 2.0000 0 0 7 3P (3/2) 4.0000 0 0 8 3D (3/2) 4.0000 0 0 9 3D (5/2) 6.0000 0 0 10 4S (1/2) 2.0000 0 0 11 4P (1/2) 2.0000 0 0 12 4P (3/2) 4.0000 0 0 13 5S (1/2) 1.0000 0 1 14 5P (1/2) 1.0000 0 2 15 5P (3/2) 0.0000 0 2 16 4D (3/2) 1.0000 0 3 17 4D (5/2) 0.0000 0 3 18 6S (1/2) 1.0000 0 4 19 6P (1/2) 1.0000 0 5 20 6P (3/2) 0.0000 0 5","title":"Example 1 - basis set for neutral atoms"},{"location":"basis_neutral/#examples-neutral-atoms","text":"The following instructions assume familiarity with the main programs of the pCI package . In this section, we describe a method used to construct basis sets for the case of neutral atoms. In these example, we utilize a single HFD.INP to construct the orbitals.","title":"Examples: Neutral atoms"},{"location":"basis_neutral/#ac","text":"Ac KL = 0 # (0 - new calculation, 1 - continue) NS = 36 # number of orbitals NSO= 24 # number of closed orbitals (in this case only 1s2, 2s2) Z = 89.0 # atomic number AM = 227.00 # atomic mass JM = -2.0 # (-2 - average for non-relativistic configuration) R2 = 60.0 # radius of cavity kbr= 2 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) rnuc= 5.7350 # (optional) rms nuclear radius (https://www-nds.iaea.org/radii/) NL J QQ KP NC 1 1S (1/2) 2.0000 0 0 2 2S (1/2) 2.0000 0 0 3 2P (1/2) 2.0000 0 0 4 2P (3/2) 4.0000 0 0 5 3S (1/2) 2.0000 0 0 6 3P (1/2) 2.0000 0 0 7 3P (3/2) 4.0000 0 0 8 3D (3/2) 4.0000 0 0 9 3D (5/2) 6.0000 0 0 10 4S (1/2) 2.0000 0 0 11 4P (1/2) 2.0000 0 0 12 4P (3/2) 4.0000 0 0 13 4D (3/2) 4.0000 0 0 14 4D (5/2) 6.0000 0 0 15 4F (5/2) 6.0000 0 0 16 4F (7/2) 8.0000 0 0 17 5S (1/2) 2.0000 0 0 18 5P (1/2) 2.0000 0 0 19 5P (3/2) 4.0000 0 0 20 5D (3/2) 4.0000 0 0 21 5D (5/2) 6.0000 0 0 22 6S (1/2) 2.0000 0 0 23 6P (1/2) 2.0000 0 0 24 6P (3/2) 4.0000 0 0 25 7S (1/2) 1.0000 0 1 26 6D (3/2) 1.0000 0 2 27 6D (5/2) 0.0000 0 2 28 7P (1/2) 1.0000 0 3 29 7P (3/2) 0.0000 0 3 30 5F (5/2) 1.0000 0 4 31 5F (7/2) 0.0000 0 4 32 8S (1/2) 1.0000 0 5 33 7D (3/2) 1.0000 0 6 34 7D (5/2) 0.0000 0 6 35 8P (1/2) 1.0000 0 7 36 8P (3/2) 0.0000 0 7","title":"Ac"},{"location":"basis_neutral/#sr","text":"Sr III KL = 0 # (0 - new calculation, 1 - continue) NS = 20 # number of orbitals NSO= 12 # number of closed orbitals (in this case only 1s2, 2s2) Z = 38.0 # atomic number AM = 90.000 # atomic mass JM = -2.0 # (-2 - average for non-relativistic configuration) R2 = 60.0 # radius of cavity kbr= 0 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) NL J QQ KP NC 1 1S (1/2) 2.0000 0 0 2 2S (1/2) 2.0000 0 0 3 2P (1/2) 2.0000 0 0 4 2P (3/2) 4.0000 0 0 5 3S (1/2) 2.0000 0 0 6 3P (1/2) 2.0000 0 0 7 3P (3/2) 4.0000 0 0 8 3D (3/2) 4.0000 0 0 9 3D (5/2) 6.0000 0 0 10 4S (1/2) 2.0000 0 0 11 4P (1/2) 2.0000 0 0 12 4P (3/2) 4.0000 0 0 13 5S (1/2) 1.0000 0 1 14 5P (1/2) 1.0000 0 2 15 5P (3/2) 0.0000 0 2 16 4D (3/2) 1.0000 0 3 17 4D (5/2) 0.0000 0 3 18 6S (1/2) 1.0000 0 4 19 6P (1/2) 1.0000 0 5 20 6P (3/2) 0.0000 0 5","title":"Sr"},{"location":"basis_x/","text":"Building a basis for CI+all-order and CI+MBPT The following instructions assume familiarity with the main programs of the pCI package . CI and all-order basis sets In this section, we describe the general method of building basis sets for the CI+all-order and CI+MBPT code packages. As the CI and all-order code packages were developed separately, they use basis sets in different formats. CI uses HFD.DAT and all-order uses hfspl.1 and hfspl.2 files. For all-order/CI+all-order/CI+MBPT calculations, most of the basis is constructed via the B-splines. However, one needs too many B-splines to reproduce core and lower valence stats with high enough accuracy for heavier atoms. Therefore, core and a few few valence electron wave functions are taken from Dirac-Hartree-Fock (DHF), and a combined basis with splines is built. More splines will mean larger basis for the CI as well, so we would like to avoid this. There also seems to be an issue with the basis converter codes for larger numbers of B-splines. All-order or MBPT calculations involve sums over all possible states. To computer these accurately, one needs a large basis. Generally, we use lmax=6 and Nmax=35 for each of the partial waves (1-35s, 2-35p_{1/2},2-35p_{3/2},\\dots) . CI does not require such large basis sets and it is reducing for CI computations. Note More splines will mean larger basis for the CI as well, so we would like to avoid this. There also seems to be an issue with the basis converter codes for larger numbers of B-splines. Note There is a technical issue of CI codes using Taylor expansion inside the nucleus, while all-order codes use a radial grid that starts from the origin. This makes format conversion somewhat imperfect near the nucleus. Instructions Now we will discuss the steps to build the basis set for CI+all-order and CI+MBPT calculations. Produce B-splines ./tdhf < bas_wj.in - solves DHF equations (reads bas_wj.in and writes fort.1 ). Click here to see a description of bas_wj.in . Sr 20 38 88 0 0 9 13 1 # this line is FORMATTED in 4's #23412341234123412341234123412341234 # Sr - name (just a label) # 20 - number of inputs (core+valence) - must # 38 - Z # 88 - AM # 0 0 do not change # 9 convergence parameter # 13 number of first valence shell # 1 valence (0 for core only) # n kappa (5 or 1 - iteration parameter) 1 -1 5 0.00 # 0.00 - energy guess (leave 0.00 for core) 2 -1 5 0.00 2 1 5 0.00 2 -2 5 0.00 3 -1 5 0.00 3 1 5 0.00 3 -2 5 0.00 3 2 5 0.00 3 -3 5 0.00 4 -1 5 0.00 4 1 5 0.00 4 -2 5 0.00 5 -1 5 -0.20 # -0.10 or -0.20 good start for neutral 5 1 5 -0.20 5 -2 5 -0.20 4 2 5 -0.10 4 -3 5 -0.10 6 -1 5 -0.10 6 1 5 -0.10 6 -2 5 -0.10 1.5 # keep 1.5 or 1, 2.0 for Yb 0.0005 0.03 500 # radial grid (0.00005 for HFS) first point, 0.03, max number of points 1 # keep 1, 2 for deformed nuclei 0.0000 4.8665 2.3 # rnuc, cnuc, t - either rnuc=0 (rms?) or cnuc=0 0.0 # don't change this ./nspl40 < spl.in - produces B-spline basis (reads fort.1 and writes hfspl.1 and hfspl.2 ). Click here to see a description of spl.in . 6 # lmax 60.0 # Radius of cavity (same as hfd) 40 7 # number of spline, order of splines 30/5 40/7 50/9 70/11 0.0 0.00 500 # do not change Note bdhf and bspl40 are used in place of tdhf and nspl40 if Breit corrections are included. Run HFD code ./hfd - solves DHF equations (reads HFD.INP and writes HFD.DAT ) Note hfd can work with partially opened shells, while tdhf and bdhf cannot. Convert B-spline basis to HFD.DAT format ./bas_wj - converts hfspl.1 and hfspl.2 B-spline files and writes WJ.DAT and BASS.INP Note There are a couple of changes that have to be made to the resulting BASS.INP file: 1. The line lst= 0s 1# last orbital to be kept in the basis set has to be deleted. 2. The line orb= 0s 1# first orbital to apply kin.bal. has to be changed to reflect the first orbital not from hfd . 3. The line orb= 5s 1# first orbital for diagonalization can either be kept or changed to reflect the first orbital not from hfd . 4. The lines 1 0.5001 3 0.5001 that include orbitals from hfd have to be changed to 1 0.5001 Build combined basis from HFD.DAT and WJ.DAT ./bass - reads BASS.INP , HFD.DAT and WJ.DAT and writes new HFD.DAT Convert combined HFD.DAT to format used for all-order and second-order codes rm hfspl.1 hfspl.2 - erase the files hfspl.1 and hfspl.2 ./bas_x - reads HFD.DAT and writes hfspl.1 and hfspl.2 The final hfspl.1 and hfspl.2 files are the basis set files that can be read from the all-order part of the package .","title":"Building a basis set for CI+X"},{"location":"basis_x/#building-a-basis-for-ciall-order-and-cimbpt","text":"The following instructions assume familiarity with the main programs of the pCI package .","title":"Building a basis for CI+all-order and CI+MBPT"},{"location":"basis_x/#ci-and-all-order-basis-sets","text":"In this section, we describe the general method of building basis sets for the CI+all-order and CI+MBPT code packages. As the CI and all-order code packages were developed separately, they use basis sets in different formats. CI uses HFD.DAT and all-order uses hfspl.1 and hfspl.2 files. For all-order/CI+all-order/CI+MBPT calculations, most of the basis is constructed via the B-splines. However, one needs too many B-splines to reproduce core and lower valence stats with high enough accuracy for heavier atoms. Therefore, core and a few few valence electron wave functions are taken from Dirac-Hartree-Fock (DHF), and a combined basis with splines is built. More splines will mean larger basis for the CI as well, so we would like to avoid this. There also seems to be an issue with the basis converter codes for larger numbers of B-splines. All-order or MBPT calculations involve sums over all possible states. To computer these accurately, one needs a large basis. Generally, we use lmax=6 and Nmax=35 for each of the partial waves (1-35s, 2-35p_{1/2},2-35p_{3/2},\\dots) . CI does not require such large basis sets and it is reducing for CI computations. Note More splines will mean larger basis for the CI as well, so we would like to avoid this. There also seems to be an issue with the basis converter codes for larger numbers of B-splines. Note There is a technical issue of CI codes using Taylor expansion inside the nucleus, while all-order codes use a radial grid that starts from the origin. This makes format conversion somewhat imperfect near the nucleus.","title":"CI and all-order basis sets"},{"location":"basis_x/#instructions","text":"Now we will discuss the steps to build the basis set for CI+all-order and CI+MBPT calculations. Produce B-splines ./tdhf < bas_wj.in - solves DHF equations (reads bas_wj.in and writes fort.1 ). Click here to see a description of bas_wj.in . Sr 20 38 88 0 0 9 13 1 # this line is FORMATTED in 4's #23412341234123412341234123412341234 # Sr - name (just a label) # 20 - number of inputs (core+valence) - must # 38 - Z # 88 - AM # 0 0 do not change # 9 convergence parameter # 13 number of first valence shell # 1 valence (0 for core only) # n kappa (5 or 1 - iteration parameter) 1 -1 5 0.00 # 0.00 - energy guess (leave 0.00 for core) 2 -1 5 0.00 2 1 5 0.00 2 -2 5 0.00 3 -1 5 0.00 3 1 5 0.00 3 -2 5 0.00 3 2 5 0.00 3 -3 5 0.00 4 -1 5 0.00 4 1 5 0.00 4 -2 5 0.00 5 -1 5 -0.20 # -0.10 or -0.20 good start for neutral 5 1 5 -0.20 5 -2 5 -0.20 4 2 5 -0.10 4 -3 5 -0.10 6 -1 5 -0.10 6 1 5 -0.10 6 -2 5 -0.10 1.5 # keep 1.5 or 1, 2.0 for Yb 0.0005 0.03 500 # radial grid (0.00005 for HFS) first point, 0.03, max number of points 1 # keep 1, 2 for deformed nuclei 0.0000 4.8665 2.3 # rnuc, cnuc, t - either rnuc=0 (rms?) or cnuc=0 0.0 # don't change this ./nspl40 < spl.in - produces B-spline basis (reads fort.1 and writes hfspl.1 and hfspl.2 ). Click here to see a description of spl.in . 6 # lmax 60.0 # Radius of cavity (same as hfd) 40 7 # number of spline, order of splines 30/5 40/7 50/9 70/11 0.0 0.00 500 # do not change Note bdhf and bspl40 are used in place of tdhf and nspl40 if Breit corrections are included. Run HFD code ./hfd - solves DHF equations (reads HFD.INP and writes HFD.DAT ) Note hfd can work with partially opened shells, while tdhf and bdhf cannot. Convert B-spline basis to HFD.DAT format ./bas_wj - converts hfspl.1 and hfspl.2 B-spline files and writes WJ.DAT and BASS.INP Note There are a couple of changes that have to be made to the resulting BASS.INP file: 1. The line lst= 0s 1# last orbital to be kept in the basis set has to be deleted. 2. The line orb= 0s 1# first orbital to apply kin.bal. has to be changed to reflect the first orbital not from hfd . 3. The line orb= 5s 1# first orbital for diagonalization can either be kept or changed to reflect the first orbital not from hfd . 4. The lines 1 0.5001 3 0.5001 that include orbitals from hfd have to be changed to 1 0.5001 Build combined basis from HFD.DAT and WJ.DAT ./bass - reads BASS.INP , HFD.DAT and WJ.DAT and writes new HFD.DAT Convert combined HFD.DAT to format used for all-order and second-order codes rm hfspl.1 hfspl.2 - erase the files hfspl.1 and hfspl.2 ./bas_x - reads HFD.DAT and writes hfspl.1 and hfspl.2 The final hfspl.1 and hfspl.2 files are the basis set files that can be read from the all-order part of the package .","title":"Instructions"},{"location":"changelog/","text":"Changelog [0.2.2] - 2021-04-23 \"sint1\" in dtm_aux.f90: bug fix [0.2.1] - 2021-04-20 \"mpi_wins\" bug fix: added lines to allocate and broadcast Iarr for zero-cores [0.2.0] - 2021-04-09 initial import of new modernized \"basc\" program \"basc\" program has been modernized and included in the build process of the parallel package. \"basc\" has been tested with serial version and all results are identical in output files \"basc\" now utilizes dynamic memory allocation [0.1.0] - 2021-03-28 kv=3 functionality fully parallelized [0.0.5] - 2021-03-23 new module \"matrix_io\": implements parallel reading and writing of matrices (Hamiltonian and J^2) [0.0.4] - 2021-03-22 new type \"Matrix\": encapsulate indices and values of matrix elements (Hamil and Jsq) new module \"mpi_wins\": implements creating and closing MPI windows/shared memory for basis set new module \"mpi_utils\": used for DARWIN's no-ucx variant of intel-2020 to bypass 1GB MPI message limit used intrinsic function PACK to remove all zero valued matrix elements from Hamiltonian and Jsquared function PACK results in seg fault if not using 'ulimit -s unlimited' Fixed discrepancy of NumH and NumJ between serial and parallel versions reorganized timing calls removed unused error variables several minor text edits [0.0.3] - 2021-03-21 dtm now uses conf_init module for reading CONF.INP revamped dtm's Input subroutine moved one-electron operator functions to a separate module amp_ops several minor text edits [0.0.2] - 2021-03-20 dtm now uses determinants module for determinant-based subroutines made arrays storing configurations (iconf1, iconf2) consistent between all programs several minor text edits descriptions added for several subroutines in davidson module [0.0.1] - 2021-03-17 fixed issue with table of J being unordered [0.0.0] - 2021-03-17 original source import","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#022-2021-04-23","text":"\"sint1\" in dtm_aux.f90: bug fix","title":"[0.2.2] - 2021-04-23"},{"location":"changelog/#021-2021-04-20","text":"\"mpi_wins\" bug fix: added lines to allocate and broadcast Iarr for zero-cores","title":"[0.2.1] - 2021-04-20"},{"location":"changelog/#020-2021-04-09","text":"initial import of new modernized \"basc\" program \"basc\" program has been modernized and included in the build process of the parallel package. \"basc\" has been tested with serial version and all results are identical in output files \"basc\" now utilizes dynamic memory allocation","title":"[0.2.0] - 2021-04-09"},{"location":"changelog/#010-2021-03-28","text":"kv=3 functionality fully parallelized","title":"[0.1.0] - 2021-03-28"},{"location":"changelog/#005-2021-03-23","text":"new module \"matrix_io\": implements parallel reading and writing of matrices (Hamiltonian and J^2)","title":"[0.0.5] - 2021-03-23"},{"location":"changelog/#004-2021-03-22","text":"new type \"Matrix\": encapsulate indices and values of matrix elements (Hamil and Jsq) new module \"mpi_wins\": implements creating and closing MPI windows/shared memory for basis set new module \"mpi_utils\": used for DARWIN's no-ucx variant of intel-2020 to bypass 1GB MPI message limit used intrinsic function PACK to remove all zero valued matrix elements from Hamiltonian and Jsquared function PACK results in seg fault if not using 'ulimit -s unlimited' Fixed discrepancy of NumH and NumJ between serial and parallel versions reorganized timing calls removed unused error variables several minor text edits","title":"[0.0.4] - 2021-03-22"},{"location":"changelog/#003-2021-03-21","text":"dtm now uses conf_init module for reading CONF.INP revamped dtm's Input subroutine moved one-electron operator functions to a separate module amp_ops several minor text edits","title":"[0.0.3] - 2021-03-21"},{"location":"changelog/#002-2021-03-20","text":"dtm now uses determinants module for determinant-based subroutines made arrays storing configurations (iconf1, iconf2) consistent between all programs several minor text edits descriptions added for several subroutines in davidson module","title":"[0.0.2] - 2021-03-20"},{"location":"changelog/#001-2021-03-17","text":"fixed issue with table of J being unordered","title":"[0.0.1] - 2021-03-17"},{"location":"changelog/#000-2021-03-17","text":"original source import","title":"[0.0.0] - 2021-03-17"},{"location":"contact/","text":"Contact Us","title":"Contact Us"},{"location":"contact/#contact-us","text":"","title":"Contact Us"},{"location":"examples/","text":"Examples","title":"Examples"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"installation/","text":"Installation Required libraries In order to compile pCI the following software libraries and tools are required: Intel Fortran compiler. CMake build tool. (Optional) MPI library to run on high-performance computing clusters. The codes have only been tested with OpenMPI so far. Obtaining the source code Users can download the latest version of the pCI code package from https://github.com/ccheung93/pCI via git From the command line, you can clone the latest version: git clone https://github.com/ccheung93/pCI.git Compiling with CMake The codes are built using the 'CMakeLists.txt' file. The following are some example builds on the DARWIN computing cluster. A Debug build can be done: $ cd pCI $ mkdir build-debug $ cd build-debug $ vpkg_require cmake openmpi/4.1.0:intel-2020 $ FC=mpifort cmake -DCMAKE_BUILD_TYPE=Debug -DCMAKE_INSTALL_PREFIX=$(pwd)/../20200402-debug .. : $ make $ make install An optimized build demands a little more: $ cd .. $ mkdir build-opt $ cd build-opt $ FC=mpifort cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=$(pwd)/../20200317-opt -DCMAKE_Fortran_FLAGS_RELEASE=\"-g -O3 -mcmodel=large -xHost -m64\" .. : $ make $ make install","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#required-libraries","text":"In order to compile pCI the following software libraries and tools are required: Intel Fortran compiler. CMake build tool. (Optional) MPI library to run on high-performance computing clusters. The codes have only been tested with OpenMPI so far.","title":"Required libraries"},{"location":"installation/#obtaining-the-source-code","text":"Users can download the latest version of the pCI code package from https://github.com/ccheung93/pCI","title":"Obtaining the source code"},{"location":"installation/#via-git","text":"From the command line, you can clone the latest version: git clone https://github.com/ccheung93/pCI.git","title":"via git"},{"location":"installation/#compiling-with-cmake","text":"The codes are built using the 'CMakeLists.txt' file. The following are some example builds on the DARWIN computing cluster. A Debug build can be done: $ cd pCI $ mkdir build-debug $ cd build-debug $ vpkg_require cmake openmpi/4.1.0:intel-2020 $ FC=mpifort cmake -DCMAKE_BUILD_TYPE=Debug -DCMAKE_INSTALL_PREFIX=$(pwd)/../20200402-debug .. : $ make $ make install An optimized build demands a little more: $ cd .. $ mkdir build-opt $ cd build-opt $ FC=mpifort cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=$(pwd)/../20200317-opt -DCMAKE_Fortran_FLAGS_RELEASE=\"-g -O3 -mcmodel=large -xHost -m64\" .. : $ make $ make install","title":"Compiling with CMake"},{"location":"main/","text":"Overview of the pCI code package Figure. pCI code scheme hfd - hartree-fock-dirac The hfd program solves restricted Hartree-Fock-Dirac (HFD) equations self-consistently under the central field approximation to find four-component Dirac-Fock (DF) orbitals and eigenvalues of the HFD Hamiltonian. The program provides the initial approximation, storing both basis radial orbitals \\phi_{nlj}\\equiv r\\left(\\begin{array}{c}f_{nlj}\\\\-g_{nlj}\\end{array}\\right), as well as the radial derivatives of the orbitals \\partial_r\\phi_{nlj} , to the file HFD.DAT . bass - constructing the basis set The bass program forms the DF orbitals for the core and valence shells, then adds virtual orbitals to account for correlations. A reasonable basis set should consist of orbitals mainly localized at the same distances from the origin as the valence orbitals. add - creating the configuration list The add program constructs a list of configurations to define the CI space by exciting electrons from a set of reference configurations to a set of active non-relativistic shells. It takes in the input file ADD.INP , which specifies the reference configurations, active non-relativistic shells, and minimum and maximum occupation numbers of each shell. It writes the file CONF.INP , which includes a list of user-defined parameters and the list of configurations constructed by exciting electrons from a list of basic configurations. The following is a sample input ADD.INP file. Each line has a description of the respective variable. The third block starting with 4f 9 14 is a list of the orbitals and minimum and maximum occupation numbers. For example, 4f 9 14 refers to having a minimum of 9 electrons or a maximum of 14 electrons for the 4f orbital. Ncor= 4 !# number of basic configurations. Must match the list below. NsvNR 16 !# number of active NR shells. The list below may be longer. mult= 2 !# multiplicity of excitations. For full CI use mult=Ne NE = 14 !# number of valence electrons L: 4f14 !# list of basic configurations L: 4f13 5p1 !# from which electrons are excited from. L: 4f12 5s2 !# the number of configurations listed here L: 4f11 5s2 5p1 !# must match the number on the first line 'Ncor= 4' ## nnlee nnlee nnlee !# formatting of configurations !# the numbers nn refer to the principal quantum number !# the letters l refer to the angular momentum quantum number !# the numbers ee refer to the occupation of that orbital 4f 9 14 5s 0 2 5p 0 3 5d 0 2 5f 0 2 5g 0 2 6s 0 2 6p 0 2 6d 0 2 6f 0 2 6g 0 2 7s 0 2 7p 0 2 7d 0 2 7f 0 2 7g 0 2 ##nnl ee ee nnl ee ee nnl ee ee nnl ee ee nnl ee ee nnl ee ee >>>>>>>>>>>>> Head of the file CONF.INP >>>>>>>>>>>>>>>>>>>>>>>> Ir17+_even # ion_parity Z = 77.0 # atomic number Am = 192.0 # atomic weight J = 4.0 # total angular momentum Jm = 4.0 # angular momentum projection Nso= 14 # number of closed core shells Nc = 10 # number of relativistic configurations (ignored in add program) Kv = 4 # Kv = (3 - use projections, 4 - no projections) Nlv= 5 # number of energy levels Ne = 14 # number of valence electrons Kl4= 1 # Kl4 = (1 - initial approx. from energy matrix, 2 - initial approx. from CONF.XIJ file) Nc4= 20 # number of relativistic configurations in initial approximation Gj = 0.0000 # Crt4= 0.0001 # cutoff criteria for davidson convergence kout= 0 # key for level of output (0 - low detail output, 1 - detailed output) Ncpt= 0 # number of relativistic configurations in PT block (ignored in add program) Cut0= 0.0001 # cutoff criteria for weights of PT configurations N_it= 100 # number of davidson iterations Kbrt= 1 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) Gnuc= 1.07 # gyromagnetic ratio 0.1002 0.2002 -0.2102 0.2104 0.3002 -0.3102 0.3104 -0.3204 0.3206 0.4002 -0.4102 0.4104 -0.4204 0.4206 ================================================================== Note The second block listing the basic configurations has a specific formatting __nnlee__ , where __ indicate spaces, nn is the principal quantum number, l is the angular momentum quantum number as a letter ( s=0 , l=1 , d=2 , ...), and ee is the number of electrons in that orbital. Note The order in which the configurations and basis orbitals must be listed identically with those from BASS.INP . basc - calculating radial integrals After the configuration list has been created, the next step is to calculate the radial integrals using the program basc . basc calculates one-electron and two-electron radial integrals, which are used by the conf program to form the Hamiltonian in the CI space. The one-electron radial integrals correspond to the DF potential of the core, and the two-electron radial integrals account for the Coulomb and Breit interactions between the valence electrons. The matrix elements of the Coulomb interaction for the multipolarity k can be written as \\langle c,d|V_q^k|a,b\\rangle \\equiv G_q^k(ca) G_q^k(bd) R_{abcd}^k, where the angular factors G_q^k(fi) (known as relativistic Gaunt coefficients) are given by G_q^k(fi)=(-1)^{m_f+1/2}\\delta_p\\sqrt{(2j_i+1)(2j_f+1)} \\begin{pmatrix} j_f & j_i & k \\\\ -m_f & m_i & q \\end{pmatrix} \\begin{pmatrix} j_f & j_i & k \\\\ 1/2 & -1/2 & 0 \\end{pmatrix}, and R_{abcd}^k are the relativistic Coulomb radial integrals, and \\delta_p accounts for the parity selection rule \\delta_p=\\xi(l_i+l_f+k), \\hspace{0.2in}\\xi(n)=\\Bigg\\{ \\begin{matrix} 1 & \\text{if \\( n \\) is even,} \\\\ 0 & \\text{if \\( n \\) is odd.} \\end{matrix} The Breit interaction has the same form as the Coulomb interaction, but without the parity selection rule. The basc reads in the files HFD.DAT and CONF.INP to determine which radial integrals are needed. These integrals are calculated and written to the files CONF.INT . The relativistic Gaunt coefficients are written to the file CONF.GNT , and the file CONF.DAT is also formed, storing the basis radial orbitals \\phi_{nlj} , as well as functions \\chi_{nlj} = h_\\text{DF}^r\\phi_{nlj} . conf - configuration interaction In this section, we will introduce how to run the CI program conf . Here is a summary of the input and output files used in conf . Input Files: HFD.DAT - basis set radial orbitals \\phi_{nlj} and radial derivatives of the orbitals \\partial_r\\phi_{nlj} CONF.DAT - basis set radial orbitals \\phi_{nlj} and functions \\chi_{nlj}=h_\\text{HF}^r\\phi_{nlj} , where h_\\text{HF}^r is the radial part of the Dirac-Fock operator CONF.GNT - relativistic Gaunt coefficients produced by basc CONF.INT - relativistic Coulomb coefficients produced by basc SGC.CON (optional) - one-electron effective radial integrals of the MBPT/all-order corrections SCRC.CON (optional) - two-electron effective radial integrals of the MBPT/all-order corrections CONF.INP - list of relativistic configurations and user defined parameters c.in - input file with keys Kl , Ksig , and Kdsig Kl = (0 - start, 1 - continue calculation, 2 - include corrections, 3 - add configurations) Ksig = (0 - pure CI, 1 - include one-electron corrections, 2 - include one- and two-electron corrections) Kdsig = (0 - no energy dependence on Sigma, 1 - energy dependence on Sigma) Output Files: CONF.DET - basis set of determinants CONF.HIJ - indices and values of the Hamiltonian matrix elements CONF.JJJ - indices and values of the matrix elements of the operator J^2 CONF.XIJ - quantum numbers, eigenvalues and eigenvectors of the Hamiltonian CONF.RES - final table of energy eigenvalues and the weights of all configurations contributing to each term The following is a sample of the head of a CONF.INP for calculating the even-parity states of Ir ^{17+} . Ir17+_even # ion_parity Z = 77.0 # atomic number Am =193.0 # atomic weight J = 2.0 # total angular momentum Jm = 2.0 # angular momentum projection Nso= 14 # number of closed core shells Nc = 481 # number of relativistic configurations Kv = 4 # Kv = (3 - use projections, 4 - no projections) Nlv= 5 # number of energy levels Ne = 14 # number of valence electrons Kl4= 1 # Kl4 = (1 - initial approx. from energy matrix, 2 - initial approx. from CONF.XIJ file) Nc4= 28 # number of relativistic configurations in initial approximation Gj = 0.0000 # Crt4= 0.0001 # cutoff criteria for davidson convergence kout= 0 # key for level of output (0 - low detail output, 1 - detailed output) Ncpt= 1132 # number of relativistic configurations in PT block (used in conf_pt) Cut0= 0.0001 # cutoff criteria for davidson convergence N_it= 100 # number of davidson iterations Kbrt= 1 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) Gnuc= 1.07 # gyromagnetic ratio 0.1002 0.2002 -0.2102 0.2104 0.3002 -0.3102 0.3104 -0.3204 0.3206 0.4002 -0.4102 0.4104 -0.4204 0.4206 1 1-0.4306 0.4308 2 2-0.4306 0.4306 0.5002 3-0.4305 0.4307 0.5002 4-0.4304 0.4308 0.5002 3 5-0.4305 0.4308 -0.5101 6-0.4305 0.4308 0.5101 7-0.4306 0.4307 -0.5101 8-0.4306 0.4307 0.5101 : Note The first 5 columns up to the '=' sign are fixed, and the program will give an error if there are any discrepancies here. Note The list of core shells are fixed to have a maximum of 6 shells per row. In the CONF.INP file shown above, we include 481 relativistic configurations in the CI space, and 1132 relativistic configurations in the PT space (if conf_pt is to be used after conf ). dtm - density transition matrix The dtm program calculates matrix elements of one-electron operators between many-electron states, under the density (or transition) matrix formalism. This formalism allows us to express the matrix elements between many-electron states via one-electron matrix elements. The dtm program forms these reduced density (or transition) matrices and calculates the reduced matrix elements. The following quantities can be calculated from this program: - electron g-factors - magnetic dipole and electronic quadrupole hyperfine structure constants A and B - electric ( Ek ) and magnetic ( Mk ) multipole transition amplitudes, where k = 1,2,3 corresponds to the dipole, quadrupole, and octupole transitions - nuclear spin independent parity nonconserving (PNC) amplitude - amplitude of the electron interaction with the P-odd nuclear anapole moment (AM) - P, T-odd interaction of the dipole electric dipole moment - nucleus magnetic quadrupole moment This program begins by reading the file CONF.INP for system parameters and the list of configurations. Next, basis radial orbitals are read from the file CONF.DAT , and radial integrals for all operators are calculated and written to the file DTM.INT . If this file already exists, dtm uses it and does not recalculate the radial integrals. For the diagonal matrix elements, the list of determinants and eigenvectors corresponding to the state of interest are read from the files CONF.DET and CONF.XIJ , respectively. For the non-diagonal matrix elements, the initial state is read from the file CONF.DET and CONF.XIJ , and the final state is read from the files CONF1.DET and CONF1.XIJ . The results of the diagonal and non-diagonal matrix elements are written to the files DM.RES and TM.RES , respectively. dtm takes in as input the input file dtm.in : 2 # 1 for DM (as g-factor or hyperfine), 2 for transitions 1 1 12 # from 1st even level to 1st-12th odd levels Note Some 3J -coefficients might be zero in some cases, such as trying to compute E1 matrix element for 5s^2\\, {}^1S_0 \\rightarrow 5s5p\\,{}^3P_1 . This will fail if the odd run had J=0 , M_J=0 . You would need to have an odd run with J=1 , M_J=1 ine - polarizabilities The ine program calculates static and dynamic polarizabilities of specified atomic levels. ine only gives the valence polarizability. Core polarizability needs to be computed separately with a different code. The code will give both scalar and tensor polarizabilities if the tensor polarizability is not zero, but not the vector polarizability. There are several version of the code, but for now we will use ine_dyn_E28 . This program requires several input files from previously ran conf and dtm programs, including CONF.DET and CONF.XIJ of the parity of the level of interest (renamed to CONF0.DET and CONF0.XIJ ), CONF.INP , CONF.XIJ , CONF.HIJ , and CONF.JJJ of the opposite parity, and the file DTM.INT from dtm . For example, if we want to calculate polarizabilities for an even state: cp CONFeven.DET CONF0.DET cp CONFeven.XIJ CONF0.XIJ cp CONFodd.INP CONF.INP cp CONFodd.XIJ CONF.XIJ cp CONFodd.HIJ CONF.HIJ cp CONFodd.JJJ CONF.JJJ ine_dyn_E28 can either solve the inhomogeneous equation iteratively by solving for a smaller matrix first, or by direct matrix inversion via the LAPACK library. It is controlled by the parameter IP1 in conf.par : PARAMETER(IP1 = 15000, ! Nd1 - number of determinants for direct diagonalization This parameter can be set to be larger than the number of determinants in your problem if you don't want the program to iterate at all. The problem with iterations is that they diverge in many cases for dynamic polarizabilities. However, the problem with the direct solution is that it takes a long time to run (about 20-30 min even for 10, 000 determinants). The program can be executed via the command: ./ine_dyn_E28 <inf.dtm Click here to see a description of inf.dtm . 0 # start new computation 1 # calculate polarizability of the first level 0 # 0 for static, omega for dynamic For dynamic polarizability, you need to run ine_dyn_E28 twice: once with +\\omega and once with -\\omega . For example, if one needs the polarizability for \\lambda=800 \\text{ nm} , compute \\omega in a.u.: \\omega=1.0\\times 10^7 / (au\\times\\lambda) = 0.056954191 , where au=219474.6313705 . Run ine_dyn_E28 twice, once with 0 1 0.056954191 and once with 0 1 -0.056954191 then average the two results.","title":"pCI code package"},{"location":"main/#overview-of-the-pci-code-package","text":"Figure. pCI code scheme","title":"Overview of the pCI code package"},{"location":"main/#hfd-hartree-fock-dirac","text":"The hfd program solves restricted Hartree-Fock-Dirac (HFD) equations self-consistently under the central field approximation to find four-component Dirac-Fock (DF) orbitals and eigenvalues of the HFD Hamiltonian. The program provides the initial approximation, storing both basis radial orbitals \\phi_{nlj}\\equiv r\\left(\\begin{array}{c}f_{nlj}\\\\-g_{nlj}\\end{array}\\right), as well as the radial derivatives of the orbitals \\partial_r\\phi_{nlj} , to the file HFD.DAT .","title":"hfd - hartree-fock-dirac"},{"location":"main/#bass-constructing-the-basis-set","text":"The bass program forms the DF orbitals for the core and valence shells, then adds virtual orbitals to account for correlations. A reasonable basis set should consist of orbitals mainly localized at the same distances from the origin as the valence orbitals.","title":"bass - constructing the basis set"},{"location":"main/#add-creating-the-configuration-list","text":"The add program constructs a list of configurations to define the CI space by exciting electrons from a set of reference configurations to a set of active non-relativistic shells. It takes in the input file ADD.INP , which specifies the reference configurations, active non-relativistic shells, and minimum and maximum occupation numbers of each shell. It writes the file CONF.INP , which includes a list of user-defined parameters and the list of configurations constructed by exciting electrons from a list of basic configurations. The following is a sample input ADD.INP file. Each line has a description of the respective variable. The third block starting with 4f 9 14 is a list of the orbitals and minimum and maximum occupation numbers. For example, 4f 9 14 refers to having a minimum of 9 electrons or a maximum of 14 electrons for the 4f orbital. Ncor= 4 !# number of basic configurations. Must match the list below. NsvNR 16 !# number of active NR shells. The list below may be longer. mult= 2 !# multiplicity of excitations. For full CI use mult=Ne NE = 14 !# number of valence electrons L: 4f14 !# list of basic configurations L: 4f13 5p1 !# from which electrons are excited from. L: 4f12 5s2 !# the number of configurations listed here L: 4f11 5s2 5p1 !# must match the number on the first line 'Ncor= 4' ## nnlee nnlee nnlee !# formatting of configurations !# the numbers nn refer to the principal quantum number !# the letters l refer to the angular momentum quantum number !# the numbers ee refer to the occupation of that orbital 4f 9 14 5s 0 2 5p 0 3 5d 0 2 5f 0 2 5g 0 2 6s 0 2 6p 0 2 6d 0 2 6f 0 2 6g 0 2 7s 0 2 7p 0 2 7d 0 2 7f 0 2 7g 0 2 ##nnl ee ee nnl ee ee nnl ee ee nnl ee ee nnl ee ee nnl ee ee >>>>>>>>>>>>> Head of the file CONF.INP >>>>>>>>>>>>>>>>>>>>>>>> Ir17+_even # ion_parity Z = 77.0 # atomic number Am = 192.0 # atomic weight J = 4.0 # total angular momentum Jm = 4.0 # angular momentum projection Nso= 14 # number of closed core shells Nc = 10 # number of relativistic configurations (ignored in add program) Kv = 4 # Kv = (3 - use projections, 4 - no projections) Nlv= 5 # number of energy levels Ne = 14 # number of valence electrons Kl4= 1 # Kl4 = (1 - initial approx. from energy matrix, 2 - initial approx. from CONF.XIJ file) Nc4= 20 # number of relativistic configurations in initial approximation Gj = 0.0000 # Crt4= 0.0001 # cutoff criteria for davidson convergence kout= 0 # key for level of output (0 - low detail output, 1 - detailed output) Ncpt= 0 # number of relativistic configurations in PT block (ignored in add program) Cut0= 0.0001 # cutoff criteria for weights of PT configurations N_it= 100 # number of davidson iterations Kbrt= 1 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) Gnuc= 1.07 # gyromagnetic ratio 0.1002 0.2002 -0.2102 0.2104 0.3002 -0.3102 0.3104 -0.3204 0.3206 0.4002 -0.4102 0.4104 -0.4204 0.4206 ================================================================== Note The second block listing the basic configurations has a specific formatting __nnlee__ , where __ indicate spaces, nn is the principal quantum number, l is the angular momentum quantum number as a letter ( s=0 , l=1 , d=2 , ...), and ee is the number of electrons in that orbital. Note The order in which the configurations and basis orbitals must be listed identically with those from BASS.INP .","title":"add - creating the configuration list"},{"location":"main/#basc-calculating-radial-integrals","text":"After the configuration list has been created, the next step is to calculate the radial integrals using the program basc . basc calculates one-electron and two-electron radial integrals, which are used by the conf program to form the Hamiltonian in the CI space. The one-electron radial integrals correspond to the DF potential of the core, and the two-electron radial integrals account for the Coulomb and Breit interactions between the valence electrons. The matrix elements of the Coulomb interaction for the multipolarity k can be written as \\langle c,d|V_q^k|a,b\\rangle \\equiv G_q^k(ca) G_q^k(bd) R_{abcd}^k, where the angular factors G_q^k(fi) (known as relativistic Gaunt coefficients) are given by G_q^k(fi)=(-1)^{m_f+1/2}\\delta_p\\sqrt{(2j_i+1)(2j_f+1)} \\begin{pmatrix} j_f & j_i & k \\\\ -m_f & m_i & q \\end{pmatrix} \\begin{pmatrix} j_f & j_i & k \\\\ 1/2 & -1/2 & 0 \\end{pmatrix}, and R_{abcd}^k are the relativistic Coulomb radial integrals, and \\delta_p accounts for the parity selection rule \\delta_p=\\xi(l_i+l_f+k), \\hspace{0.2in}\\xi(n)=\\Bigg\\{ \\begin{matrix} 1 & \\text{if \\( n \\) is even,} \\\\ 0 & \\text{if \\( n \\) is odd.} \\end{matrix} The Breit interaction has the same form as the Coulomb interaction, but without the parity selection rule. The basc reads in the files HFD.DAT and CONF.INP to determine which radial integrals are needed. These integrals are calculated and written to the files CONF.INT . The relativistic Gaunt coefficients are written to the file CONF.GNT , and the file CONF.DAT is also formed, storing the basis radial orbitals \\phi_{nlj} , as well as functions \\chi_{nlj} = h_\\text{DF}^r\\phi_{nlj} .","title":"basc - calculating radial integrals"},{"location":"main/#conf-configuration-interaction","text":"In this section, we will introduce how to run the CI program conf . Here is a summary of the input and output files used in conf . Input Files: HFD.DAT - basis set radial orbitals \\phi_{nlj} and radial derivatives of the orbitals \\partial_r\\phi_{nlj} CONF.DAT - basis set radial orbitals \\phi_{nlj} and functions \\chi_{nlj}=h_\\text{HF}^r\\phi_{nlj} , where h_\\text{HF}^r is the radial part of the Dirac-Fock operator CONF.GNT - relativistic Gaunt coefficients produced by basc CONF.INT - relativistic Coulomb coefficients produced by basc SGC.CON (optional) - one-electron effective radial integrals of the MBPT/all-order corrections SCRC.CON (optional) - two-electron effective radial integrals of the MBPT/all-order corrections CONF.INP - list of relativistic configurations and user defined parameters c.in - input file with keys Kl , Ksig , and Kdsig Kl = (0 - start, 1 - continue calculation, 2 - include corrections, 3 - add configurations) Ksig = (0 - pure CI, 1 - include one-electron corrections, 2 - include one- and two-electron corrections) Kdsig = (0 - no energy dependence on Sigma, 1 - energy dependence on Sigma) Output Files: CONF.DET - basis set of determinants CONF.HIJ - indices and values of the Hamiltonian matrix elements CONF.JJJ - indices and values of the matrix elements of the operator J^2 CONF.XIJ - quantum numbers, eigenvalues and eigenvectors of the Hamiltonian CONF.RES - final table of energy eigenvalues and the weights of all configurations contributing to each term The following is a sample of the head of a CONF.INP for calculating the even-parity states of Ir ^{17+} . Ir17+_even # ion_parity Z = 77.0 # atomic number Am =193.0 # atomic weight J = 2.0 # total angular momentum Jm = 2.0 # angular momentum projection Nso= 14 # number of closed core shells Nc = 481 # number of relativistic configurations Kv = 4 # Kv = (3 - use projections, 4 - no projections) Nlv= 5 # number of energy levels Ne = 14 # number of valence electrons Kl4= 1 # Kl4 = (1 - initial approx. from energy matrix, 2 - initial approx. from CONF.XIJ file) Nc4= 28 # number of relativistic configurations in initial approximation Gj = 0.0000 # Crt4= 0.0001 # cutoff criteria for davidson convergence kout= 0 # key for level of output (0 - low detail output, 1 - detailed output) Ncpt= 1132 # number of relativistic configurations in PT block (used in conf_pt) Cut0= 0.0001 # cutoff criteria for davidson convergence N_it= 100 # number of davidson iterations Kbrt= 1 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) Gnuc= 1.07 # gyromagnetic ratio 0.1002 0.2002 -0.2102 0.2104 0.3002 -0.3102 0.3104 -0.3204 0.3206 0.4002 -0.4102 0.4104 -0.4204 0.4206 1 1-0.4306 0.4308 2 2-0.4306 0.4306 0.5002 3-0.4305 0.4307 0.5002 4-0.4304 0.4308 0.5002 3 5-0.4305 0.4308 -0.5101 6-0.4305 0.4308 0.5101 7-0.4306 0.4307 -0.5101 8-0.4306 0.4307 0.5101 : Note The first 5 columns up to the '=' sign are fixed, and the program will give an error if there are any discrepancies here. Note The list of core shells are fixed to have a maximum of 6 shells per row. In the CONF.INP file shown above, we include 481 relativistic configurations in the CI space, and 1132 relativistic configurations in the PT space (if conf_pt is to be used after conf ).","title":"conf - configuration interaction"},{"location":"main/#dtm-density-transition-matrix","text":"The dtm program calculates matrix elements of one-electron operators between many-electron states, under the density (or transition) matrix formalism. This formalism allows us to express the matrix elements between many-electron states via one-electron matrix elements. The dtm program forms these reduced density (or transition) matrices and calculates the reduced matrix elements. The following quantities can be calculated from this program: - electron g-factors - magnetic dipole and electronic quadrupole hyperfine structure constants A and B - electric ( Ek ) and magnetic ( Mk ) multipole transition amplitudes, where k = 1,2,3 corresponds to the dipole, quadrupole, and octupole transitions - nuclear spin independent parity nonconserving (PNC) amplitude - amplitude of the electron interaction with the P-odd nuclear anapole moment (AM) - P, T-odd interaction of the dipole electric dipole moment - nucleus magnetic quadrupole moment This program begins by reading the file CONF.INP for system parameters and the list of configurations. Next, basis radial orbitals are read from the file CONF.DAT , and radial integrals for all operators are calculated and written to the file DTM.INT . If this file already exists, dtm uses it and does not recalculate the radial integrals. For the diagonal matrix elements, the list of determinants and eigenvectors corresponding to the state of interest are read from the files CONF.DET and CONF.XIJ , respectively. For the non-diagonal matrix elements, the initial state is read from the file CONF.DET and CONF.XIJ , and the final state is read from the files CONF1.DET and CONF1.XIJ . The results of the diagonal and non-diagonal matrix elements are written to the files DM.RES and TM.RES , respectively. dtm takes in as input the input file dtm.in : 2 # 1 for DM (as g-factor or hyperfine), 2 for transitions 1 1 12 # from 1st even level to 1st-12th odd levels Note Some 3J -coefficients might be zero in some cases, such as trying to compute E1 matrix element for 5s^2\\, {}^1S_0 \\rightarrow 5s5p\\,{}^3P_1 . This will fail if the odd run had J=0 , M_J=0 . You would need to have an odd run with J=1 , M_J=1","title":"dtm - density transition matrix"},{"location":"main/#ine-polarizabilities","text":"The ine program calculates static and dynamic polarizabilities of specified atomic levels. ine only gives the valence polarizability. Core polarizability needs to be computed separately with a different code. The code will give both scalar and tensor polarizabilities if the tensor polarizability is not zero, but not the vector polarizability. There are several version of the code, but for now we will use ine_dyn_E28 . This program requires several input files from previously ran conf and dtm programs, including CONF.DET and CONF.XIJ of the parity of the level of interest (renamed to CONF0.DET and CONF0.XIJ ), CONF.INP , CONF.XIJ , CONF.HIJ , and CONF.JJJ of the opposite parity, and the file DTM.INT from dtm . For example, if we want to calculate polarizabilities for an even state: cp CONFeven.DET CONF0.DET cp CONFeven.XIJ CONF0.XIJ cp CONFodd.INP CONF.INP cp CONFodd.XIJ CONF.XIJ cp CONFodd.HIJ CONF.HIJ cp CONFodd.JJJ CONF.JJJ ine_dyn_E28 can either solve the inhomogeneous equation iteratively by solving for a smaller matrix first, or by direct matrix inversion via the LAPACK library. It is controlled by the parameter IP1 in conf.par : PARAMETER(IP1 = 15000, ! Nd1 - number of determinants for direct diagonalization This parameter can be set to be larger than the number of determinants in your problem if you don't want the program to iterate at all. The problem with iterations is that they diverge in many cases for dynamic polarizabilities. However, the problem with the direct solution is that it takes a long time to run (about 20-30 min even for 10, 000 determinants). The program can be executed via the command: ./ine_dyn_E28 <inf.dtm Click here to see a description of inf.dtm . 0 # start new computation 1 # calculate polarizability of the first level 0 # 0 for static, omega for dynamic For dynamic polarizability, you need to run ine_dyn_E28 twice: once with +\\omega and once with -\\omega . For example, if one needs the polarizability for \\lambda=800 \\text{ nm} , compute \\omega in a.u.: \\omega=1.0\\times 10^7 / (au\\times\\lambda) = 0.056954191 , where au=219474.6313705 . Run ine_dyn_E28 twice, once with 0 1 0.056954191 and once with 0 1 -0.056954191 then average the two results.","title":"ine - polarizabilities"},{"location":"qed/","text":"How to include QED corrections The following instructions assume familiarity with the main programs of the pCI package . Steps to run a QED calculation In this section, we will introduce calculations including QED corrections. Construct basis set by running hfd and bass to obtain HFD.DAT Run sgc to form an empty SGC.CON file cp HFD.DAT HFD-noQED.DAT - save a copy of HFD.DAT without QED Create a file qedpot.inp with number corresponding to the variant of the QED potential and the name of the HFD.DAT file. Click here to see a description of qedpot.inp . 1 # kvar=1-5, Variant of QED potential HFD.DAT # name of file holding basis set Create a file qed.in selecting options. Click here to see a description of qed.in . 1 # 1 for general diagonalization, 2 for first-order 1 # 1 for no QED, 2 for QED 2 # 0 for Coulomb, 1 for Gaunt, 2 for Full Breit Loop until convergence: Run qedpot_conf to construct selected QED potential Run qed_rot to rotate orbitals to diagonalize Hamiltonian with QED corrections cp SGC.CON SGC-noQED.CON - save a copy of SGC.CON without QED Run qedpot_conf Rename SGCqed.CON to SGC.CON Run conf You can also use the following batch.qed script for steps 3-9, making sure to change inputs relevant to your job. #! /bin/bash -fe kvar=1 # variant of QED potential bin='./' qedpot=$bin'qedpot_conf' qedrot=$bin'qed_rot' iter=25 # max number of iterations ##################################### cat >qedpot.inp <<EndofFile $kvar HFD.DAT EndofFile ##################################### cat >qed.in <<EndofFile 1 # diagonalization 1 # 1 for noQED, 2 for QED 2 # 0 for Coulomb, 1 for Gaunt, 2 for Full Breit EndofFile ##################################### n=1 while [ $n -lt $iter ]; do echo 'Iteration '$n $qedpot >qp.res $qedrot <qed.in >qr.res grep 'changed' \"QED_ROT.RES\" if grep -q reached \"QED_ROT.RES\"; then echo 'Converged in '$n' iterations' break fi let n=n+1 done cp SGC.CON SCG-noQED.CON ./qedpot_conf >qp.res cp SGCqed.CON SGC.CON #####################################","title":"Running QED"},{"location":"qed/#how-to-include-qed-corrections","text":"The following instructions assume familiarity with the main programs of the pCI package .","title":"How to include QED corrections"},{"location":"qed/#steps-to-run-a-qed-calculation","text":"In this section, we will introduce calculations including QED corrections. Construct basis set by running hfd and bass to obtain HFD.DAT Run sgc to form an empty SGC.CON file cp HFD.DAT HFD-noQED.DAT - save a copy of HFD.DAT without QED Create a file qedpot.inp with number corresponding to the variant of the QED potential and the name of the HFD.DAT file. Click here to see a description of qedpot.inp . 1 # kvar=1-5, Variant of QED potential HFD.DAT # name of file holding basis set Create a file qed.in selecting options. Click here to see a description of qed.in . 1 # 1 for general diagonalization, 2 for first-order 1 # 1 for no QED, 2 for QED 2 # 0 for Coulomb, 1 for Gaunt, 2 for Full Breit Loop until convergence: Run qedpot_conf to construct selected QED potential Run qed_rot to rotate orbitals to diagonalize Hamiltonian with QED corrections cp SGC.CON SGC-noQED.CON - save a copy of SGC.CON without QED Run qedpot_conf Rename SGCqed.CON to SGC.CON Run conf You can also use the following batch.qed script for steps 3-9, making sure to change inputs relevant to your job. #! /bin/bash -fe kvar=1 # variant of QED potential bin='./' qedpot=$bin'qedpot_conf' qedrot=$bin'qed_rot' iter=25 # max number of iterations ##################################### cat >qedpot.inp <<EndofFile $kvar HFD.DAT EndofFile ##################################### cat >qed.in <<EndofFile 1 # diagonalization 1 # 1 for noQED, 2 for QED 2 # 0 for Coulomb, 1 for Gaunt, 2 for Full Breit EndofFile ##################################### n=1 while [ $n -lt $iter ]; do echo 'Iteration '$n $qedpot >qp.res $qedrot <qed.in >qr.res grep 'changed' \"QED_ROT.RES\" if grep -q reached \"QED_ROT.RES\"; then echo 'Converged in '$n' iterations' break fi let n=n+1 done cp SGC.CON SCG-noQED.CON ./qedpot_conf >qp.res cp SGCqed.CON SGC.CON #####################################","title":"Steps to run a QED calculation"},{"location":"theory/","text":"Theory For any many-electron system, we can divide all electrons into core and valence electrons. In this way, we can separate the electron-electron correlation problem into one describing the valence-valence correlations under the frozen-core approximation, and another describing the core-core and core-valence correlations. In the initial approximation, we start from the solution of the restricted Dirac-Hartree-Fock (HFD) equations in the central field approximation to construct one-electron orbitals for the core and valence electrons. Virtual orbitals can be constructed from B-splines or by other means to account for correlations. The valence-valence correlation problem is solved using the CI method, while core-core and core-valence correlations are included using either MBPT or the all-order method. In either case, we form an effective Hamiltonian in the valence CI space, then diagonalize the effective Hamiltonian using the CI method to find energies and wave functions for the low-lying states. Configuration Interaction The CI method is a standard ab initio method for calculating atomic properties of many-electron systems. In the valence space, the CI wave function is constructed as a linear combination of all distinct states of a specified angular momentum J and parity \\Psi_J=\\sum_ic_i\\Phi_i, where the set \\left\\{\\Phi_i\\right\\} are Slater determinants generated by exciting electrons from some reference configurations obtained to higher orbitals. Varying the coefficients $c_i$ results in a generalized eigenvalue problem \\sum_j\\langle\\Phi_i|H|\\Phi_j\\rangle c_j = Ec_i, which can be written in matrix form and diagonalized to find the energies and wave functions of the low-lying states. The energy matrix of the CI method can be obtained as a projection of the exact Hamiltonian H onto the CI subspace H^\\text{CI} [^1] H^\\text{CI}=E_\\text{core}+\\sum_{i>N_\\text{core}}h_i^\\text{CI}+\\sum_{j>i>N_\\text{core}}V_{ij}, where E_\\text{core} is the energy of the frozen core, N_\\text{core} is the number of core electrons, h_i^\\text{CI} accounts for the kinetic energy of the valence electrons and their interaction with the central field, and V_{ij} accounts for the valence-valence correlations. Having obtained from CI the many-electron states |J M\\rangle and |J' M'\\rangle with the total angular momenta J,J' and their projections M,M' , one can form density transition matrix in terms of the one-electron states |nljm\\rangle [^2] \\hat{\\rho}=\\rho_{nljm,n^\\prime l^\\prime j^\\prime m^\\prime}|nljm\\rangle\\langle n^\\prime l^\\prime j^\\prime m^\\prime|, where \\rho_{nljm,n^\\prime l^\\prime j^\\prime m^\\prime}=\\langle J^\\prime M^\\prime|a_{n^\\prime l^\\prime j^\\prime m^\\prime}^\\dagger a_{nljm}|JM\\rangle. Here un-primed indices refer to the initial state and primed indices refer to the final state. The many-electron matrix element can then be written as \\langle J^\\prime M^\\prime|T_q^L|JM\\rangle=\\text{Tr}\\,\\rho_{nljm,n^\\prime l^\\prime j^\\prime m^\\prime}\\langle n^\\prime l^\\prime j^\\prime m^\\prime|T_q^L|nljm\\rangle, where the trace sums over all quantum numbers (nljm) and (n^\\prime l^\\prime j^\\prime m^\\prime) , and T_q^L is the spherical component of the tensor operator of rank L . Using the Wigner-Eckart theorem, one can reduce the many-electron matrix element to \\langle J^\\prime \\Vert T^L \\Vert J\\rangle = \\text{Tr}\\,\\rho_{nlj,n^\\prime l^\\prime j^\\prime}^L \\langle n^\\prime l^\\prime j^\\prime\\Vert T^L \\Vert nlj\\rangle, where \\rho_{nlj,n^\\prime l^\\prime j^\\prime}^L = (-1)^{J^\\prime -M^\\prime}\\left( \\begin{array}{ccc} J^\\prime & L & J \\\\ -M^\\prime & q & M \\end{array}\\right)^{-1} \\sum_{mm^\\prime} (-1)^{j^\\prime-m^\\prime}\\left( \\begin{array}{ccc} j^\\prime & L & j \\\\ -m^\\prime & q & m \\end{array}\\right) \\rho_{nljm,n^\\prime l^\\prime j^\\prime m^\\prime}. We have developed new parallel programs based on these methods: conf realizes the CI method, which forms the CI Hamiltonian and uses Davidson's algorithm of diagonalization [^3] to find low-lying energies and wave functions; dtm calculates reduced matrix elements of one-electron operators by forming the reduced density transition matrices. Valence Perturbation Theory As the number of configurations contributing to the CI wave function grows exponentially with the number of valence electrons, efficient selection of the most important configurations from a set of configurations becomes the main challenge of accurate computations. To significantly reduce the number of configurations, we further developed a method suggested in Ref. [^4] to predict important configurations based on a set of configurations with known weights. This method can be used to optimize the CI space by identifying the most important configurations from a list of CI configurations using perturbation theory (PT). All second-order corrections are taken into account and added to the energy calculated from CI to obtain the total energy, E^\\text{CI}=E_0+E_1 , while first-order corrections to the wave functions are stored for use in subsequent CI calculations. This process of using CI on a small subspace, calculating corrections via PT, and reordering the list of configurations in descending weights can be repeated several times to form the most optimal CI subspace. Once the energy differences between subsequent CI calculations are relatively small, it can be assumed that convergence has been met. We've developed a new parallel program conf_pt that realizes the CI+PT method. The parallel version enables computations of extremely large problems, with tests running up to 400 million determinants. References [^1]: V. A. Dzuba, V. V. Flambaum, and M. G. Kozlov. Combination of the many-body perturbation theory with the configuration-interaction method. Phys. Rev. A , 54 5 :3948\u20133959, November 1996. [^2]: M. G. Kozlov, S. G. Porsev, M. S. Safronova, and I. I. Tupitsyn. CI-MBPT: A package of programs for relativistic atomic calculations based on a method combining configuration interaction and many-body perturbation theory. Computer Physics Communications , 195:199\u2013213, 2015. [^3]: Ernest R. Davidson. The iterative calculation of a few of the lowest eigenvalues and corresponding eigenvectors of large real-symmetric matrices. Journal of Computational Physics , 17 1 :87\u201394, January 1975. [^4]: Yu. G. Rakhlina, M. G. Kozlov, and S. G. Porsev. The energy of electron affinity to a zirconium atom. Optics and Spectroscopy , 90 6 :817\u2013821, June 2001.","title":"Theory"},{"location":"theory/#theory","text":"For any many-electron system, we can divide all electrons into core and valence electrons. In this way, we can separate the electron-electron correlation problem into one describing the valence-valence correlations under the frozen-core approximation, and another describing the core-core and core-valence correlations. In the initial approximation, we start from the solution of the restricted Dirac-Hartree-Fock (HFD) equations in the central field approximation to construct one-electron orbitals for the core and valence electrons. Virtual orbitals can be constructed from B-splines or by other means to account for correlations. The valence-valence correlation problem is solved using the CI method, while core-core and core-valence correlations are included using either MBPT or the all-order method. In either case, we form an effective Hamiltonian in the valence CI space, then diagonalize the effective Hamiltonian using the CI method to find energies and wave functions for the low-lying states.","title":"Theory"},{"location":"theory/#configuration-interaction","text":"The CI method is a standard ab initio method for calculating atomic properties of many-electron systems. In the valence space, the CI wave function is constructed as a linear combination of all distinct states of a specified angular momentum J and parity \\Psi_J=\\sum_ic_i\\Phi_i, where the set \\left\\{\\Phi_i\\right\\} are Slater determinants generated by exciting electrons from some reference configurations obtained to higher orbitals. Varying the coefficients $c_i$ results in a generalized eigenvalue problem \\sum_j\\langle\\Phi_i|H|\\Phi_j\\rangle c_j = Ec_i, which can be written in matrix form and diagonalized to find the energies and wave functions of the low-lying states. The energy matrix of the CI method can be obtained as a projection of the exact Hamiltonian H onto the CI subspace H^\\text{CI} [^1] H^\\text{CI}=E_\\text{core}+\\sum_{i>N_\\text{core}}h_i^\\text{CI}+\\sum_{j>i>N_\\text{core}}V_{ij}, where E_\\text{core} is the energy of the frozen core, N_\\text{core} is the number of core electrons, h_i^\\text{CI} accounts for the kinetic energy of the valence electrons and their interaction with the central field, and V_{ij} accounts for the valence-valence correlations. Having obtained from CI the many-electron states |J M\\rangle and |J' M'\\rangle with the total angular momenta J,J' and their projections M,M' , one can form density transition matrix in terms of the one-electron states |nljm\\rangle [^2] \\hat{\\rho}=\\rho_{nljm,n^\\prime l^\\prime j^\\prime m^\\prime}|nljm\\rangle\\langle n^\\prime l^\\prime j^\\prime m^\\prime|, where \\rho_{nljm,n^\\prime l^\\prime j^\\prime m^\\prime}=\\langle J^\\prime M^\\prime|a_{n^\\prime l^\\prime j^\\prime m^\\prime}^\\dagger a_{nljm}|JM\\rangle. Here un-primed indices refer to the initial state and primed indices refer to the final state. The many-electron matrix element can then be written as \\langle J^\\prime M^\\prime|T_q^L|JM\\rangle=\\text{Tr}\\,\\rho_{nljm,n^\\prime l^\\prime j^\\prime m^\\prime}\\langle n^\\prime l^\\prime j^\\prime m^\\prime|T_q^L|nljm\\rangle, where the trace sums over all quantum numbers (nljm) and (n^\\prime l^\\prime j^\\prime m^\\prime) , and T_q^L is the spherical component of the tensor operator of rank L . Using the Wigner-Eckart theorem, one can reduce the many-electron matrix element to \\langle J^\\prime \\Vert T^L \\Vert J\\rangle = \\text{Tr}\\,\\rho_{nlj,n^\\prime l^\\prime j^\\prime}^L \\langle n^\\prime l^\\prime j^\\prime\\Vert T^L \\Vert nlj\\rangle, where \\rho_{nlj,n^\\prime l^\\prime j^\\prime}^L = (-1)^{J^\\prime -M^\\prime}\\left( \\begin{array}{ccc} J^\\prime & L & J \\\\ -M^\\prime & q & M \\end{array}\\right)^{-1} \\sum_{mm^\\prime} (-1)^{j^\\prime-m^\\prime}\\left( \\begin{array}{ccc} j^\\prime & L & j \\\\ -m^\\prime & q & m \\end{array}\\right) \\rho_{nljm,n^\\prime l^\\prime j^\\prime m^\\prime}. We have developed new parallel programs based on these methods: conf realizes the CI method, which forms the CI Hamiltonian and uses Davidson's algorithm of diagonalization [^3] to find low-lying energies and wave functions; dtm calculates reduced matrix elements of one-electron operators by forming the reduced density transition matrices.","title":"Configuration Interaction"},{"location":"theory/#valence-perturbation-theory","text":"As the number of configurations contributing to the CI wave function grows exponentially with the number of valence electrons, efficient selection of the most important configurations from a set of configurations becomes the main challenge of accurate computations. To significantly reduce the number of configurations, we further developed a method suggested in Ref. [^4] to predict important configurations based on a set of configurations with known weights. This method can be used to optimize the CI space by identifying the most important configurations from a list of CI configurations using perturbation theory (PT). All second-order corrections are taken into account and added to the energy calculated from CI to obtain the total energy, E^\\text{CI}=E_0+E_1 , while first-order corrections to the wave functions are stored for use in subsequent CI calculations. This process of using CI on a small subspace, calculating corrections via PT, and reordering the list of configurations in descending weights can be repeated several times to form the most optimal CI subspace. Once the energy differences between subsequent CI calculations are relatively small, it can be assumed that convergence has been met. We've developed a new parallel program conf_pt that realizes the CI+PT method. The parallel version enables computations of extremely large problems, with tests running up to 400 million determinants.","title":"Valence Perturbation Theory"},{"location":"theory/#references","text":"[^1]: V. A. Dzuba, V. V. Flambaum, and M. G. Kozlov. Combination of the many-body perturbation theory with the configuration-interaction method. Phys. Rev. A , 54 5 :3948\u20133959, November 1996. [^2]: M. G. Kozlov, S. G. Porsev, M. S. Safronova, and I. I. Tupitsyn. CI-MBPT: A package of programs for relativistic atomic calculations based on a method combining configuration interaction and many-body perturbation theory. Computer Physics Communications , 195:199\u2013213, 2015. [^3]: Ernest R. Davidson. The iterative calculation of a few of the lowest eigenvalues and corresponding eigenvectors of large real-symmetric matrices. Journal of Computational Physics , 17 1 :87\u201394, January 1975. [^4]: Yu. G. Rakhlina, M. G. Kozlov, and S. G. Porsev. The energy of electron affinity to a zirconium atom. Optics and Spectroscopy , 90 6 :817\u2013821, June 2001.","title":"References"},{"location":"ud_instructions/","text":"Working at UD The University of Delaware currently houses and maintains three community clusters. Cluster Processor Farber 10-core 2.5 GHz Intel E5-2670 v2 (\"Ivy Bridge\") Caviness 18-core 2.10 GHz Intel E5-2695 v4 (\"Broadwell\") DARWIN 32-core AMD EPYC 7002 Series Processors More information about the community clusters can be found here . Compiling at UD The computers at UD utilize the VALET system for installing software. We load the cmake and openmpi modules into your environment using the command vpkg_require . $ vpkg_require intel Adding package `intel/2018u4` to your environment To see what packages have been added to your environment, you can use the vpkg_history command. $ vpkg_history [standard] intel/2018u4 To remove the changes produced by vpkg_require , you can use the vpkg_rollback command. $ vpkg_rollback The parallel codes are built using the CMakeLists.txt file. A Debug build can be done: $ ls pCI $ cd pCI $ mkdir build-debug $ cd build-debug $ vpkg_require cmake openmpi/4.1.0:intel-2020 $ FC=mpifort cmake -DCMAKE_BUILD_TYPE=Debug -DCMAKE_INSTALL_PREFIX=$(pwd)/../20200402-debug .. : $ make $ make install An optimized build demands a little more: $ cd .. $ mkdir build-opt $ cd build-opt $ FC=mpifort cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=$(pwd)/../20200317-opt -DCMAKE_Fortran_FLAGS_RELEASE=\"-g -O3 -mcmodel=large -xHost -m64\" .. : $ make $ make install Note Each community cluster has their own versions of cmake and openmpi. To see what versions and variants of a package are available on the cluster, use the vpkg_versions command. Running Jobs at UD Job scripts It is strongly recommended to use a job script file patterned after the prototypes in /opt/templates/ . There are README.md files in each subdirectory to explain the use of the templates. The following job scripts are valid for usage only in the Caviness and DARWIN community clusters. Serial job script An serial job script template with full descriptions of capabilities is given here. serial.qs : #!/bin/bash -l # # Sections of this script that can/should be edited are delimited by a # [EDIT] tag. All Slurm job options are denoted by a line that starts # with \"#SBATCH \" followed by flags that would otherwise be passed on # the command line. Slurm job options can easily be disabled in a # script by inserting a space in the prefix, e.g. \"# SLURM \" and # reenabled by deleting that space. # # This is a batch job template for a program using a single processor # core/thread (a serial job). # #SBATCH --ntasks=1 # # [EDIT] All jobs have memory limits imposed. The default is 1 GB per # CPU allocated to the job. The default can be overridden either # with a per-node value (--mem) or a per-CPU value (--mem-per-cpu) # with unitless values in MB and the suffixes K|M|G|T denoting # kibi, mebi, gibi, and tebibyte units. Delete the space between # the \"#\" and the word SBATCH to enable one of them: # # SBATCH --mem=8G # SBATCH --mem-per-cpu=1024M # # [EDIT] Each node in the cluster has local scratch disk of some sort # that is always mounted as /tmp. Per-job and per-step temporary # directories are automatically created and destroyed by the # auto_tmpdir plugin in the /tmp filesystem. To ensure a minimum # amount of free space on /tmp when your job is scheduled, the # --tmp option can be used; it has the same behavior unit-wise as # --mem and --mem-per-cpu. Delete the space between the \"#\" and the # word SBATCH to enable: # # SBATCH --tmp=24G # # [EDIT] It can be helpful to provide a descriptive (terse) name for # the job (be sure to use quotes if there's whitespace in the # name): # #SBATCH --job-name=serial_job # # [EDIT] The partition determines which nodes can be used and with what # maximum runtime limits, etc. Partition limits can be displayed # with the \"sinfo --summarize\" command. # # SBATCH --partition=standard # # To run with priority-access to resources owned by your workgroup, # use the \"_workgroup_\" partition: # # SBATCH --partition=_workgroup_ # # [EDIT] The maximum runtime for the job; a single integer is interpreted # as a number of minutes, otherwise use the format # # d-hh:mm:ss # # Jobs default to the default runtime limit of the chosen partition # if this option is omitted. # #SBATCH --time=0-02:00:00 # # You can also provide a minimum acceptable runtime so the scheduler # may be able to run your job sooner. If you do not provide a # value, it will be set to match the maximum runtime limit (discussed # above). # # SBATCH --time-min=0-01:00:00 # # [EDIT] By default SLURM sends the job's stdout to the file \"slurm-<jobid>.out\" # and the job's stderr to the file \"slurm-<jobid>.err\" in the working # directory. Override by deleting the space between the \"#\" and the # word SBATCH on the following lines; see the man page for sbatch for # special tokens that can be used in the filenames: # # SBATCH --output=%x-%j.out # SBATCH --error=%x-%j.out # # [EDIT] Slurm can send emails to you when a job transitions through various # states: NONE, BEGIN, END, FAIL, REQUEUE, ALL, TIME_LIMIT, # TIME_LIMIT_50, TIME_LIMIT_80, TIME_LIMIT_90, ARRAY_TASKS. One or more # of these flags (separated by commas) are permissible for the # --mail-type flag. You MUST set your mail address using --mail-user # for messages to get off the cluster. # # SBATCH --mail-user='my_address@udel.edu' # SBATCH --mail-type=END,FAIL,TIME_LIMIT_90 # # [EDIT] By default we DO NOT want to send the job submission environment # to the compute node when the job runs. # #SBATCH --export=NONE # # # [EDIT] Define a Bash function and set this variable to its # name if you want to have the function called when the # job terminates (time limit reached or job preempted). # # PLEASE NOTE: when using a signal-handling Bash # function, any long-running commands should be prefixed # with UD_EXEC, e.g. # # UD_EXEC mpirun vasp # # If you do not use UD_EXEC, then the signals will not # get handled by the job shell! # #job_exit_handler() { # # Copy all our output files back to the original job directory: # cp * \"$SLURM_SUBMIT_DIR\" # # # Don't call again on EXIT signal, please: # trap - EXIT # exit 0 #} #export UD_JOB_EXIT_FN=job_exit_handler # # [EDIT] By default, the function defined above is registered # to respond to the SIGTERM signal that Slurm sends # when jobs reach their runtime limit or are # preempted. You can override with your own list of # signals using this variable -- as in this example, # which registers for both SIGTERM and the EXIT # pseudo-signal that Bash sends when the script ends. # In effect, no matter whether the job is terminated # or completes, the UD_JOB_EXIT_FN will be called. # #export UD_JOB_EXIT_FN_SIGNALS=\"SIGTERM EXIT\" # # If you have VALET packages to load into the job environment, # uncomment and edit the following line: # #vpkg_require intel/2019 # # Do general job environment setup: # . /opt/shared/slurm/templates/libexec/common.sh # # [EDIT] Add your script statements hereafter, or execute a script or program # using the srun command. # srun date Once the job script has been set up, you can submit the job using the sbatch command: sbatch serial.qs Parallel job script An openmpi job script template with full descriptions of capabilities is given here. openmpi.qs : #!/bin/bash -l # # Sections of this script that can/should be edited are delimited by a # [EDIT] tag. All Slurm job options are denoted by a line that starts # with \"#SBATCH \" followed by flags that would otherwise be passed on # the command line. Slurm job options can easily be disabled in a # script by inserting a space in the prefix, e.g. \"# SLURM \" and # reenabled by deleting that space. # # This is a batch job template for a program using multiple processor # cores/threads on one or more nodes. This particular variant should # be used with Open MPI or another MPI library that is tightly- # integrated with Slurm. # # [EDIT] There are several ways to communicate the number and layout # of worker processes. Under GridEngine, the only option was # to request a number of slots and GridEngine would spread the # slots across an arbitrary number of nodes (not necessarily # with a common number of worker per node, either). This method # is still permissible under Slurm by providing ONLY the # --ntasks option: # # #SBATCH --ntasks=<nproc> # # To limit the number of nodes used to satisfy the distribution # of <nproc> workers, the --nodes option can be used in addition # to --ntasks: # # #SBATCH --nodes=<nhosts> # #SBATCH --ntasks=<nproc> # # in which case, <nproc> workers will be allocated to <nhosts> # nodes in round-robin fashion. # # For a uniform distribution of workers the --tasks-per-node # option should be used with the --nodes option: # # #SBATCH --nodes=<nhosts> # #SBATCH --tasks-per-node=<nproc-per-node> # # The --ntasks option can be omitted in this case and will be # implicitly equal to <nhosts> * <nproc-per-node>. # # Given the above information, set the options you want to use # and add a space between the \"#\" and the word SBATCH for the ones # you don't want to use. # #SBATCH --nodes=<nhosts> #SBATCH --ntasks=<nproc> #SBATCH --tasks-per-node=<nproc-per-node> # # [EDIT] Normally, each MPI worker will not be multithreaded; if each # worker allows thread parallelism, then alter this value to # reflect how many threads each worker process will spawn. # #SBATCH --cpus-per-task=1 # # [EDIT] All jobs have memory limits imposed. The default is 1 GB per # CPU allocated to the job. The default can be overridden either # with a per-node value (--mem) or a per-CPU value (--mem-per-cpu) # with unitless values in MB and the suffixes K|M|G|T denoting # kibi, mebi, gibi, and tebibyte units. Delete the space between # the \"#\" and the word SBATCH to enable one of them: # # SBATCH --mem=8G # SBATCH --mem-per-cpu=1024M # # [EDIT] Each node in the cluster has local scratch disk of some sort # that is always mounted as /tmp. Per-job and per-step temporary # directories are automatically created and destroyed by the # auto_tmpdir plugin in the /tmp filesystem. To ensure a minimum # amount of free space on /tmp when your job is scheduled, the # --tmp option can be used; it has the same behavior unit-wise as # --mem and --mem-per-cpu. Delete the space between the \"#\" and the # word SBATCH to enable: # # SBATCH --tmp=24G # # [EDIT] It can be helpful to provide a descriptive (terse) name for # the job (be sure to use quotes if there's whitespace in the # name): # #SBATCH --job-name=openmpi_job # # [EDIT] The partition determines which nodes can be used and with what # maximum runtime limits, etc. Partition limits can be displayed # with the \"sinfo --summarize\" command. # # SBATCH --partition=standard # # To run with priority-access to resources owned by your workgroup, # use the \"_workgroup_\" partition: # # SBATCH --partition=_workgroup_ # # [EDIT] The maximum runtime for the job; a single integer is interpreted # as a number of minutes, otherwise use the format # # d-hh:mm:ss # # Jobs default to the default runtime limit of the chosen partition # if this option is omitted. # #SBATCH --time=0-02:00:00 # # You can also provide a minimum acceptable runtime so the scheduler # may be able to run your job sooner. If you do not provide a # value, it will be set to match the maximum runtime limit (discussed # above). # # SBATCH --time-min=0-01:00:00 # # [EDIT] By default SLURM sends the job's stdout to the file \"slurm-<jobid>.out\" # and the job's stderr to the file \"slurm-<jobid>.err\" in the working # directory. Override by deleting the space between the \"#\" and the # word SBATCH on the following lines; see the man page for sbatch for # special tokens that can be used in the filenames: # # SBATCH --output=%x-%j.out # SBATCH --error=%x-%j.out # # [EDIT] Slurm can send emails to you when a job transitions through various # states: NONE, BEGIN, END, FAIL, REQUEUE, ALL, TIME_LIMIT, # TIME_LIMIT_50, TIME_LIMIT_80, TIME_LIMIT_90, ARRAY_TASKS. One or more # of these flags (separated by commas) are permissible for the # --mail-type flag. You MUST set your mail address using --mail-user # for messages to get off the cluster. # # SBATCH --mail-user='my_address@udel.edu' # SBATCH --mail-type=END,FAIL,TIME_LIMIT_90 # # [EDIT] By default we DO NOT want to send the job submission environment # to the compute node when the job runs. # #SBATCH --export=NONE # # # [EDIT] Do any pre-processing, staging, environment setup with VALET # or explicit changes to PATH, LD_LIBRARY_PATH, etc. # vpkg_require openmpi/default # # [EDIT] If you're not interested in how the job environment gets setup, # uncomment the following. # #UD_QUIET_JOB_SETUP=YES # # [EDIT] Slurm has a specific MPI-launch mechanism in srun that can speed-up # the startup of jobs with large node/worker counts. Uncomment this # line if you want to use that in lieu of mpirun. # #UD_USE_SRUN_LAUNCHER=YES # # [EDIT] By default each MPI worker process will be bound to a core/thread # for better efficiency. Uncomment this to NOT affect such binding. # #UD_DISABLE_CPU_AFFINITY=YES # # [EDIT] MPI ranks are distributed <nodename>(<rank>:<socket>.<core>,..) # # CORE sequentially to all allocated cores on each allocated node in # the sequence they occur in SLURM_NODELIST (this is the default) # # -N2 -n4 => n000(0:0.0,1:0.1,2:0.2,3:0.3); n001(4:0.0,5:0.1,6:0.2,7:0.3) # # NODE round-robin across the nodes allocated to the job in the sequence # they occur in SLURM_NODELIST # # -N2 -n4 => n000(0:0.0,2:0.1,4:0.2,6:0.3); n001(1:0.0,3:0.1,5:0.2,7:0.3) # # SOCKET round-robin across the allocated sockets on each allocated node # in the sequence they occur in SLURM_NODELIST # # -N2 -n4 => n000(0:0.0,2:0.1,4:1.0,6:1.1); n001(1:0.0,3:0.1,5:1.0,7:1.1) # # PLEASE NOTE: socket mode requires use of the --exclusive flag # to ensure uniform allocation of cores across sockets! # #UD_MPI_RANK_DISTRIB_BY=CORE # # [EDIT] By default all MPI byte transfers are limited to NOT use any # TCP interfaces on the system. Setting this variable will force # the job to NOT use any Infiniband interfaces. # #UD_DISABLE_IB_INTERFACES=YES # # [EDIT] Should Open MPI display LOTS of debugging information as the job # executes? Uncomment to enable. # #UD_SHOW_MPI_DEBUGGING=YES # # [EDIT] Define a Bash function and set this variable to its # name if you want to have the function called when the # job terminates (time limit reached or job preempted). # # PLEASE NOTE: when using a signal-handling Bash # function, any long-running commands should be prefixed # with UD_EXEC, e.g. # # UD_EXEC mpirun vasp # # If you do not use UD_EXEC, then the signals will not # get handled by the job shell! # #job_exit_handler() { # # Copy all our output files back to the original job directory: # cp * \"$SLURM_SUBMIT_DIR\" # # # Don't call again on EXIT signal, please: # trap - EXIT # exit 0 #} #export UD_JOB_EXIT_FN=job_exit_handler # # [EDIT] By default, the function defined above is registered # to respond to the SIGTERM signal that Slurm sends # when jobs reach their runtime limit or are # preempted. You can override with your own list of # signals using this variable -- as in this example, # which registers for both SIGTERM and the EXIT # pseudo-signal that Bash sends when the script ends. # In effect, no matter whether the job is terminated # or completes, the UD_JOB_EXIT_FN will be called. # #export UD_JOB_EXIT_FN_SIGNALS=\"SIGTERM EXIT\" # # Do standard Open MPI environment setup (networks, etc.) # . /opt/shared/slurm/templates/libexec/openmpi.sh # # [EDIT] Execute your MPI program # ${UD_MPIRUN} ./my_mpi_program arg1 \"arg2 has spaces\" arg3 mpi_rc=$? # # [EDIT] Do any cleanup work here... # # # Be sure to return the mpirun's result code: # exit $mpi_rc Once the job script has been set up, you can submit the job using the sbatch command: sbatch openmpi.qs Managing Jobs at UD Once the job has been submitted, you can monitor the status of your job using the squeue command: squeue -u <username> squeue -p <partition_name> You can also continuously monitor your job by using the watch command: watch squeue -u <username> watch squeue -p <partition_name> To see the amount of resources available for your job, you can run the squota command: squota To cancel your job, you can run the scancel command: scancel <job-id>","title":"Working at UD"},{"location":"ud_instructions/#working-at-ud","text":"The University of Delaware currently houses and maintains three community clusters. Cluster Processor Farber 10-core 2.5 GHz Intel E5-2670 v2 (\"Ivy Bridge\") Caviness 18-core 2.10 GHz Intel E5-2695 v4 (\"Broadwell\") DARWIN 32-core AMD EPYC 7002 Series Processors More information about the community clusters can be found here .","title":"Working at UD"},{"location":"ud_instructions/#compiling-at-ud","text":"The computers at UD utilize the VALET system for installing software. We load the cmake and openmpi modules into your environment using the command vpkg_require . $ vpkg_require intel Adding package `intel/2018u4` to your environment To see what packages have been added to your environment, you can use the vpkg_history command. $ vpkg_history [standard] intel/2018u4 To remove the changes produced by vpkg_require , you can use the vpkg_rollback command. $ vpkg_rollback The parallel codes are built using the CMakeLists.txt file. A Debug build can be done: $ ls pCI $ cd pCI $ mkdir build-debug $ cd build-debug $ vpkg_require cmake openmpi/4.1.0:intel-2020 $ FC=mpifort cmake -DCMAKE_BUILD_TYPE=Debug -DCMAKE_INSTALL_PREFIX=$(pwd)/../20200402-debug .. : $ make $ make install An optimized build demands a little more: $ cd .. $ mkdir build-opt $ cd build-opt $ FC=mpifort cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=$(pwd)/../20200317-opt -DCMAKE_Fortran_FLAGS_RELEASE=\"-g -O3 -mcmodel=large -xHost -m64\" .. : $ make $ make install Note Each community cluster has their own versions of cmake and openmpi. To see what versions and variants of a package are available on the cluster, use the vpkg_versions command.","title":"Compiling at UD"},{"location":"ud_instructions/#running-jobs-at-ud","text":"","title":"Running Jobs at UD"},{"location":"ud_instructions/#job-scripts","text":"It is strongly recommended to use a job script file patterned after the prototypes in /opt/templates/ . There are README.md files in each subdirectory to explain the use of the templates. The following job scripts are valid for usage only in the Caviness and DARWIN community clusters.","title":"Job scripts"},{"location":"ud_instructions/#serial-job-script","text":"An serial job script template with full descriptions of capabilities is given here. serial.qs : #!/bin/bash -l # # Sections of this script that can/should be edited are delimited by a # [EDIT] tag. All Slurm job options are denoted by a line that starts # with \"#SBATCH \" followed by flags that would otherwise be passed on # the command line. Slurm job options can easily be disabled in a # script by inserting a space in the prefix, e.g. \"# SLURM \" and # reenabled by deleting that space. # # This is a batch job template for a program using a single processor # core/thread (a serial job). # #SBATCH --ntasks=1 # # [EDIT] All jobs have memory limits imposed. The default is 1 GB per # CPU allocated to the job. The default can be overridden either # with a per-node value (--mem) or a per-CPU value (--mem-per-cpu) # with unitless values in MB and the suffixes K|M|G|T denoting # kibi, mebi, gibi, and tebibyte units. Delete the space between # the \"#\" and the word SBATCH to enable one of them: # # SBATCH --mem=8G # SBATCH --mem-per-cpu=1024M # # [EDIT] Each node in the cluster has local scratch disk of some sort # that is always mounted as /tmp. Per-job and per-step temporary # directories are automatically created and destroyed by the # auto_tmpdir plugin in the /tmp filesystem. To ensure a minimum # amount of free space on /tmp when your job is scheduled, the # --tmp option can be used; it has the same behavior unit-wise as # --mem and --mem-per-cpu. Delete the space between the \"#\" and the # word SBATCH to enable: # # SBATCH --tmp=24G # # [EDIT] It can be helpful to provide a descriptive (terse) name for # the job (be sure to use quotes if there's whitespace in the # name): # #SBATCH --job-name=serial_job # # [EDIT] The partition determines which nodes can be used and with what # maximum runtime limits, etc. Partition limits can be displayed # with the \"sinfo --summarize\" command. # # SBATCH --partition=standard # # To run with priority-access to resources owned by your workgroup, # use the \"_workgroup_\" partition: # # SBATCH --partition=_workgroup_ # # [EDIT] The maximum runtime for the job; a single integer is interpreted # as a number of minutes, otherwise use the format # # d-hh:mm:ss # # Jobs default to the default runtime limit of the chosen partition # if this option is omitted. # #SBATCH --time=0-02:00:00 # # You can also provide a minimum acceptable runtime so the scheduler # may be able to run your job sooner. If you do not provide a # value, it will be set to match the maximum runtime limit (discussed # above). # # SBATCH --time-min=0-01:00:00 # # [EDIT] By default SLURM sends the job's stdout to the file \"slurm-<jobid>.out\" # and the job's stderr to the file \"slurm-<jobid>.err\" in the working # directory. Override by deleting the space between the \"#\" and the # word SBATCH on the following lines; see the man page for sbatch for # special tokens that can be used in the filenames: # # SBATCH --output=%x-%j.out # SBATCH --error=%x-%j.out # # [EDIT] Slurm can send emails to you when a job transitions through various # states: NONE, BEGIN, END, FAIL, REQUEUE, ALL, TIME_LIMIT, # TIME_LIMIT_50, TIME_LIMIT_80, TIME_LIMIT_90, ARRAY_TASKS. One or more # of these flags (separated by commas) are permissible for the # --mail-type flag. You MUST set your mail address using --mail-user # for messages to get off the cluster. # # SBATCH --mail-user='my_address@udel.edu' # SBATCH --mail-type=END,FAIL,TIME_LIMIT_90 # # [EDIT] By default we DO NOT want to send the job submission environment # to the compute node when the job runs. # #SBATCH --export=NONE # # # [EDIT] Define a Bash function and set this variable to its # name if you want to have the function called when the # job terminates (time limit reached or job preempted). # # PLEASE NOTE: when using a signal-handling Bash # function, any long-running commands should be prefixed # with UD_EXEC, e.g. # # UD_EXEC mpirun vasp # # If you do not use UD_EXEC, then the signals will not # get handled by the job shell! # #job_exit_handler() { # # Copy all our output files back to the original job directory: # cp * \"$SLURM_SUBMIT_DIR\" # # # Don't call again on EXIT signal, please: # trap - EXIT # exit 0 #} #export UD_JOB_EXIT_FN=job_exit_handler # # [EDIT] By default, the function defined above is registered # to respond to the SIGTERM signal that Slurm sends # when jobs reach their runtime limit or are # preempted. You can override with your own list of # signals using this variable -- as in this example, # which registers for both SIGTERM and the EXIT # pseudo-signal that Bash sends when the script ends. # In effect, no matter whether the job is terminated # or completes, the UD_JOB_EXIT_FN will be called. # #export UD_JOB_EXIT_FN_SIGNALS=\"SIGTERM EXIT\" # # If you have VALET packages to load into the job environment, # uncomment and edit the following line: # #vpkg_require intel/2019 # # Do general job environment setup: # . /opt/shared/slurm/templates/libexec/common.sh # # [EDIT] Add your script statements hereafter, or execute a script or program # using the srun command. # srun date Once the job script has been set up, you can submit the job using the sbatch command: sbatch serial.qs","title":"Serial job script"},{"location":"ud_instructions/#parallel-job-script","text":"An openmpi job script template with full descriptions of capabilities is given here. openmpi.qs : #!/bin/bash -l # # Sections of this script that can/should be edited are delimited by a # [EDIT] tag. All Slurm job options are denoted by a line that starts # with \"#SBATCH \" followed by flags that would otherwise be passed on # the command line. Slurm job options can easily be disabled in a # script by inserting a space in the prefix, e.g. \"# SLURM \" and # reenabled by deleting that space. # # This is a batch job template for a program using multiple processor # cores/threads on one or more nodes. This particular variant should # be used with Open MPI or another MPI library that is tightly- # integrated with Slurm. # # [EDIT] There are several ways to communicate the number and layout # of worker processes. Under GridEngine, the only option was # to request a number of slots and GridEngine would spread the # slots across an arbitrary number of nodes (not necessarily # with a common number of worker per node, either). This method # is still permissible under Slurm by providing ONLY the # --ntasks option: # # #SBATCH --ntasks=<nproc> # # To limit the number of nodes used to satisfy the distribution # of <nproc> workers, the --nodes option can be used in addition # to --ntasks: # # #SBATCH --nodes=<nhosts> # #SBATCH --ntasks=<nproc> # # in which case, <nproc> workers will be allocated to <nhosts> # nodes in round-robin fashion. # # For a uniform distribution of workers the --tasks-per-node # option should be used with the --nodes option: # # #SBATCH --nodes=<nhosts> # #SBATCH --tasks-per-node=<nproc-per-node> # # The --ntasks option can be omitted in this case and will be # implicitly equal to <nhosts> * <nproc-per-node>. # # Given the above information, set the options you want to use # and add a space between the \"#\" and the word SBATCH for the ones # you don't want to use. # #SBATCH --nodes=<nhosts> #SBATCH --ntasks=<nproc> #SBATCH --tasks-per-node=<nproc-per-node> # # [EDIT] Normally, each MPI worker will not be multithreaded; if each # worker allows thread parallelism, then alter this value to # reflect how many threads each worker process will spawn. # #SBATCH --cpus-per-task=1 # # [EDIT] All jobs have memory limits imposed. The default is 1 GB per # CPU allocated to the job. The default can be overridden either # with a per-node value (--mem) or a per-CPU value (--mem-per-cpu) # with unitless values in MB and the suffixes K|M|G|T denoting # kibi, mebi, gibi, and tebibyte units. Delete the space between # the \"#\" and the word SBATCH to enable one of them: # # SBATCH --mem=8G # SBATCH --mem-per-cpu=1024M # # [EDIT] Each node in the cluster has local scratch disk of some sort # that is always mounted as /tmp. Per-job and per-step temporary # directories are automatically created and destroyed by the # auto_tmpdir plugin in the /tmp filesystem. To ensure a minimum # amount of free space on /tmp when your job is scheduled, the # --tmp option can be used; it has the same behavior unit-wise as # --mem and --mem-per-cpu. Delete the space between the \"#\" and the # word SBATCH to enable: # # SBATCH --tmp=24G # # [EDIT] It can be helpful to provide a descriptive (terse) name for # the job (be sure to use quotes if there's whitespace in the # name): # #SBATCH --job-name=openmpi_job # # [EDIT] The partition determines which nodes can be used and with what # maximum runtime limits, etc. Partition limits can be displayed # with the \"sinfo --summarize\" command. # # SBATCH --partition=standard # # To run with priority-access to resources owned by your workgroup, # use the \"_workgroup_\" partition: # # SBATCH --partition=_workgroup_ # # [EDIT] The maximum runtime for the job; a single integer is interpreted # as a number of minutes, otherwise use the format # # d-hh:mm:ss # # Jobs default to the default runtime limit of the chosen partition # if this option is omitted. # #SBATCH --time=0-02:00:00 # # You can also provide a minimum acceptable runtime so the scheduler # may be able to run your job sooner. If you do not provide a # value, it will be set to match the maximum runtime limit (discussed # above). # # SBATCH --time-min=0-01:00:00 # # [EDIT] By default SLURM sends the job's stdout to the file \"slurm-<jobid>.out\" # and the job's stderr to the file \"slurm-<jobid>.err\" in the working # directory. Override by deleting the space between the \"#\" and the # word SBATCH on the following lines; see the man page for sbatch for # special tokens that can be used in the filenames: # # SBATCH --output=%x-%j.out # SBATCH --error=%x-%j.out # # [EDIT] Slurm can send emails to you when a job transitions through various # states: NONE, BEGIN, END, FAIL, REQUEUE, ALL, TIME_LIMIT, # TIME_LIMIT_50, TIME_LIMIT_80, TIME_LIMIT_90, ARRAY_TASKS. One or more # of these flags (separated by commas) are permissible for the # --mail-type flag. You MUST set your mail address using --mail-user # for messages to get off the cluster. # # SBATCH --mail-user='my_address@udel.edu' # SBATCH --mail-type=END,FAIL,TIME_LIMIT_90 # # [EDIT] By default we DO NOT want to send the job submission environment # to the compute node when the job runs. # #SBATCH --export=NONE # # # [EDIT] Do any pre-processing, staging, environment setup with VALET # or explicit changes to PATH, LD_LIBRARY_PATH, etc. # vpkg_require openmpi/default # # [EDIT] If you're not interested in how the job environment gets setup, # uncomment the following. # #UD_QUIET_JOB_SETUP=YES # # [EDIT] Slurm has a specific MPI-launch mechanism in srun that can speed-up # the startup of jobs with large node/worker counts. Uncomment this # line if you want to use that in lieu of mpirun. # #UD_USE_SRUN_LAUNCHER=YES # # [EDIT] By default each MPI worker process will be bound to a core/thread # for better efficiency. Uncomment this to NOT affect such binding. # #UD_DISABLE_CPU_AFFINITY=YES # # [EDIT] MPI ranks are distributed <nodename>(<rank>:<socket>.<core>,..) # # CORE sequentially to all allocated cores on each allocated node in # the sequence they occur in SLURM_NODELIST (this is the default) # # -N2 -n4 => n000(0:0.0,1:0.1,2:0.2,3:0.3); n001(4:0.0,5:0.1,6:0.2,7:0.3) # # NODE round-robin across the nodes allocated to the job in the sequence # they occur in SLURM_NODELIST # # -N2 -n4 => n000(0:0.0,2:0.1,4:0.2,6:0.3); n001(1:0.0,3:0.1,5:0.2,7:0.3) # # SOCKET round-robin across the allocated sockets on each allocated node # in the sequence they occur in SLURM_NODELIST # # -N2 -n4 => n000(0:0.0,2:0.1,4:1.0,6:1.1); n001(1:0.0,3:0.1,5:1.0,7:1.1) # # PLEASE NOTE: socket mode requires use of the --exclusive flag # to ensure uniform allocation of cores across sockets! # #UD_MPI_RANK_DISTRIB_BY=CORE # # [EDIT] By default all MPI byte transfers are limited to NOT use any # TCP interfaces on the system. Setting this variable will force # the job to NOT use any Infiniband interfaces. # #UD_DISABLE_IB_INTERFACES=YES # # [EDIT] Should Open MPI display LOTS of debugging information as the job # executes? Uncomment to enable. # #UD_SHOW_MPI_DEBUGGING=YES # # [EDIT] Define a Bash function and set this variable to its # name if you want to have the function called when the # job terminates (time limit reached or job preempted). # # PLEASE NOTE: when using a signal-handling Bash # function, any long-running commands should be prefixed # with UD_EXEC, e.g. # # UD_EXEC mpirun vasp # # If you do not use UD_EXEC, then the signals will not # get handled by the job shell! # #job_exit_handler() { # # Copy all our output files back to the original job directory: # cp * \"$SLURM_SUBMIT_DIR\" # # # Don't call again on EXIT signal, please: # trap - EXIT # exit 0 #} #export UD_JOB_EXIT_FN=job_exit_handler # # [EDIT] By default, the function defined above is registered # to respond to the SIGTERM signal that Slurm sends # when jobs reach their runtime limit or are # preempted. You can override with your own list of # signals using this variable -- as in this example, # which registers for both SIGTERM and the EXIT # pseudo-signal that Bash sends when the script ends. # In effect, no matter whether the job is terminated # or completes, the UD_JOB_EXIT_FN will be called. # #export UD_JOB_EXIT_FN_SIGNALS=\"SIGTERM EXIT\" # # Do standard Open MPI environment setup (networks, etc.) # . /opt/shared/slurm/templates/libexec/openmpi.sh # # [EDIT] Execute your MPI program # ${UD_MPIRUN} ./my_mpi_program arg1 \"arg2 has spaces\" arg3 mpi_rc=$? # # [EDIT] Do any cleanup work here... # # # Be sure to return the mpirun's result code: # exit $mpi_rc Once the job script has been set up, you can submit the job using the sbatch command: sbatch openmpi.qs","title":"Parallel job script"},{"location":"ud_instructions/#managing-jobs-at-ud","text":"Once the job has been submitted, you can monitor the status of your job using the squeue command: squeue -u <username> squeue -p <partition_name> You can also continuously monitor your job by using the watch command: watch squeue -u <username> watch squeue -p <partition_name> To see the amount of resources available for your job, you can run the squota command: squota To cancel your job, you can run the scancel command: scancel <job-id>","title":"Managing Jobs at UD"}]}