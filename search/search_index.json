{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to pCI's documentation pCI is a high-precision relativistic atomic structure code package based on the configuration interaction (CI) method and methods combining CI with many-body perturbation theory and/or the all-order coupled-cluster method. The serial version of the CI+MBPT code package was modified for public use and published in Computer Physics Communications in 2015 by M. Kozlov et al.","title":"Introduction"},{"location":"#welcome-to-pcis-documentation","text":"pCI is a high-precision relativistic atomic structure code package based on the configuration interaction (CI) method and methods combining CI with many-body perturbation theory and/or the all-order coupled-cluster method. The serial version of the CI+MBPT code package was modified for public use and published in Computer Physics Communications in 2015 by M. Kozlov et al.","title":"Welcome to pCI's documentation"},{"location":"about/","text":"About Us pCI Development Team Marianna Safronova Department of Physics and Astronomy, University of Delaware Joint Quantum Institute, NIST and the University of Maryland Website : https://mariannasafronova.com Email : msafrono@udel.edu Charles Cheung Department of Physics and Astronomy, University of Delaware Email : ccheung@udel.edu Sergey Porsev Department of Physics and Astronomy, University of Delaware Petersburg Nuclear Physics Institute of NRC \"Kurchatov Institute\", Russia Mikhail Kozlov Petersburg Nuclear Physics Institute of NRC \"Kurchatov Institute\", Russia St. Petersburg Electrotechnical University \u201cLETI\u201d, Russia Ilya Tupitsyn Department of Physics, St. Petersburg State University, Russia Andrey Bondarev Peter the Great St. Petersburg Polytechnic University, Russia","title":"About the Team"},{"location":"about/#about-us","text":"","title":"About Us"},{"location":"about/#pci-development-team","text":"Marianna Safronova Department of Physics and Astronomy, University of Delaware Joint Quantum Institute, NIST and the University of Maryland Website : https://mariannasafronova.com Email : msafrono@udel.edu Charles Cheung Department of Physics and Astronomy, University of Delaware Email : ccheung@udel.edu Sergey Porsev Department of Physics and Astronomy, University of Delaware Petersburg Nuclear Physics Institute of NRC \"Kurchatov Institute\", Russia Mikhail Kozlov Petersburg Nuclear Physics Institute of NRC \"Kurchatov Institute\", Russia St. Petersburg Electrotechnical University \u201cLETI\u201d, Russia Ilya Tupitsyn Department of Physics, St. Petersburg State University, Russia Andrey Bondarev Peter the Great St. Petersburg Polytechnic University, Russia","title":"pCI Development Team"},{"location":"all-order/","text":"All-order/MBPT code package All-order package The all-order part of the package calculates corrections to the bare Hamiltonian due to the core shells for the CI code conf . The all-order label refers to inclusion of the large number of terms (second-, third-, fourth-order, etc.) in order-by-order many-body perturbation theory expansion using the iterative solutions until the sufficient numerical convergence is achieved. This code version implements a variant of the linearized single double coupled-cluster (CC) method. This CC version has been developed specifically for atoms fully utilizing atomic symmetries and capable of being efficiently ran with very large basis sets (over 1000 orbitals), reaching negligible numerical uncertainty associated with the choice of basis set. The all-order package consists of three codes: allcore-ci , valsd-ci , and sdvw-ci , which calculates core, core-valence and valence-valence excitations, respectively. These codes store resulting data in SGC.CON (small file) and SCRC.CON (up to a few GB file). The all-order package can be omitted if high precision is not required, leading to a method referred to as CI+MBPT. The following code executions read the files hfspl.1 and hfspl.2 and generate the files SGC.CON and SCRC.CON . It also takes in the input file inf.aov and writes the results to the respective out. files. ./allcore-rle-ci <inf.aov >out.core # computes Sigma_ma, Sigma_mnab -> writes pair.3 file ./valsd-rle-cis <inf.aov >out.val # computes Sigma_mv (and thus Sigma_1, the one-body correcton to the Hamiltonian), Sigma_mnva -> writes val2 and sigma files ./sdvw-rle-cis <inf.aov >out.sdvw # computes Sigma_mnvw (Sigma_2, the two-body correction to the Hamiltonian) -> writes pair.vw and sigma1 files Click here to see a description of inf.aov 12 # ncore - number of core shells 1 -1 # n and kappa of the core shells 2 -1 2 1 2 -2 3 -1 3 1 3 -2 3 2 3 -3 4 -1 4 1 4 -2 35 5 # nmax and lmax in correlation diagram summations 0 0 # internal parameters, do not change 30 # max number of iterations for core 2 7 1 # Stabilizer code parameters, see below 0.d0 # damping factor, not active not zero 1 # kval (key_en) - key for energies, see explanation below 24 # nval - number of valence orbitals in list to follow 5 -1 30 # n and kappa of the valence orbitals, max number of iteration 6 -1 30 7 -1 30 8 -1 30 5 1 30 6 1 30 7 1 30 8 1 30 5 -2 30 6 -2 30 7 -2 30 8 -2 30 4 2 30 5 2 30 6 2 30 7 2 30 4 -3 30 5 -3 30 6 -3 30 7 -3 30 4 3 30 5 3 30 4 -4 30 5 -4 30 ============================================================== Additional explanations: 2 7 1 # Stabilizer code parameters 2 - DIIS method (change to 1 for RLE method) 7 \u2013 number in iterations before stabilizer code runs 1 - do not change (controls the type of linear algebra code) Note It is important to always check all output files. In out.core and out.val , check that the core and valence orbitals have converged, respectively. If any cases completed 30 iterations (max) with values slowly drifting up, you can reduce the number of interactions to 3 or 5, and rerun the respective codes. Here, it's important to look at how much energies fluctuated during the first few iterations. In cases of severe divergence, check all inputs for errors. If nothing is found, and the atom/ion with closed d shell, but not p shell, set kval=2 . Note kval controls the values of \\tilde{\\epsilon}_v in denominators (see Phys. Rev. A 80, 012516 (2009) ) for formulas). kval=1 is the default choice, where \\tilde{\\epsilon}_v for all nl_j is set to the DHF energy of the lowest valence n for the particular partial wave. For example for Sr, \\tilde{\\epsilon}_v for v=ns is set with the DHF energy of the 5s state, v=np_{1/2} is set with the DHF energy of the 5p_{1/2} state, v=nd_{3/2} is set with the DHF energy of the 5d_{3/2} state, and so on. kval=2 is only used when the all-order valence energies are severely divergent. So far, this was observed with highly-charged ions with filling p -shell (e.g. Sn-like). By \"severe\", we mean that the energies begin to diverge after the first or second iteration, immediately driving the correlation energy to be very large. Such a divergence cannot be fixed by a stabilizer. In this case, we have to identify which partial waves diverge and manually change the energies for these orbitals - we set them to the lowest DHF energy of the partial wave for which the all-order converged. For instance, if s diverges, but p does not, then set the ns energies to the lowest np_{1/2} DHF energies. The rest are left as DHF as in kval=1 . The format would be as follows: 2 # kval 3 # lmax for the input to follow 0 -0.28000 # l=0 energies 1 -0.22000 -0.22000 # l=1 energies p1/2 p3/2 2 -0.31000 -0.31000 # l=2 energies d3/2 d5/2 3 -0.13000 -0.13000 # l=3 energies f5/2 f7/2 MBPT package The MBPT part of the package calculates corrections to the bare Hamiltonian due to the core shells using second order MBPT for the CI code conf , but for a much larger part of the Hamiltonian than the all-order code since high accuracy is not required for corrections associated with higher orbitals. The MBPT package consists of a single code second-cis , which can be omitted, but then conf will not include electronic correlations associated with any of the core shells. If the all-order calculation was carried out, it will overwrite the second-order results with the all-order results where available. Such overlay of the MBPT and the all-order parts drastically improves the efficiency of the method. The following code execution reads the files hfspl.1 , hfspl.2 and HFD.DAT (to read the list of orbitals), and writes the files SGC.CON and SCRC.CON used by conf . It also takes in the input file inf.vw and writes the results to the respective out.sdvw files. ./second-cis <inf.vw >out.second.vw # writes SGC.CON and SCRC.CON files Click here to see a description of inf.vw 12 # ncore - number of core shells 1 -1 # n and kappa of the core shells 2 -1 2 1 2 -2 3 -1 3 1 3 -2 3 2 3 -3 4 -1 4 1 4 -2 35 5 # nmax and lmax in correlation diagram summations 195 # Nmax (max_orb) # of orbitals (from BASS.INPT) for how many Sigmas to calculate 3 # Lmax (lvmax) for how many Sigmas to calculate, supersedes Nmax. Maybe best to set to 4 when 4f is important 9 # Kmax (kvmax) for higest partial wave for Sigma2 S_Kmax(ijkl). May increase for f shell cases 195 100 # nav1 nav - maximum average of orbitals to calculate Sigma for nav1=(i+j)/2 for Sigma(ij) and nav=(i+j+k+l)/4 for Sigma_K(ijkl) 0 # if just second order, 1 to read all-order input from PAIR.VW and sigma1 1 # kval, set the same as in the all-order, see explanations of inf.aov ============================================================== Additional explanations about Sigma_1 and Sigma_2 restrictions: (1) Put the first number (Sigma 1) to be the last orbital you plan to list in ADD.INP. Look up the number in BASS.INP and add the number of core shells. Here, the last orbital to be included in conf is 21d5/2, so the number is 195. Example: Last orbital in BASS.INP: 183 2.1201 3 2.1201 # Here, the last orbital to be included in conf is 21d5/2, so the number is 183+12 = 195 Note: this is inputted twice, keep it the same. (2) Set 100 as the second value. This was tested a while ago, can increase.","title":"All-order/MBPT code package"},{"location":"all-order/#all-ordermbpt-code-package","text":"","title":"All-order/MBPT code package"},{"location":"all-order/#all-order-package","text":"The all-order part of the package calculates corrections to the bare Hamiltonian due to the core shells for the CI code conf . The all-order label refers to inclusion of the large number of terms (second-, third-, fourth-order, etc.) in order-by-order many-body perturbation theory expansion using the iterative solutions until the sufficient numerical convergence is achieved. This code version implements a variant of the linearized single double coupled-cluster (CC) method. This CC version has been developed specifically for atoms fully utilizing atomic symmetries and capable of being efficiently ran with very large basis sets (over 1000 orbitals), reaching negligible numerical uncertainty associated with the choice of basis set. The all-order package consists of three codes: allcore-ci , valsd-ci , and sdvw-ci , which calculates core, core-valence and valence-valence excitations, respectively. These codes store resulting data in SGC.CON (small file) and SCRC.CON (up to a few GB file). The all-order package can be omitted if high precision is not required, leading to a method referred to as CI+MBPT. The following code executions read the files hfspl.1 and hfspl.2 and generate the files SGC.CON and SCRC.CON . It also takes in the input file inf.aov and writes the results to the respective out. files. ./allcore-rle-ci <inf.aov >out.core # computes Sigma_ma, Sigma_mnab -> writes pair.3 file ./valsd-rle-cis <inf.aov >out.val # computes Sigma_mv (and thus Sigma_1, the one-body correcton to the Hamiltonian), Sigma_mnva -> writes val2 and sigma files ./sdvw-rle-cis <inf.aov >out.sdvw # computes Sigma_mnvw (Sigma_2, the two-body correction to the Hamiltonian) -> writes pair.vw and sigma1 files Click here to see a description of inf.aov 12 # ncore - number of core shells 1 -1 # n and kappa of the core shells 2 -1 2 1 2 -2 3 -1 3 1 3 -2 3 2 3 -3 4 -1 4 1 4 -2 35 5 # nmax and lmax in correlation diagram summations 0 0 # internal parameters, do not change 30 # max number of iterations for core 2 7 1 # Stabilizer code parameters, see below 0.d0 # damping factor, not active not zero 1 # kval (key_en) - key for energies, see explanation below 24 # nval - number of valence orbitals in list to follow 5 -1 30 # n and kappa of the valence orbitals, max number of iteration 6 -1 30 7 -1 30 8 -1 30 5 1 30 6 1 30 7 1 30 8 1 30 5 -2 30 6 -2 30 7 -2 30 8 -2 30 4 2 30 5 2 30 6 2 30 7 2 30 4 -3 30 5 -3 30 6 -3 30 7 -3 30 4 3 30 5 3 30 4 -4 30 5 -4 30 ============================================================== Additional explanations: 2 7 1 # Stabilizer code parameters 2 - DIIS method (change to 1 for RLE method) 7 \u2013 number in iterations before stabilizer code runs 1 - do not change (controls the type of linear algebra code) Note It is important to always check all output files. In out.core and out.val , check that the core and valence orbitals have converged, respectively. If any cases completed 30 iterations (max) with values slowly drifting up, you can reduce the number of interactions to 3 or 5, and rerun the respective codes. Here, it's important to look at how much energies fluctuated during the first few iterations. In cases of severe divergence, check all inputs for errors. If nothing is found, and the atom/ion with closed d shell, but not p shell, set kval=2 . Note kval controls the values of \\tilde{\\epsilon}_v in denominators (see Phys. Rev. A 80, 012516 (2009) ) for formulas). kval=1 is the default choice, where \\tilde{\\epsilon}_v for all nl_j is set to the DHF energy of the lowest valence n for the particular partial wave. For example for Sr, \\tilde{\\epsilon}_v for v=ns is set with the DHF energy of the 5s state, v=np_{1/2} is set with the DHF energy of the 5p_{1/2} state, v=nd_{3/2} is set with the DHF energy of the 5d_{3/2} state, and so on. kval=2 is only used when the all-order valence energies are severely divergent. So far, this was observed with highly-charged ions with filling p -shell (e.g. Sn-like). By \"severe\", we mean that the energies begin to diverge after the first or second iteration, immediately driving the correlation energy to be very large. Such a divergence cannot be fixed by a stabilizer. In this case, we have to identify which partial waves diverge and manually change the energies for these orbitals - we set them to the lowest DHF energy of the partial wave for which the all-order converged. For instance, if s diverges, but p does not, then set the ns energies to the lowest np_{1/2} DHF energies. The rest are left as DHF as in kval=1 . The format would be as follows: 2 # kval 3 # lmax for the input to follow 0 -0.28000 # l=0 energies 1 -0.22000 -0.22000 # l=1 energies p1/2 p3/2 2 -0.31000 -0.31000 # l=2 energies d3/2 d5/2 3 -0.13000 -0.13000 # l=3 energies f5/2 f7/2","title":"All-order package"},{"location":"all-order/#mbpt-package","text":"The MBPT part of the package calculates corrections to the bare Hamiltonian due to the core shells using second order MBPT for the CI code conf , but for a much larger part of the Hamiltonian than the all-order code since high accuracy is not required for corrections associated with higher orbitals. The MBPT package consists of a single code second-cis , which can be omitted, but then conf will not include electronic correlations associated with any of the core shells. If the all-order calculation was carried out, it will overwrite the second-order results with the all-order results where available. Such overlay of the MBPT and the all-order parts drastically improves the efficiency of the method. The following code execution reads the files hfspl.1 , hfspl.2 and HFD.DAT (to read the list of orbitals), and writes the files SGC.CON and SCRC.CON used by conf . It also takes in the input file inf.vw and writes the results to the respective out.sdvw files. ./second-cis <inf.vw >out.second.vw # writes SGC.CON and SCRC.CON files Click here to see a description of inf.vw 12 # ncore - number of core shells 1 -1 # n and kappa of the core shells 2 -1 2 1 2 -2 3 -1 3 1 3 -2 3 2 3 -3 4 -1 4 1 4 -2 35 5 # nmax and lmax in correlation diagram summations 195 # Nmax (max_orb) # of orbitals (from BASS.INPT) for how many Sigmas to calculate 3 # Lmax (lvmax) for how many Sigmas to calculate, supersedes Nmax. Maybe best to set to 4 when 4f is important 9 # Kmax (kvmax) for higest partial wave for Sigma2 S_Kmax(ijkl). May increase for f shell cases 195 100 # nav1 nav - maximum average of orbitals to calculate Sigma for nav1=(i+j)/2 for Sigma(ij) and nav=(i+j+k+l)/4 for Sigma_K(ijkl) 0 # if just second order, 1 to read all-order input from PAIR.VW and sigma1 1 # kval, set the same as in the all-order, see explanations of inf.aov ============================================================== Additional explanations about Sigma_1 and Sigma_2 restrictions: (1) Put the first number (Sigma 1) to be the last orbital you plan to list in ADD.INP. Look up the number in BASS.INP and add the number of core shells. Here, the last orbital to be included in conf is 21d5/2, so the number is 195. Example: Last orbital in BASS.INP: 183 2.1201 3 2.1201 # Here, the last orbital to be included in conf is 21d5/2, so the number is 183+12 = 195 Note: this is inputted twice, keep it the same. (2) Set 100 as the second value. This was tested a while ago, can increase.","title":"MBPT package"},{"location":"applications/","text":"Applications","title":"Applications"},{"location":"applications/#applications","text":"","title":"Applications"},{"location":"auxiliary/","text":"Auxiliary scripts In this section, we describe supplementary python scripts used to automatically generate basis sets and configuration lists used for configuration interaction. It is also able to submit batch jobs to run the CI programs on the UD computers. Scripts that require the execution of a pCI program read a user-defined configuration file config.yml to set general parameters defining the atomic system of interest. config.yml The config.yml file is a YAML configuration file that defines important parameters of the atomic system of interest. This config file is divided into several sections: general parameters parameters used by basis programs (only read by basis.py) parameters used by add program (only read by add.py) parameters used by conf program (only read by add.py) optional parameters to automatically execute programs, include QED, or include isotope shift calculations (only read by basis.py) The general parameters are in the block system and is read by all python scripts: system: name: Ni12+ isotope: include_breit: True The name field takes the name of the atomic system of interest. The isotope field takes a specified isotope number. If left blank, the script will automatically find the atomic weight from nuclear radii table. The include_breit field takes in a boolean to set whether to include Breit corrections or not. basis.py The basis.py script automatically generates the basis set given information about the atomic system of interest via a configuration file config.yml . This script reads the basis block: basis: cavity_radius: 60 orbitals: core: 1s 2s 2p 3s 3p 3d 4s 4p 4d 4f 5s 5p 5d 6s 6p valence: 6d 5f 7s 7p 8s b_splines: nmax: 30 lmax: 7 k: 7 val_aov: s: 3 p: 3 d: 3 f: 3 val_energies: kval: 2 energies: s: -0.28000 p: [-0.22000, -0.22000] d: [-0.31000, -0.31000] f: [-0.13000, -0.13000] The basis.cavity_radius field takes in a value to set the size of the spherical cavity. The basis.orbitals block requires specification of the core and valence orbitals to be included in the basis. The basis.b_splines block requires specification of the maximum principal quantum number nmax (number of splines), maximum partial wave number lmax , and order of splines. The basis.val_aov block requires the number of valence orbitals to include for each partial wave. The basis.val_energies block allows specification of the energies of the valence orbitals. In addition, the script will read the optional block: optional: qed: include: False rotate_basis: False isotope_shifts: include: False K_is: 1 C_is: 0.01 code_exec: ci+all-order run_ao_codes: False The optional.qed block allows users to specify inclusion of QED corrections (this is currently not supported). The optional.isotope_shifts block allows users to specify isotope shift calculations by switching the include value to True and specifying keys K_is and C_is . The optional.code_exec field allows users to specify automatic execution of basis set codes depending on value. The optional.run_ao_codes field allows users to specify whether they would like to run the all-order set of codes after construction of the basis set. add.py The add.py script automatically generates the list of configurations given information about the atomic system of interest via a configuration file config.yml . This script reads the add block: add: # Set of even and odd parity reference configurations ref_configs: odd: even: [ 1s2 2s2 2p6 3s2 3p4, 1s2 2s2 2p6 3s2 3p3 4p1, 1s2 2s2 2p6 3s1 3p4 4s1 ] basis_set: 17spdfg orbitals: core: active: [ 1-2s: 2 2, 2p: 6 6, 3-7p: 0 6, 3-7d: 0 6, 4-5f: 0 6, ] excitations: single: True double: True triple: False The add.ref_configs block requires a list of reference configurations to excite electrons from to construct the list of configurations for the CI calculation. This list will not be constructed if left blank for a specified parity. The add.basis_set block requires specification of the basis set designated by nspdfg , where n specifies the principal quantum number, and spdfg specifies inclusion of s, p, d, f, and g orbitals. One can include higher partial waves by appending to the end of the list h , i , k , ... The add.orbitals block allows full customization of allowed orbital occupancies. For example, 1-2s: 2 2 defines the 1s and 2s orbitals to be closed, 2p: 6 6 defines the 2p orbital to be closed, 3-7p: 0 6 defines 3p to 7p orbitals to be completely open to allow up to 6 electrons on those orbitals. The add.excitations block defines the number of allowed excitations. The script first asks for the name of the configuration file, which it parses and constructs the ADD.INP files. It then asks if the user would like EVEN and ODD parity directories to be generated. If the user responds with yes , both directories will be generated, and relevant input files will be copied to each directory. The script then also asks if the user would like the CI job scripts submitted to the cluster. If responded with yes , a new job script will be generated if one doesn't already exist, then the job script will be submitted. parse_asd.py The parse_asd.py script parses the NIST Atomic Spectra Database and generates a csv-formatted file with spectral data for a user-inputed system of interest. The csv outputted by the script is formatted for the UD Portal for High-Precision Atomic Data and Computation .","title":"Auxiliary scripts"},{"location":"auxiliary/#auxiliary-scripts","text":"In this section, we describe supplementary python scripts used to automatically generate basis sets and configuration lists used for configuration interaction. It is also able to submit batch jobs to run the CI programs on the UD computers. Scripts that require the execution of a pCI program read a user-defined configuration file config.yml to set general parameters defining the atomic system of interest.","title":"Auxiliary scripts"},{"location":"auxiliary/#configyml","text":"The config.yml file is a YAML configuration file that defines important parameters of the atomic system of interest. This config file is divided into several sections: general parameters parameters used by basis programs (only read by basis.py) parameters used by add program (only read by add.py) parameters used by conf program (only read by add.py) optional parameters to automatically execute programs, include QED, or include isotope shift calculations (only read by basis.py) The general parameters are in the block system and is read by all python scripts: system: name: Ni12+ isotope: include_breit: True The name field takes the name of the atomic system of interest. The isotope field takes a specified isotope number. If left blank, the script will automatically find the atomic weight from nuclear radii table. The include_breit field takes in a boolean to set whether to include Breit corrections or not.","title":"config.yml"},{"location":"auxiliary/#basispy","text":"The basis.py script automatically generates the basis set given information about the atomic system of interest via a configuration file config.yml . This script reads the basis block: basis: cavity_radius: 60 orbitals: core: 1s 2s 2p 3s 3p 3d 4s 4p 4d 4f 5s 5p 5d 6s 6p valence: 6d 5f 7s 7p 8s b_splines: nmax: 30 lmax: 7 k: 7 val_aov: s: 3 p: 3 d: 3 f: 3 val_energies: kval: 2 energies: s: -0.28000 p: [-0.22000, -0.22000] d: [-0.31000, -0.31000] f: [-0.13000, -0.13000] The basis.cavity_radius field takes in a value to set the size of the spherical cavity. The basis.orbitals block requires specification of the core and valence orbitals to be included in the basis. The basis.b_splines block requires specification of the maximum principal quantum number nmax (number of splines), maximum partial wave number lmax , and order of splines. The basis.val_aov block requires the number of valence orbitals to include for each partial wave. The basis.val_energies block allows specification of the energies of the valence orbitals. In addition, the script will read the optional block: optional: qed: include: False rotate_basis: False isotope_shifts: include: False K_is: 1 C_is: 0.01 code_exec: ci+all-order run_ao_codes: False The optional.qed block allows users to specify inclusion of QED corrections (this is currently not supported). The optional.isotope_shifts block allows users to specify isotope shift calculations by switching the include value to True and specifying keys K_is and C_is . The optional.code_exec field allows users to specify automatic execution of basis set codes depending on value. The optional.run_ao_codes field allows users to specify whether they would like to run the all-order set of codes after construction of the basis set.","title":"basis.py"},{"location":"auxiliary/#addpy","text":"The add.py script automatically generates the list of configurations given information about the atomic system of interest via a configuration file config.yml . This script reads the add block: add: # Set of even and odd parity reference configurations ref_configs: odd: even: [ 1s2 2s2 2p6 3s2 3p4, 1s2 2s2 2p6 3s2 3p3 4p1, 1s2 2s2 2p6 3s1 3p4 4s1 ] basis_set: 17spdfg orbitals: core: active: [ 1-2s: 2 2, 2p: 6 6, 3-7p: 0 6, 3-7d: 0 6, 4-5f: 0 6, ] excitations: single: True double: True triple: False The add.ref_configs block requires a list of reference configurations to excite electrons from to construct the list of configurations for the CI calculation. This list will not be constructed if left blank for a specified parity. The add.basis_set block requires specification of the basis set designated by nspdfg , where n specifies the principal quantum number, and spdfg specifies inclusion of s, p, d, f, and g orbitals. One can include higher partial waves by appending to the end of the list h , i , k , ... The add.orbitals block allows full customization of allowed orbital occupancies. For example, 1-2s: 2 2 defines the 1s and 2s orbitals to be closed, 2p: 6 6 defines the 2p orbital to be closed, 3-7p: 0 6 defines 3p to 7p orbitals to be completely open to allow up to 6 electrons on those orbitals. The add.excitations block defines the number of allowed excitations. The script first asks for the name of the configuration file, which it parses and constructs the ADD.INP files. It then asks if the user would like EVEN and ODD parity directories to be generated. If the user responds with yes , both directories will be generated, and relevant input files will be copied to each directory. The script then also asks if the user would like the CI job scripts submitted to the cluster. If responded with yes , a new job script will be generated if one doesn't already exist, then the job script will be submitted.","title":"add.py"},{"location":"auxiliary/#parse_asdpy","text":"The parse_asd.py script parses the NIST Atomic Spectra Database and generates a csv-formatted file with spectral data for a user-inputed system of interest. The csv outputted by the script is formatted for the UD Portal for High-Precision Atomic Data and Computation .","title":"parse_asd.py"},{"location":"basis_complex/","text":"Example: Fe XVII and Ni XIX The following instructions assume familiarity with the main programs of the pCI package . In this section, we describe a method used to construct basis sets for the cases of Fe XVII and Ni XIX. In this example, we utilize multiple HFD.INP to construct the orbitals. The following is a list of the input files used in this example. h_m_1.inp - Here we construct the 1s, 2s, 2p, 3s, 3p, 3d orbitals with the 2s^2 2p^5 3d configuration. QQ is 0 for 3s and 3p, but the orbital is still formed. DF is solved with 0 occ. num., but they won't be very good orbitals. You can think of 3s0 and 3p0 as placeholders to keep the order of orbitals, and add them in the next step. h_m_2.inp - Here we freeze the 1s, 2s, 2p, 3d orbitals and re-construct the 3s, 3p orbitals from the 2s^2 2p^5 3s and 2s^2 2p^5 3p configurations. The following code block shows h_m_1.inp on the left and h_m_2.inp on the right of the partition. The head of both input files are identical. Fe XVII & Ni XIX KL = 0 # (0 - new calculation, 1 - continue) NS = 9 # number of orbitals NSO= 2 # number of closed orbitals (in this case only 1s2, 2s2) Z = 26.0 # atomic number AM = 56.000 # atomic mass JM = -2.0 # R2 = 20.0 # radius of cavity kbr= 2 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) NL J QQ KP NC | NL J QQ KP NC | 1 1S (1/2) 2.0000 0 0 | 1 1S (1/2) 2.0000 1 0 2 2S (1/2) 2.0000 0 0 | 2 2S (1/2) 2.0000 1 0 3 2P (1/2) 2.0000 0 0 | 3 2P (1/2) 2.0000 1 0 4 2P (3/2) 3.0000 0 0 | 4 2P (3/2) 3.0000 1 0 5 3S (1/2) 0.0000 0 0 | 5 3S (1/2) 1.0000 0 1 6 3P (1/2) 0.0000 0 0 | 6 3P (1/2) 1.0000 0 2 7 3P (3/2) 0.0000 0 0 | 7 3P (3/2) 0.0000 0 2 8 3D (3/2) 1.0000 0 0 | 8 3D (3/2) 0.0000 1 3 9 3D (5/2) 0.0000 0 0 | 9 3D (5/2) 0.0000 1 3 h_m_3.inp - Here we freeze the 1s, 2s, 2p, 3s, 3p, 3d orbitals then construct the 4s, 4p, 4d, 4f, 5g orbitals from the 2s^2 2p^5 4s, 2s^2 2p^5 4p, \\dots, 2s^2 2p^5 5g configurations. Note that the number of orbitals Ns has changed from 9 to 13. Fe XVII & Ni XIX KL = 0 # NS = 13 # number of orbitals NSO= 2 # number of closed orbitals (in this case only 1s2, 2s2) Z = 26.0 # atomic number AM = 56.000 # atomic mass JM = -2.0 # R2 = 20.0 # radius of cavity kbr= 2 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) NL J QQ KP NC 1 1S (1/2) 2.0000 1 0 2 2S (1/2) 2.0000 1 0 3 2P (1/2) 2.0000 1 0 4 2P (3/2) 3.0000 1 0 5 3S (1/2) 0.0000 1 0 6 3P (1/2) 0.0000 1 0 7 3P (3/2) 0.0000 1 0 8 3D (3/2) 0.0000 1 0 9 3D (5/2) 0.0000 1 0 10 4F (5/2) 1.0000 0 1 11 4F (7/2) 0.0000 0 1 12 5G (7/2) 1.0000 0 2 13 5G (9/2) 0.0000 0 2 b_m_2.inp - Here we update HFD.DAT by including virtual orbitals to account for correlations. Fe XVII & Ni XIX Z = 26.0 Am = 52.0 Nso= 4 # number of core orbitals (defines DF operator) Nv = 84 # number of valence & virtual orbitals Ksg= 1 # defines Hamiltonian: 1-DF, 3-DF+Breit Kdg= 0 # diagonalization of Hamiltonian (0=no,1,2=yes) orb= 4s 1 # first orbital for diagonalization Kkin 1 # kinetic balance (0,1,or 2) orb= 5s 1 # first orbital to apply kin.bal. orb= 2p 3 # last frozen orbital orb= 0p 3 # last orbital in basis set kout= 0 # detail rate in the output kbrt= 2 # 0,1,2 - Coulomb, Gaunt, Breit ---------------------------------------------------------- 0.1002 0.2002 -0.2102 0.2104 1 0.3001 # 2 -0.3101 # These orbitals are in HFD.DAT already run by hfd 3 0.3101 # 4 -0.3201 # 5 0.3201 # 6 0.4001 3 0.4001 # reading 4s from 4s from HFD.DAT 7 -0.4101 3 -0.4101 # key '3' means 'read in from HFD.DAT' 8 0.4101 3 0.4101 # HFD.DAT is h_m_3.inp in this case 9 -0.4201 3 -0.4201 10 0.4201 3 0.4201 11 -0.4301 3 -0.4301 12 0.4301 3 0.4301 13 0.5001 # key '0' or ' ' means 'build nl from (n-1)l' 14 -0.5101 # e.g. 5s is built from 4s, 5p from 4p 15 0.5101 # 5d from 4d, ... 16 -0.5201 17 0.5201 18 -0.5301 19 0.5301 20 -0.5401 3 -0.5401 # since key '3' is present, 5f is read in from HFD.DAT 21 0.5401 3 0.5401 22 -0.6401 23 0.6401 : : : 84 1.2401 The following bash script utilizes the above input files and forms the basis set for Fe XVII and Ni XIX. #! /bin/bash ##################################################################### # script to form basis set for Fe 16+ and Ni 18+ cp h_m_1.inp HFD.INP ./hfd cp h_m_2.inp HFD.INP ./hfd cp HFD.DAT h0.dat cp h_m_3.inp HFD.INP ./hfd mv HFD.DAT h_m.dat mv h0.dat HFD.DAT cp b_m_2.inp BASS.INP ./bass <b.in ./bass echo \" End of script\"","title":"Example 2 - constructing basis for Fe XVII and Ni XIX"},{"location":"basis_complex/#example-fe-xvii-and-ni-xix","text":"The following instructions assume familiarity with the main programs of the pCI package . In this section, we describe a method used to construct basis sets for the cases of Fe XVII and Ni XIX. In this example, we utilize multiple HFD.INP to construct the orbitals. The following is a list of the input files used in this example. h_m_1.inp - Here we construct the 1s, 2s, 2p, 3s, 3p, 3d orbitals with the 2s^2 2p^5 3d configuration. QQ is 0 for 3s and 3p, but the orbital is still formed. DF is solved with 0 occ. num., but they won't be very good orbitals. You can think of 3s0 and 3p0 as placeholders to keep the order of orbitals, and add them in the next step. h_m_2.inp - Here we freeze the 1s, 2s, 2p, 3d orbitals and re-construct the 3s, 3p orbitals from the 2s^2 2p^5 3s and 2s^2 2p^5 3p configurations. The following code block shows h_m_1.inp on the left and h_m_2.inp on the right of the partition. The head of both input files are identical. Fe XVII & Ni XIX KL = 0 # (0 - new calculation, 1 - continue) NS = 9 # number of orbitals NSO= 2 # number of closed orbitals (in this case only 1s2, 2s2) Z = 26.0 # atomic number AM = 56.000 # atomic mass JM = -2.0 # R2 = 20.0 # radius of cavity kbr= 2 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) NL J QQ KP NC | NL J QQ KP NC | 1 1S (1/2) 2.0000 0 0 | 1 1S (1/2) 2.0000 1 0 2 2S (1/2) 2.0000 0 0 | 2 2S (1/2) 2.0000 1 0 3 2P (1/2) 2.0000 0 0 | 3 2P (1/2) 2.0000 1 0 4 2P (3/2) 3.0000 0 0 | 4 2P (3/2) 3.0000 1 0 5 3S (1/2) 0.0000 0 0 | 5 3S (1/2) 1.0000 0 1 6 3P (1/2) 0.0000 0 0 | 6 3P (1/2) 1.0000 0 2 7 3P (3/2) 0.0000 0 0 | 7 3P (3/2) 0.0000 0 2 8 3D (3/2) 1.0000 0 0 | 8 3D (3/2) 0.0000 1 3 9 3D (5/2) 0.0000 0 0 | 9 3D (5/2) 0.0000 1 3 h_m_3.inp - Here we freeze the 1s, 2s, 2p, 3s, 3p, 3d orbitals then construct the 4s, 4p, 4d, 4f, 5g orbitals from the 2s^2 2p^5 4s, 2s^2 2p^5 4p, \\dots, 2s^2 2p^5 5g configurations. Note that the number of orbitals Ns has changed from 9 to 13. Fe XVII & Ni XIX KL = 0 # NS = 13 # number of orbitals NSO= 2 # number of closed orbitals (in this case only 1s2, 2s2) Z = 26.0 # atomic number AM = 56.000 # atomic mass JM = -2.0 # R2 = 20.0 # radius of cavity kbr= 2 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) NL J QQ KP NC 1 1S (1/2) 2.0000 1 0 2 2S (1/2) 2.0000 1 0 3 2P (1/2) 2.0000 1 0 4 2P (3/2) 3.0000 1 0 5 3S (1/2) 0.0000 1 0 6 3P (1/2) 0.0000 1 0 7 3P (3/2) 0.0000 1 0 8 3D (3/2) 0.0000 1 0 9 3D (5/2) 0.0000 1 0 10 4F (5/2) 1.0000 0 1 11 4F (7/2) 0.0000 0 1 12 5G (7/2) 1.0000 0 2 13 5G (9/2) 0.0000 0 2 b_m_2.inp - Here we update HFD.DAT by including virtual orbitals to account for correlations. Fe XVII & Ni XIX Z = 26.0 Am = 52.0 Nso= 4 # number of core orbitals (defines DF operator) Nv = 84 # number of valence & virtual orbitals Ksg= 1 # defines Hamiltonian: 1-DF, 3-DF+Breit Kdg= 0 # diagonalization of Hamiltonian (0=no,1,2=yes) orb= 4s 1 # first orbital for diagonalization Kkin 1 # kinetic balance (0,1,or 2) orb= 5s 1 # first orbital to apply kin.bal. orb= 2p 3 # last frozen orbital orb= 0p 3 # last orbital in basis set kout= 0 # detail rate in the output kbrt= 2 # 0,1,2 - Coulomb, Gaunt, Breit ---------------------------------------------------------- 0.1002 0.2002 -0.2102 0.2104 1 0.3001 # 2 -0.3101 # These orbitals are in HFD.DAT already run by hfd 3 0.3101 # 4 -0.3201 # 5 0.3201 # 6 0.4001 3 0.4001 # reading 4s from 4s from HFD.DAT 7 -0.4101 3 -0.4101 # key '3' means 'read in from HFD.DAT' 8 0.4101 3 0.4101 # HFD.DAT is h_m_3.inp in this case 9 -0.4201 3 -0.4201 10 0.4201 3 0.4201 11 -0.4301 3 -0.4301 12 0.4301 3 0.4301 13 0.5001 # key '0' or ' ' means 'build nl from (n-1)l' 14 -0.5101 # e.g. 5s is built from 4s, 5p from 4p 15 0.5101 # 5d from 4d, ... 16 -0.5201 17 0.5201 18 -0.5301 19 0.5301 20 -0.5401 3 -0.5401 # since key '3' is present, 5f is read in from HFD.DAT 21 0.5401 3 0.5401 22 -0.6401 23 0.6401 : : : 84 1.2401 The following bash script utilizes the above input files and forms the basis set for Fe XVII and Ni XIX. #! /bin/bash ##################################################################### # script to form basis set for Fe 16+ and Ni 18+ cp h_m_1.inp HFD.INP ./hfd cp h_m_2.inp HFD.INP ./hfd cp HFD.DAT h0.dat cp h_m_3.inp HFD.INP ./hfd mv HFD.DAT h_m.dat mv h0.dat HFD.DAT cp b_m_2.inp BASS.INP ./bass <b.in ./bass echo \" End of script\"","title":"Example: Fe XVII and Ni XIX"},{"location":"basis_neutral/","text":"Examples: Neutral atoms The following instructions assume familiarity with the main programs of the pCI package . In this section, we describe a method used to construct basis sets for the case of neutral atoms. In these example, we utilize a single HFD.INP to construct the orbitals. Ac Ac KL = 0 # (0 - new calculation, 1 - continue) NS = 36 # number of orbitals NSO= 24 # number of closed orbitals (in this case only 1s2, 2s2) Z = 89.0 # atomic number AM = 227.00 # atomic mass JM = -2.0 # (-2 - average for non-relativistic configuration) R2 = 60.0 # radius of cavity kbr= 2 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) rnuc= 5.7350 # (optional) rms nuclear radius (https://www-nds.iaea.org/radii/) NL J QQ KP NC 1 1S (1/2) 2.0000 0 0 2 2S (1/2) 2.0000 0 0 3 2P (1/2) 2.0000 0 0 4 2P (3/2) 4.0000 0 0 5 3S (1/2) 2.0000 0 0 6 3P (1/2) 2.0000 0 0 7 3P (3/2) 4.0000 0 0 8 3D (3/2) 4.0000 0 0 9 3D (5/2) 6.0000 0 0 10 4S (1/2) 2.0000 0 0 11 4P (1/2) 2.0000 0 0 12 4P (3/2) 4.0000 0 0 13 4D (3/2) 4.0000 0 0 14 4D (5/2) 6.0000 0 0 15 4F (5/2) 6.0000 0 0 16 4F (7/2) 8.0000 0 0 17 5S (1/2) 2.0000 0 0 18 5P (1/2) 2.0000 0 0 19 5P (3/2) 4.0000 0 0 20 5D (3/2) 4.0000 0 0 21 5D (5/2) 6.0000 0 0 22 6S (1/2) 2.0000 0 0 23 6P (1/2) 2.0000 0 0 24 6P (3/2) 4.0000 0 0 25 7S (1/2) 1.0000 0 1 26 6D (3/2) 1.0000 0 2 27 6D (5/2) 0.0000 0 2 28 7P (1/2) 1.0000 0 3 29 7P (3/2) 0.0000 0 3 30 5F (5/2) 1.0000 0 4 31 5F (7/2) 0.0000 0 4 32 8S (1/2) 1.0000 0 5 33 7D (3/2) 1.0000 0 6 34 7D (5/2) 0.0000 0 6 35 8P (1/2) 1.0000 0 7 36 8P (3/2) 0.0000 0 7 Sr Sr III KL = 0 # (0 - new calculation, 1 - continue) NS = 20 # number of orbitals NSO= 12 # number of closed orbitals (in this case only 1s2, 2s2) Z = 38.0 # atomic number AM = 90.000 # atomic mass JM = -2.0 # (-2 - average for non-relativistic configuration) R2 = 60.0 # radius of cavity kbr= 0 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) NL J QQ KP NC 1 1S (1/2) 2.0000 0 0 2 2S (1/2) 2.0000 0 0 3 2P (1/2) 2.0000 0 0 4 2P (3/2) 4.0000 0 0 5 3S (1/2) 2.0000 0 0 6 3P (1/2) 2.0000 0 0 7 3P (3/2) 4.0000 0 0 8 3D (3/2) 4.0000 0 0 9 3D (5/2) 6.0000 0 0 10 4S (1/2) 2.0000 0 0 11 4P (1/2) 2.0000 0 0 12 4P (3/2) 4.0000 0 0 13 5S (1/2) 1.0000 0 1 14 5P (1/2) 1.0000 0 2 15 5P (3/2) 0.0000 0 2 16 4D (3/2) 1.0000 0 3 17 4D (5/2) 0.0000 0 3 18 6S (1/2) 1.0000 0 4 19 6P (1/2) 1.0000 0 5 20 6P (3/2) 0.0000 0 5","title":"Example 1 - constructing basis for Ac and Sr"},{"location":"basis_neutral/#examples-neutral-atoms","text":"The following instructions assume familiarity with the main programs of the pCI package . In this section, we describe a method used to construct basis sets for the case of neutral atoms. In these example, we utilize a single HFD.INP to construct the orbitals.","title":"Examples: Neutral atoms"},{"location":"basis_neutral/#ac","text":"Ac KL = 0 # (0 - new calculation, 1 - continue) NS = 36 # number of orbitals NSO= 24 # number of closed orbitals (in this case only 1s2, 2s2) Z = 89.0 # atomic number AM = 227.00 # atomic mass JM = -2.0 # (-2 - average for non-relativistic configuration) R2 = 60.0 # radius of cavity kbr= 2 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) rnuc= 5.7350 # (optional) rms nuclear radius (https://www-nds.iaea.org/radii/) NL J QQ KP NC 1 1S (1/2) 2.0000 0 0 2 2S (1/2) 2.0000 0 0 3 2P (1/2) 2.0000 0 0 4 2P (3/2) 4.0000 0 0 5 3S (1/2) 2.0000 0 0 6 3P (1/2) 2.0000 0 0 7 3P (3/2) 4.0000 0 0 8 3D (3/2) 4.0000 0 0 9 3D (5/2) 6.0000 0 0 10 4S (1/2) 2.0000 0 0 11 4P (1/2) 2.0000 0 0 12 4P (3/2) 4.0000 0 0 13 4D (3/2) 4.0000 0 0 14 4D (5/2) 6.0000 0 0 15 4F (5/2) 6.0000 0 0 16 4F (7/2) 8.0000 0 0 17 5S (1/2) 2.0000 0 0 18 5P (1/2) 2.0000 0 0 19 5P (3/2) 4.0000 0 0 20 5D (3/2) 4.0000 0 0 21 5D (5/2) 6.0000 0 0 22 6S (1/2) 2.0000 0 0 23 6P (1/2) 2.0000 0 0 24 6P (3/2) 4.0000 0 0 25 7S (1/2) 1.0000 0 1 26 6D (3/2) 1.0000 0 2 27 6D (5/2) 0.0000 0 2 28 7P (1/2) 1.0000 0 3 29 7P (3/2) 0.0000 0 3 30 5F (5/2) 1.0000 0 4 31 5F (7/2) 0.0000 0 4 32 8S (1/2) 1.0000 0 5 33 7D (3/2) 1.0000 0 6 34 7D (5/2) 0.0000 0 6 35 8P (1/2) 1.0000 0 7 36 8P (3/2) 0.0000 0 7","title":"Ac"},{"location":"basis_neutral/#sr","text":"Sr III KL = 0 # (0 - new calculation, 1 - continue) NS = 20 # number of orbitals NSO= 12 # number of closed orbitals (in this case only 1s2, 2s2) Z = 38.0 # atomic number AM = 90.000 # atomic mass JM = -2.0 # (-2 - average for non-relativistic configuration) R2 = 60.0 # radius of cavity kbr= 0 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) NL J QQ KP NC 1 1S (1/2) 2.0000 0 0 2 2S (1/2) 2.0000 0 0 3 2P (1/2) 2.0000 0 0 4 2P (3/2) 4.0000 0 0 5 3S (1/2) 2.0000 0 0 6 3P (1/2) 2.0000 0 0 7 3P (3/2) 4.0000 0 0 8 3D (3/2) 4.0000 0 0 9 3D (5/2) 6.0000 0 0 10 4S (1/2) 2.0000 0 0 11 4P (1/2) 2.0000 0 0 12 4P (3/2) 4.0000 0 0 13 5S (1/2) 1.0000 0 1 14 5P (1/2) 1.0000 0 2 15 5P (3/2) 0.0000 0 2 16 4D (3/2) 1.0000 0 3 17 4D (5/2) 0.0000 0 3 18 6S (1/2) 1.0000 0 4 19 6P (1/2) 1.0000 0 5 20 6P (3/2) 0.0000 0 5","title":"Sr"},{"location":"basis_x/","text":"Building a basis for CI+all-order and CI+MBPT The following instructions assume familiarity with the main programs of the pCI package . CI and all-order basis sets In this section, we describe the general method of building basis sets for the CI+all-order and CI+MBPT code packages. As the CI and all-order code packages were developed separately, they use basis sets in different formats. CI uses HFD.DAT and all-order uses hfspl.1 and hfspl.2 files. For all-order/CI+all-order/CI+MBPT calculations, most of the basis is constructed via the B-splines. However, one needs too many B-splines to reproduce core and lower valence stats with high enough accuracy for heavier atoms. Therefore, core and a few few valence electron wave functions are taken from Dirac-Hartree-Fock (DHF), and a combined basis with splines is built. More splines will mean larger basis for the CI as well, so we would like to avoid this. There also seems to be an issue with the basis converter codes for larger numbers of B-splines. All-order or MBPT calculations involve sums over all possible states. To computer these accurately, one needs a large basis. Generally, we use lmax=6 and Nmax=35 for each of the partial waves (1-35s, 2-35p_{1/2},2-35p_{3/2},\\dots) . CI does not require such large basis sets and it is reducing for CI computations. Note More splines will mean larger basis for the CI as well, so we would like to avoid this. There also seems to be an issue with the basis converter codes for larger numbers of B-splines. Note There is a technical issue of CI codes using Taylor expansion inside the nucleus, while all-order codes use a radial grid that starts from the origin. This makes format conversion somewhat imperfect near the nucleus. Instructions Now we will discuss the steps to build the basis set for CI+all-order and CI+MBPT calculations. Produce B-splines ./tdhf < bas_wj.in - solves DHF equations (reads bas_wj.in and writes fort.1 ). Click here to see a description of bas_wj.in . Sr 20 38 88 0 0 9 13 1 # this line is FORMATTED in 4's #23412341234123412341234123412341234 # Sr - name (just a label) # 20 - number of inputs (core+valence) - must # 38 - Z # 88 - AM # 0 0 do not change # 9 convergence parameter # 13 number of first valence shell # 1 valence (0 for core only) # n kappa (5 or 1 - iteration parameter) 1 -1 5 0.00 # 0.00 - energy guess (leave 0.00 for core) 2 -1 5 0.00 2 1 5 0.00 2 -2 5 0.00 3 -1 5 0.00 3 1 5 0.00 3 -2 5 0.00 3 2 5 0.00 3 -3 5 0.00 4 -1 5 0.00 4 1 5 0.00 4 -2 5 0.00 5 -1 5 -0.20 # -0.10 or -0.20 good start for neutral 5 1 5 -0.20 5 -2 5 -0.20 4 2 5 -0.10 4 -3 5 -0.10 6 -1 5 -0.10 6 1 5 -0.10 6 -2 5 -0.10 1.5 # keep 1.5 or 1, 2.0 for Yb 0.0005 0.03 500 # radial grid (0.00005 for HFS) first point, 0.03, max number of points 1 # keep 1, 2 for deformed nuclei 0.0000 4.8665 2.3 # rnuc, cnuc, t - either rnuc=0 (rms?) or cnuc=0 0.0 # don't change this ./nspl40 < spl.in - produces B-spline basis (reads fort.1 and writes hfspl.1 and hfspl.2 ). Click here to see a description of spl.in . 6 # lmax 60.0 # Radius of cavity (same as hfd) 40 7 # number of spline, order of splines 30/5 40/7 50/9 70/11 0.0 0.00 500 # do not change Note bdhf and bspl40 are used in place of tdhf and nspl40 if Breit corrections are included. Run HFD code ./hfd - solves DHF equations (reads HFD.INP and writes HFD.DAT ) Note hfd can work with partially opened shells, while tdhf and bdhf cannot. Convert B-spline basis to HFD.DAT format ./bas_wj - converts hfspl.1 and hfspl.2 B-spline files and writes WJ.DAT and BASS.INP Note There are a couple of changes that have to be made to the resulting BASS.INP file: 1. The line lst= 0s 1# last orbital to be kept in the basis set has to be deleted. 2. The line orb= 0s 1# first orbital to apply kin.bal. has to be changed to reflect the first orbital not from hfd . 3. The line orb= 5s 1# first orbital for diagonalization can either be kept or changed to reflect the first orbital not from hfd . 4. The lines 1 0.5001 3 0.5001 that include orbitals from hfd have to be changed to 1 0.5001 Build combined basis from HFD.DAT and WJ.DAT ./bass - reads BASS.INP , HFD.DAT and WJ.DAT and writes new HFD.DAT Convert combined HFD.DAT to format used for all-order and second-order codes rm hfspl.1 hfspl.2 - erase the files hfspl.1 and hfspl.2 ./bas_x - reads HFD.DAT and writes hfspl.1 and hfspl.2 The final hfspl.1 and hfspl.2 files are the basis set files that can be read from the all-order part of the package .","title":"Building a basis set for CI+X"},{"location":"basis_x/#building-a-basis-for-ciall-order-and-cimbpt","text":"The following instructions assume familiarity with the main programs of the pCI package .","title":"Building a basis for CI+all-order and CI+MBPT"},{"location":"basis_x/#ci-and-all-order-basis-sets","text":"In this section, we describe the general method of building basis sets for the CI+all-order and CI+MBPT code packages. As the CI and all-order code packages were developed separately, they use basis sets in different formats. CI uses HFD.DAT and all-order uses hfspl.1 and hfspl.2 files. For all-order/CI+all-order/CI+MBPT calculations, most of the basis is constructed via the B-splines. However, one needs too many B-splines to reproduce core and lower valence stats with high enough accuracy for heavier atoms. Therefore, core and a few few valence electron wave functions are taken from Dirac-Hartree-Fock (DHF), and a combined basis with splines is built. More splines will mean larger basis for the CI as well, so we would like to avoid this. There also seems to be an issue with the basis converter codes for larger numbers of B-splines. All-order or MBPT calculations involve sums over all possible states. To computer these accurately, one needs a large basis. Generally, we use lmax=6 and Nmax=35 for each of the partial waves (1-35s, 2-35p_{1/2},2-35p_{3/2},\\dots) . CI does not require such large basis sets and it is reducing for CI computations. Note More splines will mean larger basis for the CI as well, so we would like to avoid this. There also seems to be an issue with the basis converter codes for larger numbers of B-splines. Note There is a technical issue of CI codes using Taylor expansion inside the nucleus, while all-order codes use a radial grid that starts from the origin. This makes format conversion somewhat imperfect near the nucleus.","title":"CI and all-order basis sets"},{"location":"basis_x/#instructions","text":"Now we will discuss the steps to build the basis set for CI+all-order and CI+MBPT calculations. Produce B-splines ./tdhf < bas_wj.in - solves DHF equations (reads bas_wj.in and writes fort.1 ). Click here to see a description of bas_wj.in . Sr 20 38 88 0 0 9 13 1 # this line is FORMATTED in 4's #23412341234123412341234123412341234 # Sr - name (just a label) # 20 - number of inputs (core+valence) - must # 38 - Z # 88 - AM # 0 0 do not change # 9 convergence parameter # 13 number of first valence shell # 1 valence (0 for core only) # n kappa (5 or 1 - iteration parameter) 1 -1 5 0.00 # 0.00 - energy guess (leave 0.00 for core) 2 -1 5 0.00 2 1 5 0.00 2 -2 5 0.00 3 -1 5 0.00 3 1 5 0.00 3 -2 5 0.00 3 2 5 0.00 3 -3 5 0.00 4 -1 5 0.00 4 1 5 0.00 4 -2 5 0.00 5 -1 5 -0.20 # -0.10 or -0.20 good start for neutral 5 1 5 -0.20 5 -2 5 -0.20 4 2 5 -0.10 4 -3 5 -0.10 6 -1 5 -0.10 6 1 5 -0.10 6 -2 5 -0.10 1.5 # keep 1.5 or 1, 2.0 for Yb 0.0005 0.03 500 # radial grid (0.00005 for HFS) first point, 0.03, max number of points 1 # keep 1, 2 for deformed nuclei 0.0000 4.8665 2.3 # rnuc, cnuc, t - either rnuc=0 (rms?) or cnuc=0 0.0 # don't change this ./nspl40 < spl.in - produces B-spline basis (reads fort.1 and writes hfspl.1 and hfspl.2 ). Click here to see a description of spl.in . 6 # lmax 60.0 # Radius of cavity (same as hfd) 40 7 # number of spline, order of splines 30/5 40/7 50/9 70/11 0.0 0.00 500 # do not change Note bdhf and bspl40 are used in place of tdhf and nspl40 if Breit corrections are included. Run HFD code ./hfd - solves DHF equations (reads HFD.INP and writes HFD.DAT ) Note hfd can work with partially opened shells, while tdhf and bdhf cannot. Convert B-spline basis to HFD.DAT format ./bas_wj - converts hfspl.1 and hfspl.2 B-spline files and writes WJ.DAT and BASS.INP Note There are a couple of changes that have to be made to the resulting BASS.INP file: 1. The line lst= 0s 1# last orbital to be kept in the basis set has to be deleted. 2. The line orb= 0s 1# first orbital to apply kin.bal. has to be changed to reflect the first orbital not from hfd . 3. The line orb= 5s 1# first orbital for diagonalization can either be kept or changed to reflect the first orbital not from hfd . 4. The lines 1 0.5001 3 0.5001 that include orbitals from hfd have to be changed to 1 0.5001 Build combined basis from HFD.DAT and WJ.DAT ./bass - reads BASS.INP , HFD.DAT and WJ.DAT and writes new HFD.DAT Convert combined HFD.DAT to format used for all-order and second-order codes rm hfspl.1 hfspl.2 - erase the files hfspl.1 and hfspl.2 ./bas_x - reads HFD.DAT and writes hfspl.1 and hfspl.2 The final hfspl.1 and hfspl.2 files are the basis set files that can be read from the all-order part of the package .","title":"Instructions"},{"location":"changelog/","text":"Changelog ## [0.11.15] - 2023-05-19 - basc v2.5 - increased data size of Ngint arrays to int*8 to fix integer overflow problems - conf v5.15 - increased data size of Ngint arrays to int*8 to fix integer overflow problems - conf_lsj v2.4 - increased data size of Ngint arrays to int*8 to fix integer overflow problems - conf_pt v2.2 - increased data size of Ngint arrays to int*8 to fix integer overflow problems - basc.f90 - changed datatype of Ngint related variables and arrays to int*8 - basc.f90 - added code to split arrays in half if array size goes over 2^31 for MPI_AllReduce calls - basc.f90 - increased string formatting for Ngint to I11 - basc.f90 - CONF.INT datatypes changed to reflect int*8 Ngint arrays - basc_variables.f90 - gets Ngint from conf_variables module - conf.f90 - changed datatype of Ngint related variables and arrays to int*8 - conf.f90 - changed broadcast of Ngint related arrays to use mpi_utils functions - conf_variables.f90 - Ngint and IntOrd datatypes changed to int*8 - integrals.f90 - changed datatype of Ngint related variables and arrays to int*8 - integrals.f90 - reads new CONF.INT with int*8 Ngint arrays ## [0.11.14] - 2023-04-18 - basc v2.4 - automated detection of max partial wave from configuration list and calculation of number of gaunt factors - conf v5.14 - automated detection of max partial wave from configuration list and number of gaunt factors - global change: renamed IPlx to Nlx since it is no longer a parameter - conf_variables.f90 - added Nlx as a global variable - conf_variables.f90 - added num_gaunts_per_partial_wave as a global array to store number of gaunt factors per partial wave - basc_variables.f90 - include num_gaunts_per_partial wave from conf_variables module - breit.f90 - save number of gaunt factors per partial wave to new allocatable array num_gaunts_per_partial_wave - breit.f90 - CONF.GNT now writes Nlx and num_gaunts_per_partial_wave in addition to Ngaunt and In, Gnt arrays - conf.f90 - subroutine Input now reads new version of CONF.GNT with additional Nlx and num_gaunts_per_partial_wave - conf.f90 - added exception if Nlx or num_gaunts_per_partial_wave could not be found to set Nlx=5 and Ngaunt=2891 as default - conf_init.f90 - added code to set Nlx to be the maximum partial wave read from configuration list from CONF.INP - integrals.f90 - removed assignment of IPlx since it is now determined from configuration list ## [0.11.13] - 2023-01-31 - conf v5.13 - bug fix for lost allocation of lsj arrays xj, xl, xs ## [0.11.12] - 2022-12-26 - conf v5.12 - bug fix for integer overflow for very large CI calculations ## [0.11.11] - 2022-11-30 - conf v5.11 - writing of configurations in ADD.INP format - conf.f90 - increased weight threshold to 0.9999 - conf.f90 - new code to write top weighted configurations in the format of ADD.INP basic configurations ## [0.11.10] - 2022-11-21 - conf v5.10 - bug fixes for ScaLAPACK routines causing openmp-related issues done by Dr. Jeffrey Frey of UD IT-RCI - conf.f90 - reimplemented DiagInitApprox subroutine with parallel blocking method - conf.f90 - bug fix for memory estimation of Davidson producing negative values - conf.f90 - threshold for Nd0 for use of S/DSYEV over ScaLAPACK can now be set from environment (defaulted to 800MiB matrix) - str_fmt.f90 - error catching for negative value of memory - env_var.f90 - new subroutine GetEnvInteger64 gets an environment variable as an int64 integer ## [0.11.9] - 2022-08-17 - ine v1.19 - fixed numerical errors in calculating a_1 value ## [0.11.8] - 2022-08-10 - conf v5.9 - bug fix for error occuring in allocation of arrays for LSJ - conf.f90 - removed unnecessary allocations and broadcasts to arrays storing integrals in InitLSJ and AllocateLSJArrays - conf.f90 - removed devel comments from previous version of conf - conf.f90 - increased integer formatting of j for configurations to 2 - conf.f90 - increased integer formatting for energy levels to 3 - davidson.f90 - increased integer formatting for energy levels to 3 ## [0.11.7] - 2022-08-05 - basc v2.2 - addition of Andrey's code for core Breit contributions - basc.f90 - added import of breit_int subroutine in subroutine Core - basc.f90 - new variable Ebcore for Breit contribution to core energy and its calculation - basc.f90 - core energy output now includes Breit - basc.f90 - changed formatting for Rint2Table ## [0.11.6] - 2022-08-03 - conf v5.8 - fix for weight subroutines when missing lines and counters for non-rel configs in CONF.INP - conf_init.f90 - removed counting of configurations (Nrnrc) and moved to subroutine Init - conf.f90 - new code to count number of non-relativistic configurations ## [0.11.5] - 2022-07-29 - ine v1.18 - bug fix for integer overflow in Mxmpy - ine.f90 - changed type for variable i8 to kind int64 ## [0.11.4] - 2022-07-28 - ine v1.17 - implemented 2-step iteration method - ine.f90 - new key value for kIters = 2 allows for 2-step iteration method - ine.f90 - new key N_it4 defines number of 2-step iterations ## [0.11.3] - 2022-07-25 - conf v5.7 - bug fix for Kv = 3 - davidson.f90 - re-added broadcast of Z1 in initial stage of FormB0 - conf.f90, davidson.f90, formj2.f90 - reverted generic intrisic functions (abs, sqrt) back to double-type intrinsic functions (dabs, dsqrt) - formj2.f90 - counter for J is now set by counter1 variable instead of size(Jsq%val) ## [0.11.2] - 2022-07-25 - create_add_inp.py - if no BASS.INP is present, the list of subshells and occupation numbers will now be ordered starting from basic configurations, then ascending by principal quantum number ## [0.11.1] - 2022-07-23 - added initial developmental version of create_conf_inp.py to py-lib folder ## [0.11.0] - 2022-07-22 - added python scripts to new py-lib folder - basis.py - python script that generates input files used to create basis sets - create_add_yml.py - python script that creates yaml-formatted input file used to create ADD.INP via create_add_inp.py script - create_add_inp.py - python script that creates ADD.INP using yaml-formatted input file - orbitals.py - python library of functions related to working with orbitals - read_bass.py - python library of functions related to reading BASS.INP - get_excitations.py - python library of functions related to generating excited configurations - get_atomic_data.py - python library of functions scraping atomic data tables online - add.yml - sample yaml input file using Sr - basis.yml - sample yaml input file using Ac - moved sort.py to new py-lib folder ## [0.10.6] - 2022-07-21 - tm v3.2 - addition of transition rates to E1, E2, M1 tables - dtm.f90 - removed cases E_i -> E_i from M1, E2 tables - dtm.f90 - new variables wl for wavelengths and tr for transition rates ## [0.10.5] - 2022-07-20 - ine v1.15 - bug fix for E2 polarizabilities - ine.f90 - added missing allocation statement in Ort subroutine - ine.f90 - removed calculation for negative parity for E2 polarizabilities - ine.f90 - remove xlamb_next from last iteration ## [0.10.4] - 2022-07-10 - dm v3.0 - implemented new tables for g-factors, A_hfs and B_hfs - dtm.f90 - rewrote SetKeys subroutine to open files depending on dm/tm and which keys are used - dtm.f90 - broadcasting of Iarr now uses mpi_utils subroutine BroadcastI - dtm.f90 - added code to get terms from CONFSTR.RES - dtm.f90 - added code to write tables for g-factors, A_hfs and B_hfs in DM regime ## [0.10.3] - 2022-07-07 - conf v5.6 - bug fixes and CONFFINAL.RES table changes - conf.f90 - CONF.ENG and CONF.LVL are now written correctly from a new file - conf.f90 - increased allocation of weight strings by factor of 5 - conf.f90 - change greek symbol delta in tables to DEL and energies EV to E_n (a.u.) in CONFFINAL.RES - conf_init.f90 - bug fix for reading blank lines between configurations from result of merge_ci or con_cut ## [0.10.2] - 2022-07-01 - conf v5.5 - bug fixes - conf.f90 - removed unused variables and arrays - conf.f90 - changed argument of WriteMatrix to be Type(Matrix) instead of 3 arrays - conf.f90 - added EXTERNAL declarations for BLACS_GRIDEXIT, BLACS_EXIT - conf.f90 - removed temporary Jsq fixes - formj2.f90 - added support for when core 0 obtains no non-zero matrix elements from FormJ - formj2.f90 - changed argument of WriteMatrix to be Type(Matrix) instead of 3 arrays - matrix_io.f90 - changed argument of WriteMatrix to be Type(Matrix) instead of 3 arrays - ine.f90 - removed unused variables - add.f90 - removed unused variables - mpi_utils.f90 - added EXTERNAL declaration for GetEnv ## [0.10.1] - 2022-06-30 - tm v3.1 - bug fixes - dtm.f90 - added conditionals to track terms if range does not start from level 1 - dtm.f90 - removed unused variables and arrays - dtm.f90 - increased format digits for NSP to 6 - dtm.f90 - broadcasting of Iarr now uses mpi_utils subroutine BroadcastI ## [0.10.0] - 2022-06-29 - tm v3.0 - implemented new tables for improved data analysis - dtm.f90: new type Key to encapsulate all keys for matrix elements - dtm.f90: new subroutines SetKeys and CloseKeys to open and close files for writing individual matrix element tables - dtm.f90: TM part now reads in 2 ranges: nterm1 to nterm1f and nterm2 to nterm2f - dtm.f90: dtm.in reads list of matrix elements starting from 3rd line: 'E1', 'E1_L', 'E1_V', 'E2', 'E3', 'M1', 'M2', 'M3', 'EDM', 'PNC', 'MQM', 'AM' - dtm.f90: added missing Read(Nsu) part for CONF.DET - dtm.f90: removed progress report - dtm.f90: added broadcast of new global variable nterm1f - dtm.f90: added code to read configurations and terms from CONFSTR.RES - dtm.f90: added loop over first range of energy levels - dtm.f90: moved reading of wavefunctions to inside loop over first range of energy levels - dtm.f90: added code to write tables of matrix elements ## [0.9.6] - 2022-06-16 - combined dtm_aux.f90 with dtm.f90 ## [0.9.5] - 2022-06-16 - conf v5.4 - implemented writing of weights of configurations when CONF.XIJ is written - conf.f90: new subroutine PrintWeightsDvdsn writes weights of configurations for each energy level to CONF.LVL every kXIJ iterations ## [0.9.4] - 2022-05-27 - ine v1.14 - bug fix for kl>0 where required arrays were not allocated ## [0.9.3] - 2022-05-26 - conf v5.3 - implemented writing of top weights of configurations to end of CONFLEVELS.RES - conf.f90: wrote code to count the top configurations in CONFLEVELS.RES and list them in weighted order - conf.f90: new Type(WeightTable) stores data for top weights - conf.f90: moved energies in EV and Delta(cm^-1) to after term if LSJ is used in CONFFINAL.RES - conf.f90: added a significant figure to gfactors - conf.f90: gfactors are no longer written if J=0 ## [0.9.2] - 2022-05-19 - conf v5.2 - implemented writing of terms if LSJ is used - conf.f90: separated writing of files from calculation of weights - conf.f90: changed Wsave, Wpsave, strcsave to dimension(nconfs,Nlv) - conf.f90: new function term to write term based on LSJ values - conf.f90: changed formatting of CONFFINAL.RES table to include term for LSJ - conf.f90: fixed formatting for different size configuration strings in CONFFINAL.RES table ## [0.9.1] - 2022-05-14 - conf v5.1 - implemented writing of energies every N davidson iteration - conf_init.f90: fixed out-of-bound error for array Nrnrc in subroutine ReadConfigurations - conf.f90: new subroutine PrintEnergiesDvdsn that writes table of energies to file CONF.ENG whenever CONF.XIJ is written - conf.f90: updated all mpi subroutines to use mpi_f08 - conf.f90: fixed out-of-bound error for array Wsave, Wpsave, strcsave in subroutine PrintWeights - conf.f90: updated CONFLEVELS.RES file to print energies with more digits ## [0.9.0] - 2022-05-13 - conf v5.0 - implemented ikarus and conf_lsj into conf - new energy table with weights (and optionally, LSJ) are written to file CONFFINAL.RES - list of top contributing configurations to each energy level are written to file CONFLEVELS.RES - conf_variables.f90: global variable kLSJ added to allow user to decide whether L, S, J are printed - conf_variables.f90: global variable Nnr added to count number of non-relativistic configurations required by subroutine PrintWeights - conf_variables.f90: global array Nrnrc added to store number of relativistic configurations in each non-relativistic configuration - determinants.f90: deallocation statements for Nq and Nip have been removed for subsequent LSJ calculations - determinants.f90: subroutine Rdet has been revamped to take in the name of the file to read determinants Iarr from - conf_init.f90: added code in subroutine ReadConfigurations to count number of non-rel. configurations Nnr and number of rel. configurations in each nr configuration Nrnrc - formj2.f90: deallocation statements for Jz and Nh have been removed for subsequent LSJ calculations - conf.f90: added code to calculate LSJ if key kLSJ = 1 - conf.f90: removed deallocation statements for Nvc and Nc0 in subroutine DeAllocateFormHArrays - conf.f90: new subroutine AllocateLSJArrays and InitLSJ added to allocate and initialize arrays required to calculate LSJ - conf.f90: new subroutines lsj, calcLSJ, lsj_det, plus_s, plus_l, p0_s, p0_l, and plus_j added to calculate LSJ - conf.f90: new array W2 added to store weights of non-relativistic configurations - conf.f90: new array Wsave and Wpsave added to store the top weights and indices of top weights - conf.f90: new code written to write a table of main configuration, S, L, J, gfactor, energy in eV, energy from ground state in cm-1, weight% for main configuration, secondary configuration and secondary weight if the weight for main configuration is < 70%, for each energy level - conf.f90: new code written to write list of top weights and configurations for each energy level - conf.f90: new function g_factor to calculate g-factors given L, S, J ## [0.8.6] - 2022-05-19 - conf v4.5 - bug fix for seg faults when writing intermediate CONF.XIJ caused by deallocation of Jsq after LAPACK subroutines ## [0.8.5] - 2022-04-04 - added serial modernized add program ## [0.8.4] - 2022-04-04 - conf v4.4 - merged Kl=3 and Kl=4 routines to single block - added exception for when no additional configurations were added with key Kl=3 - added additional timing tests to file CONF.PRG ## [0.8.3] - 2022-03-28 - conf v4.3 - bug fixes for ArrB vectorization - added determinant number in cntarray ## [0.8.2] - 2022-03-01 - conf v4.2 - changed BroadcastD to MPI_AllReduce ## [0.8.1] - 2022-02-04 - conf v4.1 - implementation of parameterized derived type for matrices to switch between single and double precision - added minimum value of matrix in type Matrix ## [0.8.0] - 2022-01-21 - conf v4.0 - implementation of Kl=3 - additional configurations - added global variables Nc_prev and Nd_prev to conf_variables.f90 - added subroutines IVAccumultorContinue and RVAccumulatorContinue to vaccumulator.f90 - added condition for Kl=3 to cycle when constructing initial approximation - added new conditional under FormH for Kl=3 - changed arguments of ReadMatrix and WriteMatrix to take in three 1d arrays instead of a matrix - added conditional to read and write previous Nc and Nd in subroutines ReadMatrix and WriteMatrix ## [0.7.56] - 2022-02-08 - ine v1.13 - increased limit of NumJ from integer to int*8 ## [0.7.56] - 2022-02-04 - ine v1.12 - increased limit of NumH from integer to int*8 ## [0.7.55] - 2022-01-13 - conf v3.46 - bug fix for Jsq%n for Kv=3 ## [0.7.54] - 2022-01-11 - ine v1.11 - ine now accepts a list of wavelengths to be entered with different step sizes - formatting of ine input file is now: Kl, Kli, Klf, N0, N2, nlambda, (xlambda1, xlambda2, xlambdastep), ... ## [0.7.53] - 2021-12-27 - ine v1.10 - small components of J-decomposition are no longer put to 0 ## [0.7.52] - 2021-11-30 - changed type of vLen, vSize, vGrowBy in vaccumulator.f90 to int64 ## [0.7.51] - 2021-11-26 - conf v3.45 - updated Rint/RintS errors if IPx was changed - split memCalcReqs FormH message into static arrays and FormH arrays - removed FormH memory from comparison stage of FormH - added memory summary after comparison stage of FormH ## [0.7.50] - 2021-11-24 - conf v3.44 - updated memory routines for FormH ## [0.7.49] - 2021-11-22 - conf v3.43 - bug fix for temporary fix for Jsq%n changing during LAPACK ZSYEV subroutine ## [0.7.48] - 2021-11-10 - conf v3.42 - fixed integer overflow in mpi_utils subroutines - increased data size of count_remain and i to integer(kind=int64) - mpi_utils subroutine now take in int*8 argument for count ## [0.7.47] - 2021-11-10 - conf v3.41 - bug fix for mpi_utils subroutine BroadcastI for extra large integer arrays ## [0.7.46] - 2021-11-1 - ine v1.9 - fix for overflow in final result of RdcE1 ## [0.7.45] - 2021-10-31 - ine v1.8 - increased number of significant figures in final result in RdcE1 ## [0.7.44] - 2021-10-31 - ine v1.7 - fixed formatting error in RdcE1 ## [0.7.43] - 2021-10-31 - conf v3.40 - added if allocated statement when deallocating arrays after Diag4 ## [0.7.42] - 2021-10-30 - conf v3.39 - bug fix in conf resolving slow down of convergence during Davidson procedure ## [0.7.41] - 2021-10-27 - conf v3.38 - bug fix in conf when reading old version of CONF.GNT ## [0.7.40] - 2021-10-27 - conf_lsj v2.3 - added g-factors to final table ## [0.7.39] - 2021-10-26 - conf v3.37 - re-implemented Jeff's Broadcast subroutines for MPI_Bcast calls for 2d arrays ## [0.7.38] - 2021-10-25 - conf v3.36 - added memory check after allocating arrays for Davidson procedure ## [0.7.37] - 2021-10-24 - conf v3.35 - added exception to set Ngaunt if not in CONF.GNT - added temporary fix for value of Jsq%n changing during LAPACK ZSYEV subroutine - all programs - removed all unused variables and arrays determined by compiler flag \"--check all --warn all\" ## [0.7.36] - 2021-10-20 - conf v3.34 - removed reading of CONF.XIJ from PrintEnergies subroutine - moved the printing of weights from PrintEnergies subroutine to a new PrintWeights subroutine - cleaned up formatting of PrintEnergies and PrintWeight subroutine ## [0.7.35] - 2021-10-19 - dtm v2.1 - bug fix for Kout /= 0 ## [0.7.34] - 2021-10-13 - conf v3.33 - added calculation of J for energy levels if writing CONF.XIJ file ## [0.7.33] - 2021-10-13 - removed dead code 'testwigner' used for testing from ine ## [0.7.32] - 2021-10-13 - global change: replaced parameter IPgnt with global variable Ngaunt read in from CONF.GNT - basc v2.1 - number of tabulated gaunts Ngaunt is now written to CONF.GNT and read in by subsequent codes - conf v3.32 - reads in Ngaunt from CONF.GNT - conf_lsj v2.2 - reads in Ngaunt from CONF.GNT - conf_pt v2.1 - reads in Ngaunt from CONF.GNT ## [0.7.31] - 2021-10-12 - conf v0.3.31 - moved Bcast of ax to end of loop ## [0.7.30] - 2021-10-12 - conf v0.3.30 - added conditional to optimize construction of initial approximation depending on Kl ## [0.7.29] - 2021-10-12 - conf v0.3.29 - fixed print message for number of conf-s in starting approximation ## [0.7.28] - 2021-10-10 - conf v0.3.28 - added mpi error handling for WriteMatrix and ReadMatrix subroutines ## [0.7.27] - 2021-10-08 - cleaned up dead code in some conf subroutines ## [0.7.26] - 2021-10-07 - conf v0.3.27 - moved averaging of diagonal over configurations to its own subroutine AvgDiag ## [0.7.25] - 2021-10-07 - conf v0.3.26 - moved initial diagonalization to its own subroutine DiagInitApprox ## [0.7.24] - 2021-10-07 - conf v0.3.25 - implemented ScaLAPACK subroutine PDSYEVD to replace DSYEV in initial diagonalization ## [0.7.23] - 2021-10-07 - conf v0.3.24 - reduced size of arrays E and Iconverge from IPlv to Nlv ## [0.7.22] - 2021-10-06 - conf v0.3.23 - replaced all Hould routines with LAPACK 'DSYEV' - removed global D, D1 work arrays and replaced with local W work array - initialize work array for diagonalizing P before iterative procedure - bug fix for Kl4=2 - broadcast Nlv in FormB0 after reading CONF.XIJ ## [0.7.21] - 2021-10-06 - conf v0.3.22 - optimized DSYEV subroutine in Diag4 ## [0.7.20] - 2021-10-04 - conf v0.3.21 - bug fix for Kv=3 - added MPI_Bcast of Z1 after diagonalization - more code documentation in Davidson procedure - refactored parts of Davidson procedure ## [0.7.19] - 2021-10-04 - conf v0.3.20 - changed MPI_Reduce of Z1 in Init4 to MPI_AllReduce, removing MPI_Bcast from FormB0 ## [0.7.18] - 2021-10-01 - conf v0.3.19 - subroutine ReadMatrix now distributes total number of matrix elements equally across cores ## [0.7.17] - 2021-09-29 - conf v0.3.18 - bug fix for writing/reading CONF.HIJ/CONF.JJJ for large scale runs - changed disp and disps in matrix_io.f90 to kind=MPI_OFFSET_KIND - added error message if matrix file doesn't exist ## [0.7.16] - 2021-09-29 - conf v0.3.17 - changed formatting, increasing range for Nc4 and Nd0 for initial approximation ## [0.7.15] - 2021-09-29 - conf v0.3.16 - replaced Hould subroutine during initial diagonalization with LAPACK 'dsyev' subroutine - removed redundant MPI_Bcast call of D1=Tk in subroutine WriteFinalXIj ## [0.7.14] - 2021-09-28 - conf v0.3.15 - added timing for initial diagonalization ## [0.7.13] - 2021-09-26 - conf_lsj v0.2.1 - include comparison of rel. conf-s after comparison of non-rel. confs ## [0.7.12] - 2021-09-26 - ine v1.6 - added new subroutine SetParams to set parameters for job and arrays - added new key kIters=(0-iterate and invert if diverged, 2-invert only) ## [0.7.11] - 2021-09-26 - ine v1.5 - removed IP1 dependency from params.f90 and allows it to be set it in subroutine Input ## [0.7.10] - 2021-09-26 - ine v1.4 - revamped input for range of wavelengths to accept single wavelength by inputting (wavelength1 wavelength1 x) as range - added several code documentation for ine ## [0.7.9] - 2021-09-25 - minor text fix: total computation time of conf_lsj ## [0.7.8] - 2021-09-25 - conf_lsj v0.2.0 - major revamp and optimization of lsj routine - relevant eigenvectors are all stored in memory in array B1h(Nd,nlvs) - one-electron matrix elements of l, s and j are stored in arrays instead of calculated every iteration - in the inner loop of lsj, conf-s are skipped if they belong to different non-rel. conf-s (using new subroutine CompNRC) - selection rules are added to skip zero contributions - many-electron matrix elements are calculated only once and used for all levels ## [0.7.7] - 2021-09-25 - conf v0.3.14 - key Kw revamped to be an input parameter along with Kl, Ksig and Kdsig ## [0.7.6] - 2021-09-25 - minor text fix for ine - moved subroutines from ine_aux.f90 to ine.f90 ## [0.7.5] - 2021-09-25 - ine now reads a range of wavelengths along with step instead of a single wavelength ## [0.7.4] - 2021-09-23 - conf v0.3.13 - bug fix for MPI_AllReduce to same buffer with large number of cores ## [0.7.3] - 2021-09-22 - conf_lsj v0.1.2 - bug fix for conf_lsj to read correct eigenvectors ## [0.7.2] - 2021-09-22 - conf_lsj v0.1.1 - conf_lsj now asks for 2 input parameters rec1 and rec2 to select range of desired levels ## [0.7.1] - 2021-09-21 - basc has been parallelized with MPI using simple static workload distribution - minor refactoring and text revisions ## [0.6.12] - 2021-09-18 - removed writing/reading of processors from HIJ and JJJ files - added more code documentation - initialization of global variables moved to module - revamped F_J2 function to remove redundant comparisons by Rspq ## [0.6.11] - 2021-09-16 - implemented dynamic workload scheduling in conf_lsj - added completion time message to ine - added several timing messages to conf_lsj ## [0.6.10] - 2021-09-15 - bug fix in conf - fixed energies for case of Kl4 = 2 - added more code documentation to conf ## [0.6.9] - 2021-09-14 - restructed conf program - removed conf_aux module and combined with main conf program - parallelized contruction of initial approximation in conf ## [0.6.8] - 2021-09-08 - bug fix in conf.f90 - Kl4 is now broadcasted ## [0.6.7] - 2021-09-03 - global variable Scr is now allocatable ## [0.6.6] - 2021-09-03 - bug fix in conf.f90 - fixed required memory count display before Davidson procedure ## [0.6.5] - 2021-09-03 - bug fix in ine.f90 - added rounding of Tj0 to prevent errors with NaN ## [0.6.4] - 2021-09-02 - bug fix in ine.f90 - fixed ReadJJJ subroutine to read CONF.JJJ file ## [0.6.3] - 2021-09-01 - bug fixes in matrix_io.f90 - ReadMatrix subroutine now reads nprocs.conf for # processors ## [0.6.2] - 2021-09-01 - bug fixes in \"conf\", allowing use of Kl=1 - reading of CONF.HIJ and CONF.JJJ files in conf ## [0.6.1] - 2021-08-31 - \"conf\" program now writes CONFp.HIJ and CONFp.JJJ instead of CONF.HIJ and CONF.JJJ - \"sort.py\" program converts CONFp.HIJ and CONFp.JJJ into serial CONF.HIJ and CONF.JJJ ## [0.6.0] - 2021-08-30 - Initial import of new modernized \"ine\" prgram - \"ine\" program has been modernized - \"ine\" program now uses intel mkl subroutine 'zspsv' - \"dtm\" program subroutine OpenFS has been updated to conf_pt_breit version - \"conf\" program subroutine WriteMatrix has been updated to write separate file nprocs.conf containing number of processors used in conf calculation - \"sort.py\" program has been updated to read nprocs.conf ## [0.5.0] - 2021-08-09 - Initial import of new parallelized \"conf_lsj\" program - \"conf_lsj\" program has been modernized and parallelized - \"conf_lsj\" program is now included in the build process of the parallel package. - \"conf_lsj\" has been tested with serial version and all results are identical in output files - \"conf_lsj\" now utilizes dynamic memory allocation ## [0.4.0] - 2021-07-29 - Revamp of FormH procedure to use split comparison and calculation stages to save memory - Many small bug fixes regarding memory - Progress bar fixed ## [0.3.6] - 2021-07-14 - Removed usage of MPI windows - suspected memory leak due to pointers not being deallocated - FormH bug fix - initial workload distribution was missing a chunk of determinants ## [0.3.5] - 2021-07-13 - Minor bug fixes and progress text changes ## [0.3.4] - 2021-07-09 - Updated progress report of FormJ to include memory per core ## [0.3.3] - 2021-07-08 - Bug fix for FormH and FormJ subroutines ## [0.3.2] - 2021-07-07 - Updated progress report of FormH and FormJ to include memory progress - Removed print statements of number of elements for each core ## [0.3.1] - 2021-06-30 - Bug fixes in FormH and FormJ new work schedulers - Implemented progress report at increments of 10% for FormH ## [0.3.0] - 2021-06-23 - Major parallelism revamp: FormH and FormJ subroutines now use a work scheduler to distribute workloads - Householder diagonalization parameters changed (tol: -103 to -1021; eps: -24 to -53) ## [0.2.4] - 2021-05-14 - \"Rspq\" bug fix: added diagnostics for dets to make sure orbitals are in correct order ## [0.2.3] - 2021-05-10 - \"matrix_io\" bug fix: compatibility fix for sortJJJ.py and sortHIJ.py - new code \"sort.py\": python code that converts parallel CONF.JJJ or CONF.HIJ to serial format ## [0.2.2] - 2021-04-23 - \"sint1\" in dtm_aux.f90: bug fix ## [0.2.1] - 2021-04-20 - \"mpi_wins\" bug fix: added lines to allocate and broadcast Iarr for zero-cores ## [0.2.0] - 2021-04-09 - initial import of new modernized \"basc\" program - \"basc\" program has been modernized and included in the build process of the parallel package. - \"basc\" has been tested with serial version and all results are identical in output files - \"basc\" now utilizes dynamic memory allocation ## [0.1.0] - 2021-03-28 - kv=3 functionality fully parallelized ## [0.0.5] - 2021-03-23 - new module \"matrix_io\": implements parallel reading and writing of matrices (Hamiltonian and J^2) ## [0.0.4] - 2021-03-22 - new type \"Matrix\": encapsulate indices and values of matrix elements (Hamil and Jsq) - new module \"mpi_wins\": implements creating and closing MPI windows/shared memory for basis set - new module \"mpi_utils\": used for DARWIN's no-ucx variant of intel-2020 to bypass 1GB MPI message limit - used intrinsic function PACK to remove all zero valued matrix elements from Hamiltonian and Jsquared - function PACK results in seg fault if not using 'ulimit -s unlimited' - Fixed discrepancy of NumH and NumJ between serial and parallel versions - reorganized timing calls - removed unused error variables - several minor text edits ## [0.0.3] - 2021-03-21 - dtm now uses conf_init module for reading CONF.INP - revamped dtm's Input subroutine - moved one-electron operator functions to a separate module amp_ops - several minor text edits ## [0.0.2] - 2021-03-20 - dtm now uses determinants module for determinant-based subroutines - made arrays storing configurations (iconf1, iconf2) consistent between all programs - several minor text edits - descriptions added for several subroutines in davidson module ## [0.0.1] - 2021-03-17 Fixed issue with table of J being unordered. ## [0.0.0] - 2021-03-17 Original source import.","title":"Changelog"},{"location":"changelog/#changelog","text":"## [0.11.15] - 2023-05-19 - basc v2.5 - increased data size of Ngint arrays to int*8 to fix integer overflow problems - conf v5.15 - increased data size of Ngint arrays to int*8 to fix integer overflow problems - conf_lsj v2.4 - increased data size of Ngint arrays to int*8 to fix integer overflow problems - conf_pt v2.2 - increased data size of Ngint arrays to int*8 to fix integer overflow problems - basc.f90 - changed datatype of Ngint related variables and arrays to int*8 - basc.f90 - added code to split arrays in half if array size goes over 2^31 for MPI_AllReduce calls - basc.f90 - increased string formatting for Ngint to I11 - basc.f90 - CONF.INT datatypes changed to reflect int*8 Ngint arrays - basc_variables.f90 - gets Ngint from conf_variables module - conf.f90 - changed datatype of Ngint related variables and arrays to int*8 - conf.f90 - changed broadcast of Ngint related arrays to use mpi_utils functions - conf_variables.f90 - Ngint and IntOrd datatypes changed to int*8 - integrals.f90 - changed datatype of Ngint related variables and arrays to int*8 - integrals.f90 - reads new CONF.INT with int*8 Ngint arrays ## [0.11.14] - 2023-04-18 - basc v2.4 - automated detection of max partial wave from configuration list and calculation of number of gaunt factors - conf v5.14 - automated detection of max partial wave from configuration list and number of gaunt factors - global change: renamed IPlx to Nlx since it is no longer a parameter - conf_variables.f90 - added Nlx as a global variable - conf_variables.f90 - added num_gaunts_per_partial_wave as a global array to store number of gaunt factors per partial wave - basc_variables.f90 - include num_gaunts_per_partial wave from conf_variables module - breit.f90 - save number of gaunt factors per partial wave to new allocatable array num_gaunts_per_partial_wave - breit.f90 - CONF.GNT now writes Nlx and num_gaunts_per_partial_wave in addition to Ngaunt and In, Gnt arrays - conf.f90 - subroutine Input now reads new version of CONF.GNT with additional Nlx and num_gaunts_per_partial_wave - conf.f90 - added exception if Nlx or num_gaunts_per_partial_wave could not be found to set Nlx=5 and Ngaunt=2891 as default - conf_init.f90 - added code to set Nlx to be the maximum partial wave read from configuration list from CONF.INP - integrals.f90 - removed assignment of IPlx since it is now determined from configuration list ## [0.11.13] - 2023-01-31 - conf v5.13 - bug fix for lost allocation of lsj arrays xj, xl, xs ## [0.11.12] - 2022-12-26 - conf v5.12 - bug fix for integer overflow for very large CI calculations ## [0.11.11] - 2022-11-30 - conf v5.11 - writing of configurations in ADD.INP format - conf.f90 - increased weight threshold to 0.9999 - conf.f90 - new code to write top weighted configurations in the format of ADD.INP basic configurations ## [0.11.10] - 2022-11-21 - conf v5.10 - bug fixes for ScaLAPACK routines causing openmp-related issues done by Dr. Jeffrey Frey of UD IT-RCI - conf.f90 - reimplemented DiagInitApprox subroutine with parallel blocking method - conf.f90 - bug fix for memory estimation of Davidson producing negative values - conf.f90 - threshold for Nd0 for use of S/DSYEV over ScaLAPACK can now be set from environment (defaulted to 800MiB matrix) - str_fmt.f90 - error catching for negative value of memory - env_var.f90 - new subroutine GetEnvInteger64 gets an environment variable as an int64 integer ## [0.11.9] - 2022-08-17 - ine v1.19 - fixed numerical errors in calculating a_1 value ## [0.11.8] - 2022-08-10 - conf v5.9 - bug fix for error occuring in allocation of arrays for LSJ - conf.f90 - removed unnecessary allocations and broadcasts to arrays storing integrals in InitLSJ and AllocateLSJArrays - conf.f90 - removed devel comments from previous version of conf - conf.f90 - increased integer formatting of j for configurations to 2 - conf.f90 - increased integer formatting for energy levels to 3 - davidson.f90 - increased integer formatting for energy levels to 3 ## [0.11.7] - 2022-08-05 - basc v2.2 - addition of Andrey's code for core Breit contributions - basc.f90 - added import of breit_int subroutine in subroutine Core - basc.f90 - new variable Ebcore for Breit contribution to core energy and its calculation - basc.f90 - core energy output now includes Breit - basc.f90 - changed formatting for Rint2Table ## [0.11.6] - 2022-08-03 - conf v5.8 - fix for weight subroutines when missing lines and counters for non-rel configs in CONF.INP - conf_init.f90 - removed counting of configurations (Nrnrc) and moved to subroutine Init - conf.f90 - new code to count number of non-relativistic configurations ## [0.11.5] - 2022-07-29 - ine v1.18 - bug fix for integer overflow in Mxmpy - ine.f90 - changed type for variable i8 to kind int64 ## [0.11.4] - 2022-07-28 - ine v1.17 - implemented 2-step iteration method - ine.f90 - new key value for kIters = 2 allows for 2-step iteration method - ine.f90 - new key N_it4 defines number of 2-step iterations ## [0.11.3] - 2022-07-25 - conf v5.7 - bug fix for Kv = 3 - davidson.f90 - re-added broadcast of Z1 in initial stage of FormB0 - conf.f90, davidson.f90, formj2.f90 - reverted generic intrisic functions (abs, sqrt) back to double-type intrinsic functions (dabs, dsqrt) - formj2.f90 - counter for J is now set by counter1 variable instead of size(Jsq%val) ## [0.11.2] - 2022-07-25 - create_add_inp.py - if no BASS.INP is present, the list of subshells and occupation numbers will now be ordered starting from basic configurations, then ascending by principal quantum number ## [0.11.1] - 2022-07-23 - added initial developmental version of create_conf_inp.py to py-lib folder ## [0.11.0] - 2022-07-22 - added python scripts to new py-lib folder - basis.py - python script that generates input files used to create basis sets - create_add_yml.py - python script that creates yaml-formatted input file used to create ADD.INP via create_add_inp.py script - create_add_inp.py - python script that creates ADD.INP using yaml-formatted input file - orbitals.py - python library of functions related to working with orbitals - read_bass.py - python library of functions related to reading BASS.INP - get_excitations.py - python library of functions related to generating excited configurations - get_atomic_data.py - python library of functions scraping atomic data tables online - add.yml - sample yaml input file using Sr - basis.yml - sample yaml input file using Ac - moved sort.py to new py-lib folder ## [0.10.6] - 2022-07-21 - tm v3.2 - addition of transition rates to E1, E2, M1 tables - dtm.f90 - removed cases E_i -> E_i from M1, E2 tables - dtm.f90 - new variables wl for wavelengths and tr for transition rates ## [0.10.5] - 2022-07-20 - ine v1.15 - bug fix for E2 polarizabilities - ine.f90 - added missing allocation statement in Ort subroutine - ine.f90 - removed calculation for negative parity for E2 polarizabilities - ine.f90 - remove xlamb_next from last iteration ## [0.10.4] - 2022-07-10 - dm v3.0 - implemented new tables for g-factors, A_hfs and B_hfs - dtm.f90 - rewrote SetKeys subroutine to open files depending on dm/tm and which keys are used - dtm.f90 - broadcasting of Iarr now uses mpi_utils subroutine BroadcastI - dtm.f90 - added code to get terms from CONFSTR.RES - dtm.f90 - added code to write tables for g-factors, A_hfs and B_hfs in DM regime ## [0.10.3] - 2022-07-07 - conf v5.6 - bug fixes and CONFFINAL.RES table changes - conf.f90 - CONF.ENG and CONF.LVL are now written correctly from a new file - conf.f90 - increased allocation of weight strings by factor of 5 - conf.f90 - change greek symbol delta in tables to DEL and energies EV to E_n (a.u.) in CONFFINAL.RES - conf_init.f90 - bug fix for reading blank lines between configurations from result of merge_ci or con_cut ## [0.10.2] - 2022-07-01 - conf v5.5 - bug fixes - conf.f90 - removed unused variables and arrays - conf.f90 - changed argument of WriteMatrix to be Type(Matrix) instead of 3 arrays - conf.f90 - added EXTERNAL declarations for BLACS_GRIDEXIT, BLACS_EXIT - conf.f90 - removed temporary Jsq fixes - formj2.f90 - added support for when core 0 obtains no non-zero matrix elements from FormJ - formj2.f90 - changed argument of WriteMatrix to be Type(Matrix) instead of 3 arrays - matrix_io.f90 - changed argument of WriteMatrix to be Type(Matrix) instead of 3 arrays - ine.f90 - removed unused variables - add.f90 - removed unused variables - mpi_utils.f90 - added EXTERNAL declaration for GetEnv ## [0.10.1] - 2022-06-30 - tm v3.1 - bug fixes - dtm.f90 - added conditionals to track terms if range does not start from level 1 - dtm.f90 - removed unused variables and arrays - dtm.f90 - increased format digits for NSP to 6 - dtm.f90 - broadcasting of Iarr now uses mpi_utils subroutine BroadcastI ## [0.10.0] - 2022-06-29 - tm v3.0 - implemented new tables for improved data analysis - dtm.f90: new type Key to encapsulate all keys for matrix elements - dtm.f90: new subroutines SetKeys and CloseKeys to open and close files for writing individual matrix element tables - dtm.f90: TM part now reads in 2 ranges: nterm1 to nterm1f and nterm2 to nterm2f - dtm.f90: dtm.in reads list of matrix elements starting from 3rd line: 'E1', 'E1_L', 'E1_V', 'E2', 'E3', 'M1', 'M2', 'M3', 'EDM', 'PNC', 'MQM', 'AM' - dtm.f90: added missing Read(Nsu) part for CONF.DET - dtm.f90: removed progress report - dtm.f90: added broadcast of new global variable nterm1f - dtm.f90: added code to read configurations and terms from CONFSTR.RES - dtm.f90: added loop over first range of energy levels - dtm.f90: moved reading of wavefunctions to inside loop over first range of energy levels - dtm.f90: added code to write tables of matrix elements ## [0.9.6] - 2022-06-16 - combined dtm_aux.f90 with dtm.f90 ## [0.9.5] - 2022-06-16 - conf v5.4 - implemented writing of weights of configurations when CONF.XIJ is written - conf.f90: new subroutine PrintWeightsDvdsn writes weights of configurations for each energy level to CONF.LVL every kXIJ iterations ## [0.9.4] - 2022-05-27 - ine v1.14 - bug fix for kl>0 where required arrays were not allocated ## [0.9.3] - 2022-05-26 - conf v5.3 - implemented writing of top weights of configurations to end of CONFLEVELS.RES - conf.f90: wrote code to count the top configurations in CONFLEVELS.RES and list them in weighted order - conf.f90: new Type(WeightTable) stores data for top weights - conf.f90: moved energies in EV and Delta(cm^-1) to after term if LSJ is used in CONFFINAL.RES - conf.f90: added a significant figure to gfactors - conf.f90: gfactors are no longer written if J=0 ## [0.9.2] - 2022-05-19 - conf v5.2 - implemented writing of terms if LSJ is used - conf.f90: separated writing of files from calculation of weights - conf.f90: changed Wsave, Wpsave, strcsave to dimension(nconfs,Nlv) - conf.f90: new function term to write term based on LSJ values - conf.f90: changed formatting of CONFFINAL.RES table to include term for LSJ - conf.f90: fixed formatting for different size configuration strings in CONFFINAL.RES table ## [0.9.1] - 2022-05-14 - conf v5.1 - implemented writing of energies every N davidson iteration - conf_init.f90: fixed out-of-bound error for array Nrnrc in subroutine ReadConfigurations - conf.f90: new subroutine PrintEnergiesDvdsn that writes table of energies to file CONF.ENG whenever CONF.XIJ is written - conf.f90: updated all mpi subroutines to use mpi_f08 - conf.f90: fixed out-of-bound error for array Wsave, Wpsave, strcsave in subroutine PrintWeights - conf.f90: updated CONFLEVELS.RES file to print energies with more digits ## [0.9.0] - 2022-05-13 - conf v5.0 - implemented ikarus and conf_lsj into conf - new energy table with weights (and optionally, LSJ) are written to file CONFFINAL.RES - list of top contributing configurations to each energy level are written to file CONFLEVELS.RES - conf_variables.f90: global variable kLSJ added to allow user to decide whether L, S, J are printed - conf_variables.f90: global variable Nnr added to count number of non-relativistic configurations required by subroutine PrintWeights - conf_variables.f90: global array Nrnrc added to store number of relativistic configurations in each non-relativistic configuration - determinants.f90: deallocation statements for Nq and Nip have been removed for subsequent LSJ calculations - determinants.f90: subroutine Rdet has been revamped to take in the name of the file to read determinants Iarr from - conf_init.f90: added code in subroutine ReadConfigurations to count number of non-rel. configurations Nnr and number of rel. configurations in each nr configuration Nrnrc - formj2.f90: deallocation statements for Jz and Nh have been removed for subsequent LSJ calculations - conf.f90: added code to calculate LSJ if key kLSJ = 1 - conf.f90: removed deallocation statements for Nvc and Nc0 in subroutine DeAllocateFormHArrays - conf.f90: new subroutine AllocateLSJArrays and InitLSJ added to allocate and initialize arrays required to calculate LSJ - conf.f90: new subroutines lsj, calcLSJ, lsj_det, plus_s, plus_l, p0_s, p0_l, and plus_j added to calculate LSJ - conf.f90: new array W2 added to store weights of non-relativistic configurations - conf.f90: new array Wsave and Wpsave added to store the top weights and indices of top weights - conf.f90: new code written to write a table of main configuration, S, L, J, gfactor, energy in eV, energy from ground state in cm-1, weight% for main configuration, secondary configuration and secondary weight if the weight for main configuration is < 70%, for each energy level - conf.f90: new code written to write list of top weights and configurations for each energy level - conf.f90: new function g_factor to calculate g-factors given L, S, J ## [0.8.6] - 2022-05-19 - conf v4.5 - bug fix for seg faults when writing intermediate CONF.XIJ caused by deallocation of Jsq after LAPACK subroutines ## [0.8.5] - 2022-04-04 - added serial modernized add program ## [0.8.4] - 2022-04-04 - conf v4.4 - merged Kl=3 and Kl=4 routines to single block - added exception for when no additional configurations were added with key Kl=3 - added additional timing tests to file CONF.PRG ## [0.8.3] - 2022-03-28 - conf v4.3 - bug fixes for ArrB vectorization - added determinant number in cntarray ## [0.8.2] - 2022-03-01 - conf v4.2 - changed BroadcastD to MPI_AllReduce ## [0.8.1] - 2022-02-04 - conf v4.1 - implementation of parameterized derived type for matrices to switch between single and double precision - added minimum value of matrix in type Matrix ## [0.8.0] - 2022-01-21 - conf v4.0 - implementation of Kl=3 - additional configurations - added global variables Nc_prev and Nd_prev to conf_variables.f90 - added subroutines IVAccumultorContinue and RVAccumulatorContinue to vaccumulator.f90 - added condition for Kl=3 to cycle when constructing initial approximation - added new conditional under FormH for Kl=3 - changed arguments of ReadMatrix and WriteMatrix to take in three 1d arrays instead of a matrix - added conditional to read and write previous Nc and Nd in subroutines ReadMatrix and WriteMatrix ## [0.7.56] - 2022-02-08 - ine v1.13 - increased limit of NumJ from integer to int*8 ## [0.7.56] - 2022-02-04 - ine v1.12 - increased limit of NumH from integer to int*8 ## [0.7.55] - 2022-01-13 - conf v3.46 - bug fix for Jsq%n for Kv=3 ## [0.7.54] - 2022-01-11 - ine v1.11 - ine now accepts a list of wavelengths to be entered with different step sizes - formatting of ine input file is now: Kl, Kli, Klf, N0, N2, nlambda, (xlambda1, xlambda2, xlambdastep), ... ## [0.7.53] - 2021-12-27 - ine v1.10 - small components of J-decomposition are no longer put to 0 ## [0.7.52] - 2021-11-30 - changed type of vLen, vSize, vGrowBy in vaccumulator.f90 to int64 ## [0.7.51] - 2021-11-26 - conf v3.45 - updated Rint/RintS errors if IPx was changed - split memCalcReqs FormH message into static arrays and FormH arrays - removed FormH memory from comparison stage of FormH - added memory summary after comparison stage of FormH ## [0.7.50] - 2021-11-24 - conf v3.44 - updated memory routines for FormH ## [0.7.49] - 2021-11-22 - conf v3.43 - bug fix for temporary fix for Jsq%n changing during LAPACK ZSYEV subroutine ## [0.7.48] - 2021-11-10 - conf v3.42 - fixed integer overflow in mpi_utils subroutines - increased data size of count_remain and i to integer(kind=int64) - mpi_utils subroutine now take in int*8 argument for count ## [0.7.47] - 2021-11-10 - conf v3.41 - bug fix for mpi_utils subroutine BroadcastI for extra large integer arrays ## [0.7.46] - 2021-11-1 - ine v1.9 - fix for overflow in final result of RdcE1 ## [0.7.45] - 2021-10-31 - ine v1.8 - increased number of significant figures in final result in RdcE1 ## [0.7.44] - 2021-10-31 - ine v1.7 - fixed formatting error in RdcE1 ## [0.7.43] - 2021-10-31 - conf v3.40 - added if allocated statement when deallocating arrays after Diag4 ## [0.7.42] - 2021-10-30 - conf v3.39 - bug fix in conf resolving slow down of convergence during Davidson procedure ## [0.7.41] - 2021-10-27 - conf v3.38 - bug fix in conf when reading old version of CONF.GNT ## [0.7.40] - 2021-10-27 - conf_lsj v2.3 - added g-factors to final table ## [0.7.39] - 2021-10-26 - conf v3.37 - re-implemented Jeff's Broadcast subroutines for MPI_Bcast calls for 2d arrays ## [0.7.38] - 2021-10-25 - conf v3.36 - added memory check after allocating arrays for Davidson procedure ## [0.7.37] - 2021-10-24 - conf v3.35 - added exception to set Ngaunt if not in CONF.GNT - added temporary fix for value of Jsq%n changing during LAPACK ZSYEV subroutine - all programs - removed all unused variables and arrays determined by compiler flag \"--check all --warn all\" ## [0.7.36] - 2021-10-20 - conf v3.34 - removed reading of CONF.XIJ from PrintEnergies subroutine - moved the printing of weights from PrintEnergies subroutine to a new PrintWeights subroutine - cleaned up formatting of PrintEnergies and PrintWeight subroutine ## [0.7.35] - 2021-10-19 - dtm v2.1 - bug fix for Kout /= 0 ## [0.7.34] - 2021-10-13 - conf v3.33 - added calculation of J for energy levels if writing CONF.XIJ file ## [0.7.33] - 2021-10-13 - removed dead code 'testwigner' used for testing from ine ## [0.7.32] - 2021-10-13 - global change: replaced parameter IPgnt with global variable Ngaunt read in from CONF.GNT - basc v2.1 - number of tabulated gaunts Ngaunt is now written to CONF.GNT and read in by subsequent codes - conf v3.32 - reads in Ngaunt from CONF.GNT - conf_lsj v2.2 - reads in Ngaunt from CONF.GNT - conf_pt v2.1 - reads in Ngaunt from CONF.GNT ## [0.7.31] - 2021-10-12 - conf v0.3.31 - moved Bcast of ax to end of loop ## [0.7.30] - 2021-10-12 - conf v0.3.30 - added conditional to optimize construction of initial approximation depending on Kl ## [0.7.29] - 2021-10-12 - conf v0.3.29 - fixed print message for number of conf-s in starting approximation ## [0.7.28] - 2021-10-10 - conf v0.3.28 - added mpi error handling for WriteMatrix and ReadMatrix subroutines ## [0.7.27] - 2021-10-08 - cleaned up dead code in some conf subroutines ## [0.7.26] - 2021-10-07 - conf v0.3.27 - moved averaging of diagonal over configurations to its own subroutine AvgDiag ## [0.7.25] - 2021-10-07 - conf v0.3.26 - moved initial diagonalization to its own subroutine DiagInitApprox ## [0.7.24] - 2021-10-07 - conf v0.3.25 - implemented ScaLAPACK subroutine PDSYEVD to replace DSYEV in initial diagonalization ## [0.7.23] - 2021-10-07 - conf v0.3.24 - reduced size of arrays E and Iconverge from IPlv to Nlv ## [0.7.22] - 2021-10-06 - conf v0.3.23 - replaced all Hould routines with LAPACK 'DSYEV' - removed global D, D1 work arrays and replaced with local W work array - initialize work array for diagonalizing P before iterative procedure - bug fix for Kl4=2 - broadcast Nlv in FormB0 after reading CONF.XIJ ## [0.7.21] - 2021-10-06 - conf v0.3.22 - optimized DSYEV subroutine in Diag4 ## [0.7.20] - 2021-10-04 - conf v0.3.21 - bug fix for Kv=3 - added MPI_Bcast of Z1 after diagonalization - more code documentation in Davidson procedure - refactored parts of Davidson procedure ## [0.7.19] - 2021-10-04 - conf v0.3.20 - changed MPI_Reduce of Z1 in Init4 to MPI_AllReduce, removing MPI_Bcast from FormB0 ## [0.7.18] - 2021-10-01 - conf v0.3.19 - subroutine ReadMatrix now distributes total number of matrix elements equally across cores ## [0.7.17] - 2021-09-29 - conf v0.3.18 - bug fix for writing/reading CONF.HIJ/CONF.JJJ for large scale runs - changed disp and disps in matrix_io.f90 to kind=MPI_OFFSET_KIND - added error message if matrix file doesn't exist ## [0.7.16] - 2021-09-29 - conf v0.3.17 - changed formatting, increasing range for Nc4 and Nd0 for initial approximation ## [0.7.15] - 2021-09-29 - conf v0.3.16 - replaced Hould subroutine during initial diagonalization with LAPACK 'dsyev' subroutine - removed redundant MPI_Bcast call of D1=Tk in subroutine WriteFinalXIj ## [0.7.14] - 2021-09-28 - conf v0.3.15 - added timing for initial diagonalization ## [0.7.13] - 2021-09-26 - conf_lsj v0.2.1 - include comparison of rel. conf-s after comparison of non-rel. confs ## [0.7.12] - 2021-09-26 - ine v1.6 - added new subroutine SetParams to set parameters for job and arrays - added new key kIters=(0-iterate and invert if diverged, 2-invert only) ## [0.7.11] - 2021-09-26 - ine v1.5 - removed IP1 dependency from params.f90 and allows it to be set it in subroutine Input ## [0.7.10] - 2021-09-26 - ine v1.4 - revamped input for range of wavelengths to accept single wavelength by inputting (wavelength1 wavelength1 x) as range - added several code documentation for ine ## [0.7.9] - 2021-09-25 - minor text fix: total computation time of conf_lsj ## [0.7.8] - 2021-09-25 - conf_lsj v0.2.0 - major revamp and optimization of lsj routine - relevant eigenvectors are all stored in memory in array B1h(Nd,nlvs) - one-electron matrix elements of l, s and j are stored in arrays instead of calculated every iteration - in the inner loop of lsj, conf-s are skipped if they belong to different non-rel. conf-s (using new subroutine CompNRC) - selection rules are added to skip zero contributions - many-electron matrix elements are calculated only once and used for all levels ## [0.7.7] - 2021-09-25 - conf v0.3.14 - key Kw revamped to be an input parameter along with Kl, Ksig and Kdsig ## [0.7.6] - 2021-09-25 - minor text fix for ine - moved subroutines from ine_aux.f90 to ine.f90 ## [0.7.5] - 2021-09-25 - ine now reads a range of wavelengths along with step instead of a single wavelength ## [0.7.4] - 2021-09-23 - conf v0.3.13 - bug fix for MPI_AllReduce to same buffer with large number of cores ## [0.7.3] - 2021-09-22 - conf_lsj v0.1.2 - bug fix for conf_lsj to read correct eigenvectors ## [0.7.2] - 2021-09-22 - conf_lsj v0.1.1 - conf_lsj now asks for 2 input parameters rec1 and rec2 to select range of desired levels ## [0.7.1] - 2021-09-21 - basc has been parallelized with MPI using simple static workload distribution - minor refactoring and text revisions ## [0.6.12] - 2021-09-18 - removed writing/reading of processors from HIJ and JJJ files - added more code documentation - initialization of global variables moved to module - revamped F_J2 function to remove redundant comparisons by Rspq ## [0.6.11] - 2021-09-16 - implemented dynamic workload scheduling in conf_lsj - added completion time message to ine - added several timing messages to conf_lsj ## [0.6.10] - 2021-09-15 - bug fix in conf - fixed energies for case of Kl4 = 2 - added more code documentation to conf ## [0.6.9] - 2021-09-14 - restructed conf program - removed conf_aux module and combined with main conf program - parallelized contruction of initial approximation in conf ## [0.6.8] - 2021-09-08 - bug fix in conf.f90 - Kl4 is now broadcasted ## [0.6.7] - 2021-09-03 - global variable Scr is now allocatable ## [0.6.6] - 2021-09-03 - bug fix in conf.f90 - fixed required memory count display before Davidson procedure ## [0.6.5] - 2021-09-03 - bug fix in ine.f90 - added rounding of Tj0 to prevent errors with NaN ## [0.6.4] - 2021-09-02 - bug fix in ine.f90 - fixed ReadJJJ subroutine to read CONF.JJJ file ## [0.6.3] - 2021-09-01 - bug fixes in matrix_io.f90 - ReadMatrix subroutine now reads nprocs.conf for # processors ## [0.6.2] - 2021-09-01 - bug fixes in \"conf\", allowing use of Kl=1 - reading of CONF.HIJ and CONF.JJJ files in conf ## [0.6.1] - 2021-08-31 - \"conf\" program now writes CONFp.HIJ and CONFp.JJJ instead of CONF.HIJ and CONF.JJJ - \"sort.py\" program converts CONFp.HIJ and CONFp.JJJ into serial CONF.HIJ and CONF.JJJ ## [0.6.0] - 2021-08-30 - Initial import of new modernized \"ine\" prgram - \"ine\" program has been modernized - \"ine\" program now uses intel mkl subroutine 'zspsv' - \"dtm\" program subroutine OpenFS has been updated to conf_pt_breit version - \"conf\" program subroutine WriteMatrix has been updated to write separate file nprocs.conf containing number of processors used in conf calculation - \"sort.py\" program has been updated to read nprocs.conf ## [0.5.0] - 2021-08-09 - Initial import of new parallelized \"conf_lsj\" program - \"conf_lsj\" program has been modernized and parallelized - \"conf_lsj\" program is now included in the build process of the parallel package. - \"conf_lsj\" has been tested with serial version and all results are identical in output files - \"conf_lsj\" now utilizes dynamic memory allocation ## [0.4.0] - 2021-07-29 - Revamp of FormH procedure to use split comparison and calculation stages to save memory - Many small bug fixes regarding memory - Progress bar fixed ## [0.3.6] - 2021-07-14 - Removed usage of MPI windows - suspected memory leak due to pointers not being deallocated - FormH bug fix - initial workload distribution was missing a chunk of determinants ## [0.3.5] - 2021-07-13 - Minor bug fixes and progress text changes ## [0.3.4] - 2021-07-09 - Updated progress report of FormJ to include memory per core ## [0.3.3] - 2021-07-08 - Bug fix for FormH and FormJ subroutines ## [0.3.2] - 2021-07-07 - Updated progress report of FormH and FormJ to include memory progress - Removed print statements of number of elements for each core ## [0.3.1] - 2021-06-30 - Bug fixes in FormH and FormJ new work schedulers - Implemented progress report at increments of 10% for FormH ## [0.3.0] - 2021-06-23 - Major parallelism revamp: FormH and FormJ subroutines now use a work scheduler to distribute workloads - Householder diagonalization parameters changed (tol: -103 to -1021; eps: -24 to -53) ## [0.2.4] - 2021-05-14 - \"Rspq\" bug fix: added diagnostics for dets to make sure orbitals are in correct order ## [0.2.3] - 2021-05-10 - \"matrix_io\" bug fix: compatibility fix for sortJJJ.py and sortHIJ.py - new code \"sort.py\": python code that converts parallel CONF.JJJ or CONF.HIJ to serial format ## [0.2.2] - 2021-04-23 - \"sint1\" in dtm_aux.f90: bug fix ## [0.2.1] - 2021-04-20 - \"mpi_wins\" bug fix: added lines to allocate and broadcast Iarr for zero-cores ## [0.2.0] - 2021-04-09 - initial import of new modernized \"basc\" program - \"basc\" program has been modernized and included in the build process of the parallel package. - \"basc\" has been tested with serial version and all results are identical in output files - \"basc\" now utilizes dynamic memory allocation ## [0.1.0] - 2021-03-28 - kv=3 functionality fully parallelized ## [0.0.5] - 2021-03-23 - new module \"matrix_io\": implements parallel reading and writing of matrices (Hamiltonian and J^2) ## [0.0.4] - 2021-03-22 - new type \"Matrix\": encapsulate indices and values of matrix elements (Hamil and Jsq) - new module \"mpi_wins\": implements creating and closing MPI windows/shared memory for basis set - new module \"mpi_utils\": used for DARWIN's no-ucx variant of intel-2020 to bypass 1GB MPI message limit - used intrinsic function PACK to remove all zero valued matrix elements from Hamiltonian and Jsquared - function PACK results in seg fault if not using 'ulimit -s unlimited' - Fixed discrepancy of NumH and NumJ between serial and parallel versions - reorganized timing calls - removed unused error variables - several minor text edits ## [0.0.3] - 2021-03-21 - dtm now uses conf_init module for reading CONF.INP - revamped dtm's Input subroutine - moved one-electron operator functions to a separate module amp_ops - several minor text edits ## [0.0.2] - 2021-03-20 - dtm now uses determinants module for determinant-based subroutines - made arrays storing configurations (iconf1, iconf2) consistent between all programs - several minor text edits - descriptions added for several subroutines in davidson module ## [0.0.1] - 2021-03-17 Fixed issue with table of J being unordered. ## [0.0.0] - 2021-03-17 Original source import.","title":"Changelog"},{"location":"clocks/","text":"Atomic clocks","title":"Atomic clocks"},{"location":"clocks/#atomic-clocks","text":"","title":"Atomic clocks"},{"location":"contact/","text":"Contact Us","title":"Contact Us"},{"location":"contact/#contact-us","text":"","title":"Contact Us"},{"location":"examples/","text":"Examples","title":"Examples"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"installation/","text":"Installation Required libraries In order to compile pCI the following software libraries and tools are required: Fortran compiler. CMake build tool. MPI library to run on high-performance computing clusters. The codes have only been tested with OpenMPI so far. Python v3.xx Pre-built pCI on UD clusters If working on a UD cluster, a pre-built copy of the latest pCI package can be readily obtainable via VALET: vpkg_require pci This command automatically configures your environment to include all necessary libraries to run pCI: an Intel Fortran compiler and OpenMPI library, as well as latest version of Python to run auxiliary scripts. If using the pre-built pCI distribution, you do not have to compile anything and can ignore the rest of this page. Obtaining the source code Users can download the latest version of the pCI code package from https://github.com/ud-pci/pCI via git From the command line, you can clone the latest version: git clone https://github.com/ud-pci/pCI.git pci Compiling with CMake The codes are built using the 'CMakeLists.txt' files. The following are some example builds on the DARWIN computing cluster. A standard build can be done: $ vpkg_require cmake openmpi/4.1.0:intel-2020 $ git clone https://github.com/ud-pci/pCI.git pci $ cd pci $ mkdir build $ cd build $ FC=mpifort cmake .. : $ make $ make install A Debug build can be done: $ vpkg_require cmake openmpi/4.1.0:intel-2020 $ git clone https://github.com/ud-pci/pCI.git pci $ cd pci $ mkdir build-debug $ cd build-debug $ FC=mpifort cmake -DCMAKE_BUILD_TYPE=Debug -DCMAKE_INSTALL_PREFIX=$(pwd)/../20200402-debug .. : $ make $ make install An optimized build demands a little more: $ vpkg_require cmake openmpi/4.1.0:intel-2020 $ git clone https://github.com/ud-pci/pCI.git pci $ cd pci $ mkdir build-opt $ cd build-opt $ FC=mpifort cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=$(pwd)/../20200317-opt -DCMAKE_Fortran_FLAGS_RELEASE=\"-g -O3 -mcmodel=large -xHost -m64\" .. : $ make $ make install","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#required-libraries","text":"In order to compile pCI the following software libraries and tools are required: Fortran compiler. CMake build tool. MPI library to run on high-performance computing clusters. The codes have only been tested with OpenMPI so far. Python v3.xx","title":"Required libraries"},{"location":"installation/#pre-built-pci-on-ud-clusters","text":"If working on a UD cluster, a pre-built copy of the latest pCI package can be readily obtainable via VALET: vpkg_require pci This command automatically configures your environment to include all necessary libraries to run pCI: an Intel Fortran compiler and OpenMPI library, as well as latest version of Python to run auxiliary scripts. If using the pre-built pCI distribution, you do not have to compile anything and can ignore the rest of this page.","title":"Pre-built pCI on UD clusters"},{"location":"installation/#obtaining-the-source-code","text":"Users can download the latest version of the pCI code package from https://github.com/ud-pci/pCI","title":"Obtaining the source code"},{"location":"installation/#via-git","text":"From the command line, you can clone the latest version: git clone https://github.com/ud-pci/pCI.git pci","title":"via git"},{"location":"installation/#compiling-with-cmake","text":"The codes are built using the 'CMakeLists.txt' files. The following are some example builds on the DARWIN computing cluster. A standard build can be done: $ vpkg_require cmake openmpi/4.1.0:intel-2020 $ git clone https://github.com/ud-pci/pCI.git pci $ cd pci $ mkdir build $ cd build $ FC=mpifort cmake .. : $ make $ make install A Debug build can be done: $ vpkg_require cmake openmpi/4.1.0:intel-2020 $ git clone https://github.com/ud-pci/pCI.git pci $ cd pci $ mkdir build-debug $ cd build-debug $ FC=mpifort cmake -DCMAKE_BUILD_TYPE=Debug -DCMAKE_INSTALL_PREFIX=$(pwd)/../20200402-debug .. : $ make $ make install An optimized build demands a little more: $ vpkg_require cmake openmpi/4.1.0:intel-2020 $ git clone https://github.com/ud-pci/pCI.git pci $ cd pci $ mkdir build-opt $ cd build-opt $ FC=mpifort cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=$(pwd)/../20200317-opt -DCMAKE_Fortran_FLAGS_RELEASE=\"-g -O3 -mcmodel=large -xHost -m64\" .. : $ make $ make install","title":"Compiling with CMake"},{"location":"main/","text":"Overview of the pCI code package Figure. pCI code scheme hfd - hartree-fock-dirac The hfd program solves restricted Hartree-Fock-Dirac (HFD) equations self-consistently under the central field approximation to find four-component Dirac-Fock (DF) orbitals and eigenvalues of the HFD Hamiltonian. The program provides the initial approximation, storing both basis radial orbitals \\phi_{nlj}\\equiv r\\left(\\begin{array}{c}f_{nlj}\\\\-g_{nlj}\\end{array}\\right), as well as the radial derivatives of the orbitals \\partial_r\\phi_{nlj} , to the file HFD.DAT . bass - constructing the basis set The bass program forms the DF orbitals for the core and valence shells, then adds virtual orbitals to account for correlations. A reasonable basis set should consist of orbitals mainly localized at the same distances from the origin as the valence orbitals. add - creating the configuration list The add program constructs a list of configurations to define the CI space by exciting electrons from a set of reference configurations to a set of active non-relativistic shells. It takes in the input file ADD.INP , which specifies the reference configurations, active non-relativistic shells, and minimum and maximum occupation numbers of each shell. It writes the file CONF.INP , which includes a list of user-defined parameters and the list of configurations constructed by exciting electrons from a list of basic configurations. The following is a sample input ADD.INP file. Each line has a description of the respective variable. The third block starting with 4f 9 14 is a list of the orbitals and minimum and maximum occupation numbers. For example, 4f 9 14 refers to having a minimum of 9 electrons or a maximum of 14 electrons for the 4f orbital. Ncor= 4 !# number of basic configurations. Must match the list below. NsvNR 16 !# number of active NR shells. The list below may be longer. mult= 2 !# multiplicity of excitations. For full CI use mult=Ne NE = 14 !# number of valence electrons L: 4f14 !# list of basic configurations L: 4f13 5p1 !# from which electrons are excited from. L: 4f12 5s2 !# the number of configurations listed here L: 4f11 5s2 5p1 !# must match the number on the first line 'Ncor= 4' ## nnlee nnlee nnlee !# formatting of configurations !# the numbers nn refer to the principal quantum number !# the letters l refer to the angular momentum quantum number !# the numbers ee refer to the occupation of that orbital 4f 9 14 5s 0 2 5p 0 3 5d 0 2 5f 0 2 5g 0 2 6s 0 2 6p 0 2 6d 0 2 6f 0 2 6g 0 2 7s 0 2 7p 0 2 7d 0 2 7f 0 2 7g 0 2 ##nnl ee ee nnl ee ee nnl ee ee nnl ee ee nnl ee ee nnl ee ee >>>>>>>>>>>>> Head of the file CONF.INP >>>>>>>>>>>>>>>>>>>>>>>> Ir17+_even # ion_parity Z = 77.0 # atomic number Am = 192.0 # atomic weight J = 4.0 # total angular momentum Jm = 4.0 # angular momentum projection Nso= 14 # number of closed core shells Nc = 10 # number of relativistic configurations (ignored in add program) Kv = 4 # Kv = (3 - use projections, 4 - no projections) Nlv= 5 # number of energy levels Ne = 14 # number of valence electrons Kl4= 1 # Kl4 = (1 - initial approx. from energy matrix, 2 - initial approx. from CONF.XIJ file) Nc4= 20 # number of relativistic configurations in initial approximation Gj = 0.0000 # Crt4= 0.0001 # cutoff criteria for davidson convergence kout= 0 # key for level of output (0 - low detail output, 1 - detailed output) Ncpt= 0 # number of relativistic configurations in PT block (ignored in add program) Cut0= 0.0001 # cutoff criteria for weights of PT configurations N_it= 100 # number of davidson iterations Kbrt= 1 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) Gnuc= 1.07 # gyromagnetic ratio 0.1002 0.2002 -0.2102 0.2104 0.3002 -0.3102 0.3104 -0.3204 0.3206 0.4002 -0.4102 0.4104 -0.4204 0.4206 ================================================================== Note The second block listing the basic configurations has a specific formatting __nnlee__ , where __ indicate spaces, nn is the principal quantum number, l is the angular momentum quantum number as a letter ( s=0 , l=1 , d=2 , ...), and ee is the number of electrons in that orbital. Note The order in which the configurations and basis orbitals must be listed identically with those from BASS.INP . basc - calculating radial integrals After the configuration list has been created, the next step is to calculate the radial integrals using the program basc . basc calculates one-electron and two-electron radial integrals, which are used by the conf program to form the Hamiltonian in the CI space. The one-electron radial integrals correspond to the DF potential of the core, and the two-electron radial integrals account for the Coulomb and Breit interactions between the valence electrons. The matrix elements of the Coulomb interaction for the multipolarity k can be written as \\langle c,d|V_q^k|a,b\\rangle \\equiv G_q^k(ca) G_q^k(bd) R_{abcd}^k, where the angular factors G_q^k(fi) (known as relativistic Gaunt coefficients) are given by G_q^k(fi)=(-1)^{m_f+1/2}\\delta_p\\sqrt{(2j_i+1)(2j_f+1)} \\begin{pmatrix} j_f & j_i & k \\\\ -m_f & m_i & q \\end{pmatrix} \\begin{pmatrix} j_f & j_i & k \\\\ 1/2 & -1/2 & 0 \\end{pmatrix}, and R_{abcd}^k are the relativistic Coulomb radial integrals, and \\delta_p accounts for the parity selection rule \\delta_p=\\xi(l_i+l_f+k), \\hspace{0.2in}\\xi(n)=\\Bigg\\{ \\begin{matrix} 1 & \\text{if \\( n \\) is even,} \\\\ 0 & \\text{if \\( n \\) is odd.} \\end{matrix} The Breit interaction has the same form as the Coulomb interaction, but without the parity selection rule. The basc reads in the files HFD.DAT and CONF.INP to determine which radial integrals are needed. These integrals are calculated and written to the files CONF.INT . The relativistic Gaunt coefficients are written to the file CONF.GNT , and the file CONF.DAT is also formed, storing the basis radial orbitals \\phi_{nlj} , as well as functions \\chi_{nlj} = h_\\text{DF}^r\\phi_{nlj} . conf - configuration interaction In this section, we will introduce how to run the CI program conf . Here is a summary of the input and output files used in conf . Input Files: HFD.DAT - basis set radial orbitals \\phi_{nlj} and radial derivatives of the orbitals \\partial_r\\phi_{nlj} CONF.DAT - basis set radial orbitals \\phi_{nlj} and functions \\chi_{nlj}=h_\\text{HF}^r\\phi_{nlj} , where h_\\text{HF}^r is the radial part of the Dirac-Fock operator CONF.GNT - relativistic Gaunt coefficients produced by basc CONF.INT - relativistic Coulomb coefficients produced by basc SGC.CON (optional) - one-electron effective radial integrals of the MBPT/all-order corrections SCRC.CON (optional) - two-electron effective radial integrals of the MBPT/all-order corrections CONF.INP - list of relativistic configurations and user defined parameters c.in - input file with keys Kl , Ksig , and Kdsig Kl = (0 - start, 1 - continue calculation, 2 - include corrections, 3 - add configurations) Ksig = (0 - pure CI, 1 - include one-electron corrections, 2 - include one- and two-electron corrections) Kdsig = (0 - no energy dependence on Sigma, 1 - energy dependence on Sigma) Output Files: CONF.DET - basis set of determinants CONF.HIJ - indices and values of the Hamiltonian matrix elements CONF.JJJ - indices and values of the matrix elements of the operator J^2 CONF.XIJ - quantum numbers, eigenvalues and eigenvectors of the Hamiltonian CONF.RES - final table of energy eigenvalues and the weights of all configurations contributing to each term The following is a sample of the head of a CONF.INP for calculating the even-parity states of Ir ^{17+} . Ir17+_even # ion_parity Z = 77.0 # atomic number Am =193.0 # atomic weight J = 2.0 # total angular momentum Jm = 2.0 # angular momentum projection Nso= 14 # number of closed core shells Nc = 481 # number of relativistic configurations Kv = 4 # Kv = (3 - use projections, 4 - no projections) Nlv= 5 # number of energy levels Ne = 14 # number of valence electrons Kl4= 1 # Kl4 = (1 - initial approx. from energy matrix, 2 - initial approx. from CONF.XIJ file) Nc4= 28 # number of relativistic configurations in initial approximation Gj = 0.0000 # Crt4= 0.0001 # cutoff criteria for davidson convergence kout= 0 # key for level of output (0 - low detail output, 1 - detailed output) Ncpt= 1132 # number of relativistic configurations in PT block (used in conf_pt) Cut0= 0.0001 # cutoff criteria for davidson convergence N_it= 100 # number of davidson iterations Kbrt= 1 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) Gnuc= 1.07 # gyromagnetic ratio 0.1002 0.2002 -0.2102 0.2104 0.3002 -0.3102 0.3104 -0.3204 0.3206 0.4002 -0.4102 0.4104 -0.4204 0.4206 1 1-0.4306 0.4308 2 2-0.4306 0.4306 0.5002 3-0.4305 0.4307 0.5002 4-0.4304 0.4308 0.5002 3 5-0.4305 0.4308 -0.5101 6-0.4305 0.4308 0.5101 7-0.4306 0.4307 -0.5101 8-0.4306 0.4307 0.5101 : Note The first 5 columns up to the '=' sign are fixed, and the program will give an error if there are any discrepancies here. Note The list of core shells are fixed to have a maximum of 6 shells per row. In the CONF.INP file shown above, we include 481 relativistic configurations in the CI space, and 1132 relativistic configurations in the PT space (if conf_pt is to be used after conf ). dtm - density transition matrix The dtm program calculates matrix elements of one-electron operators between many-electron states, under the density (or transition) matrix formalism. This formalism allows us to express the matrix elements between many-electron states via one-electron matrix elements. The dtm program forms these reduced density (or transition) matrices and calculates the reduced matrix elements. The following quantities can be calculated from this program: - electron g-factors - magnetic dipole and electronic quadrupole hyperfine structure constants A and B - electric ( Ek ) and magnetic ( Mk ) multipole transition amplitudes, where k = 1,2,3 corresponds to the dipole, quadrupole, and octupole transitions - nuclear spin independent parity nonconserving (PNC) amplitude - amplitude of the electron interaction with the P-odd nuclear anapole moment (AM) - P, T-odd interaction of the dipole electric dipole moment - nucleus magnetic quadrupole moment This program begins by reading the file CONF.INP for system parameters and the list of configurations. Next, basis radial orbitals are read from the file CONF.DAT , and radial integrals for all operators are calculated and written to the file DTM.INT . If this file already exists, dtm uses it and does not recalculate the radial integrals. For the diagonal matrix elements, the list of determinants and eigenvectors corresponding to the state of interest are read from the files CONF.DET and CONF.XIJ , respectively. For the non-diagonal matrix elements, the initial state is read from the file CONF.DET and CONF.XIJ , and the final state is read from the files CONF1.DET and CONF1.XIJ . The results of the diagonal and non-diagonal matrix elements are written to the files DM.RES and TM.RES , respectively. dtm takes in as input the input file dtm.in : 2 # 1 for DM (as g-factor or hyperfine), 2 for transitions 1 1 12 # from 1st even level to 1st-12th odd levels Note Some 3J -coefficients might be zero in some cases, such as trying to compute E1 matrix element for 5s^2\\, {}^1S_0 \\rightarrow 5s5p\\,{}^3P_1 . This will fail if the odd run had J=0 , M_J=0 . You would need to have an odd run with J=1 , M_J=1 ine - polarizabilities The ine program calculates static and dynamic polarizabilities of specified atomic levels. ine only gives the valence polarizability. Core polarizability needs to be computed separately with a different code. The code will give both scalar and tensor polarizabilities if the tensor polarizability is not zero, but not the vector polarizability. There are several version of the code, but for now we will use ine_dyn_E28 . This program requires several input files from previously ran conf and dtm programs, including CONF.DET and CONF.XIJ of the parity of the level of interest (renamed to CONF0.DET and CONF0.XIJ ), CONF.INP , CONF.XIJ , CONF.HIJ , and CONF.JJJ of the opposite parity, and the file DTM.INT from dtm . For example, if we want to calculate polarizabilities for an even state: cp CONFeven.DET CONF0.DET cp CONFeven.XIJ CONF0.XIJ cp CONFodd.INP CONF.INP cp CONFodd.XIJ CONF.XIJ cp CONFodd.HIJ CONF.HIJ cp CONFodd.JJJ CONF.JJJ ine_dyn_E28 can either solve the inhomogeneous equation iteratively by solving for a smaller matrix first, or by direct matrix inversion via the LAPACK library. It is controlled by the parameter IP1 in conf.par : PARAMETER(IP1 = 15000, ! Nd1 - number of determinants for direct diagonalization This parameter can be set to be larger than the number of determinants in your problem if you don't want the program to iterate at all. The problem with iterations is that they diverge in many cases for dynamic polarizabilities. However, the problem with the direct solution is that it takes a long time to run (about 20-30 min even for 10, 000 determinants). The program can be executed via the command: ./ine_dyn_E28 <inf.dtm Click here to see a description of inf.dtm . 0 # start new computation 1 # calculate polarizability of the first level 0 # 0 for static, omega for dynamic For dynamic polarizability, you need to run ine_dyn_E28 twice: once with +\\omega and once with -\\omega . For example, if one needs the polarizability for \\lambda=800 \\text{ nm} , compute \\omega in a.u.: \\omega=1.0\\times 10^7 / (au\\times\\lambda) = 0.056954191 , where au=219474.6313705 . Run ine_dyn_E28 twice, once with 0 1 0.056954191 and once with 0 1 -0.056954191 then average the two results.","title":"pCI code package"},{"location":"main/#overview-of-the-pci-code-package","text":"Figure. pCI code scheme","title":"Overview of the pCI code package"},{"location":"main/#hfd-hartree-fock-dirac","text":"The hfd program solves restricted Hartree-Fock-Dirac (HFD) equations self-consistently under the central field approximation to find four-component Dirac-Fock (DF) orbitals and eigenvalues of the HFD Hamiltonian. The program provides the initial approximation, storing both basis radial orbitals \\phi_{nlj}\\equiv r\\left(\\begin{array}{c}f_{nlj}\\\\-g_{nlj}\\end{array}\\right), as well as the radial derivatives of the orbitals \\partial_r\\phi_{nlj} , to the file HFD.DAT .","title":"hfd - hartree-fock-dirac"},{"location":"main/#bass-constructing-the-basis-set","text":"The bass program forms the DF orbitals for the core and valence shells, then adds virtual orbitals to account for correlations. A reasonable basis set should consist of orbitals mainly localized at the same distances from the origin as the valence orbitals.","title":"bass - constructing the basis set"},{"location":"main/#add-creating-the-configuration-list","text":"The add program constructs a list of configurations to define the CI space by exciting electrons from a set of reference configurations to a set of active non-relativistic shells. It takes in the input file ADD.INP , which specifies the reference configurations, active non-relativistic shells, and minimum and maximum occupation numbers of each shell. It writes the file CONF.INP , which includes a list of user-defined parameters and the list of configurations constructed by exciting electrons from a list of basic configurations. The following is a sample input ADD.INP file. Each line has a description of the respective variable. The third block starting with 4f 9 14 is a list of the orbitals and minimum and maximum occupation numbers. For example, 4f 9 14 refers to having a minimum of 9 electrons or a maximum of 14 electrons for the 4f orbital. Ncor= 4 !# number of basic configurations. Must match the list below. NsvNR 16 !# number of active NR shells. The list below may be longer. mult= 2 !# multiplicity of excitations. For full CI use mult=Ne NE = 14 !# number of valence electrons L: 4f14 !# list of basic configurations L: 4f13 5p1 !# from which electrons are excited from. L: 4f12 5s2 !# the number of configurations listed here L: 4f11 5s2 5p1 !# must match the number on the first line 'Ncor= 4' ## nnlee nnlee nnlee !# formatting of configurations !# the numbers nn refer to the principal quantum number !# the letters l refer to the angular momentum quantum number !# the numbers ee refer to the occupation of that orbital 4f 9 14 5s 0 2 5p 0 3 5d 0 2 5f 0 2 5g 0 2 6s 0 2 6p 0 2 6d 0 2 6f 0 2 6g 0 2 7s 0 2 7p 0 2 7d 0 2 7f 0 2 7g 0 2 ##nnl ee ee nnl ee ee nnl ee ee nnl ee ee nnl ee ee nnl ee ee >>>>>>>>>>>>> Head of the file CONF.INP >>>>>>>>>>>>>>>>>>>>>>>> Ir17+_even # ion_parity Z = 77.0 # atomic number Am = 192.0 # atomic weight J = 4.0 # total angular momentum Jm = 4.0 # angular momentum projection Nso= 14 # number of closed core shells Nc = 10 # number of relativistic configurations (ignored in add program) Kv = 4 # Kv = (3 - use projections, 4 - no projections) Nlv= 5 # number of energy levels Ne = 14 # number of valence electrons Kl4= 1 # Kl4 = (1 - initial approx. from energy matrix, 2 - initial approx. from CONF.XIJ file) Nc4= 20 # number of relativistic configurations in initial approximation Gj = 0.0000 # Crt4= 0.0001 # cutoff criteria for davidson convergence kout= 0 # key for level of output (0 - low detail output, 1 - detailed output) Ncpt= 0 # number of relativistic configurations in PT block (ignored in add program) Cut0= 0.0001 # cutoff criteria for weights of PT configurations N_it= 100 # number of davidson iterations Kbrt= 1 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) Gnuc= 1.07 # gyromagnetic ratio 0.1002 0.2002 -0.2102 0.2104 0.3002 -0.3102 0.3104 -0.3204 0.3206 0.4002 -0.4102 0.4104 -0.4204 0.4206 ================================================================== Note The second block listing the basic configurations has a specific formatting __nnlee__ , where __ indicate spaces, nn is the principal quantum number, l is the angular momentum quantum number as a letter ( s=0 , l=1 , d=2 , ...), and ee is the number of electrons in that orbital. Note The order in which the configurations and basis orbitals must be listed identically with those from BASS.INP .","title":"add - creating the configuration list"},{"location":"main/#basc-calculating-radial-integrals","text":"After the configuration list has been created, the next step is to calculate the radial integrals using the program basc . basc calculates one-electron and two-electron radial integrals, which are used by the conf program to form the Hamiltonian in the CI space. The one-electron radial integrals correspond to the DF potential of the core, and the two-electron radial integrals account for the Coulomb and Breit interactions between the valence electrons. The matrix elements of the Coulomb interaction for the multipolarity k can be written as \\langle c,d|V_q^k|a,b\\rangle \\equiv G_q^k(ca) G_q^k(bd) R_{abcd}^k, where the angular factors G_q^k(fi) (known as relativistic Gaunt coefficients) are given by G_q^k(fi)=(-1)^{m_f+1/2}\\delta_p\\sqrt{(2j_i+1)(2j_f+1)} \\begin{pmatrix} j_f & j_i & k \\\\ -m_f & m_i & q \\end{pmatrix} \\begin{pmatrix} j_f & j_i & k \\\\ 1/2 & -1/2 & 0 \\end{pmatrix}, and R_{abcd}^k are the relativistic Coulomb radial integrals, and \\delta_p accounts for the parity selection rule \\delta_p=\\xi(l_i+l_f+k), \\hspace{0.2in}\\xi(n)=\\Bigg\\{ \\begin{matrix} 1 & \\text{if \\( n \\) is even,} \\\\ 0 & \\text{if \\( n \\) is odd.} \\end{matrix} The Breit interaction has the same form as the Coulomb interaction, but without the parity selection rule. The basc reads in the files HFD.DAT and CONF.INP to determine which radial integrals are needed. These integrals are calculated and written to the files CONF.INT . The relativistic Gaunt coefficients are written to the file CONF.GNT , and the file CONF.DAT is also formed, storing the basis radial orbitals \\phi_{nlj} , as well as functions \\chi_{nlj} = h_\\text{DF}^r\\phi_{nlj} .","title":"basc - calculating radial integrals"},{"location":"main/#conf-configuration-interaction","text":"In this section, we will introduce how to run the CI program conf . Here is a summary of the input and output files used in conf . Input Files: HFD.DAT - basis set radial orbitals \\phi_{nlj} and radial derivatives of the orbitals \\partial_r\\phi_{nlj} CONF.DAT - basis set radial orbitals \\phi_{nlj} and functions \\chi_{nlj}=h_\\text{HF}^r\\phi_{nlj} , where h_\\text{HF}^r is the radial part of the Dirac-Fock operator CONF.GNT - relativistic Gaunt coefficients produced by basc CONF.INT - relativistic Coulomb coefficients produced by basc SGC.CON (optional) - one-electron effective radial integrals of the MBPT/all-order corrections SCRC.CON (optional) - two-electron effective radial integrals of the MBPT/all-order corrections CONF.INP - list of relativistic configurations and user defined parameters c.in - input file with keys Kl , Ksig , and Kdsig Kl = (0 - start, 1 - continue calculation, 2 - include corrections, 3 - add configurations) Ksig = (0 - pure CI, 1 - include one-electron corrections, 2 - include one- and two-electron corrections) Kdsig = (0 - no energy dependence on Sigma, 1 - energy dependence on Sigma) Output Files: CONF.DET - basis set of determinants CONF.HIJ - indices and values of the Hamiltonian matrix elements CONF.JJJ - indices and values of the matrix elements of the operator J^2 CONF.XIJ - quantum numbers, eigenvalues and eigenvectors of the Hamiltonian CONF.RES - final table of energy eigenvalues and the weights of all configurations contributing to each term The following is a sample of the head of a CONF.INP for calculating the even-parity states of Ir ^{17+} . Ir17+_even # ion_parity Z = 77.0 # atomic number Am =193.0 # atomic weight J = 2.0 # total angular momentum Jm = 2.0 # angular momentum projection Nso= 14 # number of closed core shells Nc = 481 # number of relativistic configurations Kv = 4 # Kv = (3 - use projections, 4 - no projections) Nlv= 5 # number of energy levels Ne = 14 # number of valence electrons Kl4= 1 # Kl4 = (1 - initial approx. from energy matrix, 2 - initial approx. from CONF.XIJ file) Nc4= 28 # number of relativistic configurations in initial approximation Gj = 0.0000 # Crt4= 0.0001 # cutoff criteria for davidson convergence kout= 0 # key for level of output (0 - low detail output, 1 - detailed output) Ncpt= 1132 # number of relativistic configurations in PT block (used in conf_pt) Cut0= 0.0001 # cutoff criteria for davidson convergence N_it= 100 # number of davidson iterations Kbrt= 1 # key for Breit (0 - Coulomb, 1 - Gaunt, 2 - Full Breit) Gnuc= 1.07 # gyromagnetic ratio 0.1002 0.2002 -0.2102 0.2104 0.3002 -0.3102 0.3104 -0.3204 0.3206 0.4002 -0.4102 0.4104 -0.4204 0.4206 1 1-0.4306 0.4308 2 2-0.4306 0.4306 0.5002 3-0.4305 0.4307 0.5002 4-0.4304 0.4308 0.5002 3 5-0.4305 0.4308 -0.5101 6-0.4305 0.4308 0.5101 7-0.4306 0.4307 -0.5101 8-0.4306 0.4307 0.5101 : Note The first 5 columns up to the '=' sign are fixed, and the program will give an error if there are any discrepancies here. Note The list of core shells are fixed to have a maximum of 6 shells per row. In the CONF.INP file shown above, we include 481 relativistic configurations in the CI space, and 1132 relativistic configurations in the PT space (if conf_pt is to be used after conf ).","title":"conf - configuration interaction"},{"location":"main/#dtm-density-transition-matrix","text":"The dtm program calculates matrix elements of one-electron operators between many-electron states, under the density (or transition) matrix formalism. This formalism allows us to express the matrix elements between many-electron states via one-electron matrix elements. The dtm program forms these reduced density (or transition) matrices and calculates the reduced matrix elements. The following quantities can be calculated from this program: - electron g-factors - magnetic dipole and electronic quadrupole hyperfine structure constants A and B - electric ( Ek ) and magnetic ( Mk ) multipole transition amplitudes, where k = 1,2,3 corresponds to the dipole, quadrupole, and octupole transitions - nuclear spin independent parity nonconserving (PNC) amplitude - amplitude of the electron interaction with the P-odd nuclear anapole moment (AM) - P, T-odd interaction of the dipole electric dipole moment - nucleus magnetic quadrupole moment This program begins by reading the file CONF.INP for system parameters and the list of configurations. Next, basis radial orbitals are read from the file CONF.DAT , and radial integrals for all operators are calculated and written to the file DTM.INT . If this file already exists, dtm uses it and does not recalculate the radial integrals. For the diagonal matrix elements, the list of determinants and eigenvectors corresponding to the state of interest are read from the files CONF.DET and CONF.XIJ , respectively. For the non-diagonal matrix elements, the initial state is read from the file CONF.DET and CONF.XIJ , and the final state is read from the files CONF1.DET and CONF1.XIJ . The results of the diagonal and non-diagonal matrix elements are written to the files DM.RES and TM.RES , respectively. dtm takes in as input the input file dtm.in : 2 # 1 for DM (as g-factor or hyperfine), 2 for transitions 1 1 12 # from 1st even level to 1st-12th odd levels Note Some 3J -coefficients might be zero in some cases, such as trying to compute E1 matrix element for 5s^2\\, {}^1S_0 \\rightarrow 5s5p\\,{}^3P_1 . This will fail if the odd run had J=0 , M_J=0 . You would need to have an odd run with J=1 , M_J=1","title":"dtm - density transition matrix"},{"location":"main/#ine-polarizabilities","text":"The ine program calculates static and dynamic polarizabilities of specified atomic levels. ine only gives the valence polarizability. Core polarizability needs to be computed separately with a different code. The code will give both scalar and tensor polarizabilities if the tensor polarizability is not zero, but not the vector polarizability. There are several version of the code, but for now we will use ine_dyn_E28 . This program requires several input files from previously ran conf and dtm programs, including CONF.DET and CONF.XIJ of the parity of the level of interest (renamed to CONF0.DET and CONF0.XIJ ), CONF.INP , CONF.XIJ , CONF.HIJ , and CONF.JJJ of the opposite parity, and the file DTM.INT from dtm . For example, if we want to calculate polarizabilities for an even state: cp CONFeven.DET CONF0.DET cp CONFeven.XIJ CONF0.XIJ cp CONFodd.INP CONF.INP cp CONFodd.XIJ CONF.XIJ cp CONFodd.HIJ CONF.HIJ cp CONFodd.JJJ CONF.JJJ ine_dyn_E28 can either solve the inhomogeneous equation iteratively by solving for a smaller matrix first, or by direct matrix inversion via the LAPACK library. It is controlled by the parameter IP1 in conf.par : PARAMETER(IP1 = 15000, ! Nd1 - number of determinants for direct diagonalization This parameter can be set to be larger than the number of determinants in your problem if you don't want the program to iterate at all. The problem with iterations is that they diverge in many cases for dynamic polarizabilities. However, the problem with the direct solution is that it takes a long time to run (about 20-30 min even for 10, 000 determinants). The program can be executed via the command: ./ine_dyn_E28 <inf.dtm Click here to see a description of inf.dtm . 0 # start new computation 1 # calculate polarizability of the first level 0 # 0 for static, omega for dynamic For dynamic polarizability, you need to run ine_dyn_E28 twice: once with +\\omega and once with -\\omega . For example, if one needs the polarizability for \\lambda=800 \\text{ nm} , compute \\omega in a.u.: \\omega=1.0\\times 10^7 / (au\\times\\lambda) = 0.056954191 , where au=219474.6313705 . Run ine_dyn_E28 twice, once with 0 1 0.056954191 and once with 0 1 -0.056954191 then average the two results.","title":"ine - polarizabilities"},{"location":"parallel_codes/","text":"Running the parallel codes basc Before you run basc , make sure the following input files have been appropriately generated by hfd , bass , and add : HFD.DAT (obtained from basis) CONF.INP (obtained from add) conf Before you run conf , make sure the following input files have been appropriately generated by hfd , bass , add , and basc : HFD.DAT (obtained from basis) CONF.INP (obtained from add) CONF.DAT (obtained from basc) CONF.GNT (obtained from basc) CONF.INT (obtained from basc) SGC.CON (optionally obtained from all-order) SCRC.CON (optionally obtained from all-order) Next, you must create a file named c.in with the following parameters: Kl ! (0 - start, 1 - continue calculation, 2 - include corrections, 3 - add configurations) Ksig ! (0 - pure CI, 1 - include one-electron corrections, 2 - include one- and two-electron corrections) Kdsig ! (0 - no energy dependence on Sigma, 1 - energy dependence on Sigma) Kw ! (0 - do not write CONF.HIJ, 1 - write CONF.HIJ) kLSJ ! (0 - do not calculate expectation values of S^2 and L^2 and form approximate terms for each energy level, 1 - calculate LSJ) To run parallel conf , run the command: mpirun -n <nprocs> ./conf dtm ine Before parallel ine can be run, the parallel matrix files CONFp.HIJ and CONFp.JJJ have to be sorted. This can be done using the python script /pCI/src/auxiliary/sort.py . First load the python3 vpkg_require python/3 Run python script sort.py python3 sort.py The program will ask which file you want sorted, so enter CONFp.HIJ or CONFp.JJJ CONFp.HIJ Now re-run the program for the other matrix python3 sort.py CONFp.JJJ After these files are obtained, you can run the parallel version of ine . This requires the input file : 0 | Kl = (0-new, 1-use X1, 2-use X1,Y1,Y2) 2 | Kli = (1 - H_p, 2 - E1(L), 5 - E2) for RHS of equation 2 | Klf = (1 - H_p, 2 - E1(L), 3 - H_am, 4 - E1(V), 5 - E2) for LHS of equation 5 | N0 = record number of X0 5 | N2 = record number of X2 3 | nlambda - number of ranges of wavelengths to include (in this case, 3) 813.01 813.02 0.01 | range 1 (lambda1, lambda2, step_size) 813.02 813.025 0.001 | range 2 813.025 813.03 0.01 | range 3","title":"Running the parallel codes"},{"location":"parallel_codes/#running-the-parallel-codes","text":"","title":"Running the parallel codes"},{"location":"parallel_codes/#basc","text":"Before you run basc , make sure the following input files have been appropriately generated by hfd , bass , and add : HFD.DAT (obtained from basis) CONF.INP (obtained from add)","title":"basc"},{"location":"parallel_codes/#conf","text":"Before you run conf , make sure the following input files have been appropriately generated by hfd , bass , add , and basc : HFD.DAT (obtained from basis) CONF.INP (obtained from add) CONF.DAT (obtained from basc) CONF.GNT (obtained from basc) CONF.INT (obtained from basc) SGC.CON (optionally obtained from all-order) SCRC.CON (optionally obtained from all-order) Next, you must create a file named c.in with the following parameters: Kl ! (0 - start, 1 - continue calculation, 2 - include corrections, 3 - add configurations) Ksig ! (0 - pure CI, 1 - include one-electron corrections, 2 - include one- and two-electron corrections) Kdsig ! (0 - no energy dependence on Sigma, 1 - energy dependence on Sigma) Kw ! (0 - do not write CONF.HIJ, 1 - write CONF.HIJ) kLSJ ! (0 - do not calculate expectation values of S^2 and L^2 and form approximate terms for each energy level, 1 - calculate LSJ) To run parallel conf , run the command: mpirun -n <nprocs> ./conf","title":"conf"},{"location":"parallel_codes/#dtm","text":"","title":"dtm"},{"location":"parallel_codes/#ine","text":"Before parallel ine can be run, the parallel matrix files CONFp.HIJ and CONFp.JJJ have to be sorted. This can be done using the python script /pCI/src/auxiliary/sort.py . First load the python3 vpkg_require python/3 Run python script sort.py python3 sort.py The program will ask which file you want sorted, so enter CONFp.HIJ or CONFp.JJJ CONFp.HIJ Now re-run the program for the other matrix python3 sort.py CONFp.JJJ After these files are obtained, you can run the parallel version of ine . This requires the input file : 0 | Kl = (0-new, 1-use X1, 2-use X1,Y1,Y2) 2 | Kli = (1 - H_p, 2 - E1(L), 5 - E2) for RHS of equation 2 | Klf = (1 - H_p, 2 - E1(L), 3 - H_am, 4 - E1(V), 5 - E2) for LHS of equation 5 | N0 = record number of X0 5 | N2 = record number of X2 3 | nlambda - number of ranges of wavelengths to include (in this case, 3) 813.01 813.02 0.01 | range 1 (lambda1, lambda2, step_size) 813.02 813.025 0.001 | range 2 813.025 813.03 0.01 | range 3","title":"ine"},{"location":"pci-py-example/","text":"Example: Sr with pCI-py In this example, we will utilize the pCI-py scripts detailed here to generate atomic data for neutral Sr. config.yml The config.yml file defines the entire calculation from beginning to end. You can find a detailed explanation of the contents of this configuration file here . Click here to see the contents of the sample config.yml we will be using. # General parameters system: name: Sr isotope: include_breit: True # Parameters used by basis programs basis: cavity_radius: 60 orbitals: core: 1s 2s 2p 3s 3p 3d 4s 4p valence: 5s 5p 4d 6s 6p 5d 4f nmax: 35 lmax: 6 b_splines: nmax: 40 lmax: 6 k: 7 val_aov: s: 4 p: 4 d: 4 f: 3 val_energies: kval: 1 energies: s: -0.28000 p: [-0.22000, -0.22000] d: [-0.31000, -0.31000] f: [-0.13000, -0.13000] # Parameters used by add program add: # Lists of even and odd parity reference configurations ref_configs: odd: [5s1 5p1] even: [5s2] basis_set: 22spdfg orbitals: core: 1s 2s 2p 3s 3p 3d 4s 4p active: [ 4-7p: 0 4, 4-7d: 0 4, 4-7f: 0 4, 5-7g: 0 4, ] excitations: single: True double: True triple: False # Parameters used by conf program conf: J: 0.0 JM: 0.0 J_selection: False num_energy_levels: 24 num_dvdsn_iterations: 100 include_lsj: True write_hij: False # Parameters used by dtm program dtm: matrix_elements: E1 include_rpa: True # Parameters used by portal script portal: ignore_g: True # Optional parameters optional: qed: include: False rotate_basis: False isotope_shifts: include: False K_is: 1 C_is: 0.01 code_method: [ci+all-order, ci+second-order] generate_directories: False run_ao_codes: True run_ci: True pci_version: default Running pCI-py scripts Write config.yml to root directory. Make sure to re-define optional.pci_version = default to the correct version if parameters such as the radial grid size has to be changed. On UD computers, one can use the command vpkg_versions pci to list all version of pCI. Make sure to set optional.run_ao_codes = True in order for job scripts to be automatically submitted to SLURM for all-order codes to run. Run basis.py This script will generate basis sets for CI+all-order and CI+MBPT in their respective directories. Input: config.yml (specifically blocks system , basis , optional ) Output: /CI+all-order/basis/ /CI+second-order/basis/ Run add.py Make sure to set optional.run_ci = True in order for job scripts to be automatically submitted to SLURM for conf to run. Input: config.yml (specifically blocks system , basis , conf , optional ) /CI+all-order/basis/BASS.INP /CI+second-order/basis/BASS.INP Output: /CI+all-order/even/CONFFINAL.RES /CI+all-order/odd/CONFFINAL.RES /CI+second-order/even/CONFFINAL.RES /CI+second-order/odd/CONFFINAL.RES Run dtm.py Make sure to set dtm.matrix_elements = E1 so E1.RES , which contains a table of E1 transitions, is formed. Make sure to set dtm.include_rpa = True to include RPA corrections. Input: config.yml (specifically blocks system , conf , dtm ) /CI+all-order/even/ , /CI+all-order/odd/ , /CI+second-order/even/ , /CI+second-order/odd/ CONF.INP CONF.DAT CONF.DET CONF.XIJ CONFSTR.RES Output: /CI+all-order/dtm/ , /CI+second-order/dtm/ TM.RES E1.RES Run gen_portal_csv.py Make sure to set portal.ignore_g = True if configurations with g orbitals or terms with G are to be ignored in final outputs. If CI+second-order directory is not found, uncertainties will be set to 0. Input: /CI+all-order/even/ , /CI+all-order/odd/ , /CI+second-order/even/ , /CI+second-order/odd/ CONFFINAL.RES /CI+all-order/dtm/ , /CI+second-order/dtm/ (optional) E1.RES E1MBPT.RES Output: Sr1_Energies.csv Sr1_Matrix_Elements.csv Sr1_Transition_Rates.csv Run calc_lifetimes.py Input: Sr1_Transition_Rates.csv Output: Sr1_Lifetimes_Error_Check.csv Sr1_Transition_Rates_Error_Check.csv","title":"Sr with pCI-py"},{"location":"pci-py-example/#example-sr-with-pci-py","text":"In this example, we will utilize the pCI-py scripts detailed here to generate atomic data for neutral Sr.","title":"Example: Sr with pCI-py"},{"location":"pci-py-example/#configyml","text":"The config.yml file defines the entire calculation from beginning to end. You can find a detailed explanation of the contents of this configuration file here . Click here to see the contents of the sample config.yml we will be using. # General parameters system: name: Sr isotope: include_breit: True # Parameters used by basis programs basis: cavity_radius: 60 orbitals: core: 1s 2s 2p 3s 3p 3d 4s 4p valence: 5s 5p 4d 6s 6p 5d 4f nmax: 35 lmax: 6 b_splines: nmax: 40 lmax: 6 k: 7 val_aov: s: 4 p: 4 d: 4 f: 3 val_energies: kval: 1 energies: s: -0.28000 p: [-0.22000, -0.22000] d: [-0.31000, -0.31000] f: [-0.13000, -0.13000] # Parameters used by add program add: # Lists of even and odd parity reference configurations ref_configs: odd: [5s1 5p1] even: [5s2] basis_set: 22spdfg orbitals: core: 1s 2s 2p 3s 3p 3d 4s 4p active: [ 4-7p: 0 4, 4-7d: 0 4, 4-7f: 0 4, 5-7g: 0 4, ] excitations: single: True double: True triple: False # Parameters used by conf program conf: J: 0.0 JM: 0.0 J_selection: False num_energy_levels: 24 num_dvdsn_iterations: 100 include_lsj: True write_hij: False # Parameters used by dtm program dtm: matrix_elements: E1 include_rpa: True # Parameters used by portal script portal: ignore_g: True # Optional parameters optional: qed: include: False rotate_basis: False isotope_shifts: include: False K_is: 1 C_is: 0.01 code_method: [ci+all-order, ci+second-order] generate_directories: False run_ao_codes: True run_ci: True pci_version: default","title":"config.yml"},{"location":"pci-py-example/#running-pci-py-scripts","text":"Write config.yml to root directory. Make sure to re-define optional.pci_version = default to the correct version if parameters such as the radial grid size has to be changed. On UD computers, one can use the command vpkg_versions pci to list all version of pCI. Make sure to set optional.run_ao_codes = True in order for job scripts to be automatically submitted to SLURM for all-order codes to run. Run basis.py This script will generate basis sets for CI+all-order and CI+MBPT in their respective directories. Input: config.yml (specifically blocks system , basis , optional ) Output: /CI+all-order/basis/ /CI+second-order/basis/ Run add.py Make sure to set optional.run_ci = True in order for job scripts to be automatically submitted to SLURM for conf to run. Input: config.yml (specifically blocks system , basis , conf , optional ) /CI+all-order/basis/BASS.INP /CI+second-order/basis/BASS.INP Output: /CI+all-order/even/CONFFINAL.RES /CI+all-order/odd/CONFFINAL.RES /CI+second-order/even/CONFFINAL.RES /CI+second-order/odd/CONFFINAL.RES Run dtm.py Make sure to set dtm.matrix_elements = E1 so E1.RES , which contains a table of E1 transitions, is formed. Make sure to set dtm.include_rpa = True to include RPA corrections. Input: config.yml (specifically blocks system , conf , dtm ) /CI+all-order/even/ , /CI+all-order/odd/ , /CI+second-order/even/ , /CI+second-order/odd/ CONF.INP CONF.DAT CONF.DET CONF.XIJ CONFSTR.RES Output: /CI+all-order/dtm/ , /CI+second-order/dtm/ TM.RES E1.RES Run gen_portal_csv.py Make sure to set portal.ignore_g = True if configurations with g orbitals or terms with G are to be ignored in final outputs. If CI+second-order directory is not found, uncertainties will be set to 0. Input: /CI+all-order/even/ , /CI+all-order/odd/ , /CI+second-order/even/ , /CI+second-order/odd/ CONFFINAL.RES /CI+all-order/dtm/ , /CI+second-order/dtm/ (optional) E1.RES E1MBPT.RES Output: Sr1_Energies.csv Sr1_Matrix_Elements.csv Sr1_Transition_Rates.csv Run calc_lifetimes.py Input: Sr1_Transition_Rates.csv Output: Sr1_Lifetimes_Error_Check.csv Sr1_Transition_Rates_Error_Check.csv","title":"Running pCI-py scripts"},{"location":"pci-py/","text":"pci-py scripts In this section, we describe supplementary python scripts used to automate the entire pCI process, from the generation of basis sets and configuration lists to the actual calculations themselves. These python scripts read a user-defined YAML-formatted configuration file config.yml to set general parameters defining the atomic system of interest, as well as parameters defining the types of the calculations. These python scripts are currently designed primarily for massive data generation via CI+all-order and CI+MBPT calculations on the UD clusters. They can be modified in the future for more general calculations. config.yml The config.yml file is a YAML-formatted configuration file that defines important parameters of the atomic system of interest. This config file is divided into several sections: general parameters parameters used by basis programs (only read by basis.py) parameters used by add program (only read by add.py) parameters used by conf program (only read by add.py) parameters used by dtm program (only read by dtm.py) optional parameters The general parameters are in the block system and is read by all python scripts: system: name: Sr isotope: include_breit: True The system.name field takes the name of the atomic system of interest. The system.isotope field takes a specified isotope number. If left blank, the script will automatically find the atomic weight from nuclear radii table. The system.include_breit field takes in a boolean to set whether to include Breit corrections or not. basis.py The basis.py script automatically generates the basis set given information about the atomic system of interest via a configuration file config.yml . This script reads the basis block: basis: cavity_radius: 60 orbitals: core: 1s 2s 2p 3s 3p 3d 4s 4p valence: 5s 5p 4d 6s 6p 5d 4f nmax: 35 lmax: 6 b_splines: nmax: 40 lmax: 6 k: 7 val_aov: s: 4 p: 4 d: 4 f: 3 val_energies: kval: 1 energies: s: -0.28000 p: [-0.22000, -0.22000] d: [-0.31000, -0.31000] f: [-0.13000, -0.13000] The basis.cavity_radius field takes in a value to set the size of the spherical cavity. The basis.orbitals block requires specification of the core and valence orbitals to be included in the basis. It also requires a maximum principal quantum number nmax and maximum partial wave lmax for basis set orbital generation. The basis.b_splines block requires specification of the maximum principal quantum number nmax (number of splines), maximum partial wave lmax , and order of splines. The basis.val_aov block requires the number of valence orbitals to include for each partial wave. The basis.val_energies block allows specification of the energies of the valence orbitals. In addition, the script will read the optional block: optional: qed: include: False rotate_basis: False isotope_shifts: include: False K_is: 1 C_is: 0.01 code_exec: ci+all-order run_ao_codes: False The optional.qed block allows users to specify inclusion of QED corrections (this is currently not supported). The optional.isotope_shifts block allows users to specify isotope shift calculations by switching the include value to True and specifying keys K_is and C_is . The optional.code_exec field allows users to specify automatic execution of basis set codes depending on value. The optional.run_ao_codes field allows users to specify whether they would like to run the all-order set of codes after construction of the basis set. add.py The add.py script automatically generates the list of configurations given information about the atomic system of interest via a configuration file config.yml . Note that if BASS.INP is not present, the order of the conigurations will be by $nl$, and not how it is defined in the basis set construction. This script reads the add block: add: # Lists of even and odd parity reference configurations ref_configs: odd: [5s1 5p1] even: [5s2] basis_set: 22spdfg orbitals: core: 1s 2s 2p 3s 3p 3d 4s 4p active: [ 4-7p: 0 4, 4-7d: 0 4, 4-7f: 0 4, 5-7g: 0 4, ] excitations: single: True double: True triple: False The add.ref_configs block requires a list of reference configurations to excite electrons from to construct the list of configurations for the CI calculation. This list will not be constructed if left blank for a specified parity. The add.basis_set block requires specification of the basis set designated by nspdfg , where n specifies the principal quantum number, and spdfg specifies inclusion of s, p, d, f, and g orbitals. One can include higher partial waves by appending to the end of the list h , i , k , ... The add.orbitals block allows full customization of allowed orbital occupancies. For example, 1-2s: 2 2 defines the 1s and 2s orbitals to be closed, 2p: 6 6 defines the 2p orbital to be closed, 3-7p: 0 6 defines 3p to 7p orbitals to be completely open to allow up to 6 electrons on those orbitals. The add.excitations block defines the number of allowed excitations. This script also reads parameters for the CI execution from the conf block: conf: J: 0.0 JM: 0.0 J_selection: False num_energy_levels: 24 num_dvdsn_iterations: 100 include_lsj: True write_hij: False The conf.J field defines the total angular momentum of the energy levels. The conf.JM field defines the projection of the total angular momentum. The conf.J_selection field defines whether the user wants energy levels of a specific J value defined by J and JM. The conf.num_energy_levels field defines the number of energy levels to be calculated. The conf.num_dvdsn_iterations field defines the total number of Davidson iterations allowed. The conf.include_lsj field defines whether the user wants expectation values L^2 and S^2 to be calculated. The conf.write_hij field defines whether the user wants the Hamiltonian matrix to be written to file CONF.HIJ . dtm.py The dtm.py script automatically generates the density and/or transition matrix elements given output files from conf runs. This script reads the dtm block from config.yml : dtm: matrix_elements: E1 include_rpa: True The dtm.matrix_elements field defines the types of matrix elements to be calculated. This can be a single matrix element or an array of matrix elements such as [E1, M2] The dtm.include_rpa field defines whether the user would like to include RPA corrections. gen_portal_csv.py The gen_portal_csv.py script generates csv-formatted datafiles of atomic energy levels and matrix elements given output files from conf and dtm runs. This script reads the portal block from config.yml : portal: ignore_g: True min_uncertainty: 1.5 The portal.ignore_g field removes all atomic properties with >g orbitals in the configuration or >G terms. The porta.min_uncertainty field sets a minimum uncertainty in percentage for matrix value uncertainties. The script has predefined minimum uncertainties set for a few systems. calc_lifetimes.py The calc_lifetimes.py script generates csv-formatted datafiles of lifetimes and transition rates given output files from gen_portal_csv.py . More information about atomic data generation for the UD ATOM portal can be found here .","title":"pCI-py scripts"},{"location":"pci-py/#pci-py-scripts","text":"In this section, we describe supplementary python scripts used to automate the entire pCI process, from the generation of basis sets and configuration lists to the actual calculations themselves. These python scripts read a user-defined YAML-formatted configuration file config.yml to set general parameters defining the atomic system of interest, as well as parameters defining the types of the calculations. These python scripts are currently designed primarily for massive data generation via CI+all-order and CI+MBPT calculations on the UD clusters. They can be modified in the future for more general calculations.","title":"pci-py scripts"},{"location":"pci-py/#configyml","text":"The config.yml file is a YAML-formatted configuration file that defines important parameters of the atomic system of interest. This config file is divided into several sections: general parameters parameters used by basis programs (only read by basis.py) parameters used by add program (only read by add.py) parameters used by conf program (only read by add.py) parameters used by dtm program (only read by dtm.py) optional parameters The general parameters are in the block system and is read by all python scripts: system: name: Sr isotope: include_breit: True The system.name field takes the name of the atomic system of interest. The system.isotope field takes a specified isotope number. If left blank, the script will automatically find the atomic weight from nuclear radii table. The system.include_breit field takes in a boolean to set whether to include Breit corrections or not.","title":"config.yml"},{"location":"pci-py/#basispy","text":"The basis.py script automatically generates the basis set given information about the atomic system of interest via a configuration file config.yml . This script reads the basis block: basis: cavity_radius: 60 orbitals: core: 1s 2s 2p 3s 3p 3d 4s 4p valence: 5s 5p 4d 6s 6p 5d 4f nmax: 35 lmax: 6 b_splines: nmax: 40 lmax: 6 k: 7 val_aov: s: 4 p: 4 d: 4 f: 3 val_energies: kval: 1 energies: s: -0.28000 p: [-0.22000, -0.22000] d: [-0.31000, -0.31000] f: [-0.13000, -0.13000] The basis.cavity_radius field takes in a value to set the size of the spherical cavity. The basis.orbitals block requires specification of the core and valence orbitals to be included in the basis. It also requires a maximum principal quantum number nmax and maximum partial wave lmax for basis set orbital generation. The basis.b_splines block requires specification of the maximum principal quantum number nmax (number of splines), maximum partial wave lmax , and order of splines. The basis.val_aov block requires the number of valence orbitals to include for each partial wave. The basis.val_energies block allows specification of the energies of the valence orbitals. In addition, the script will read the optional block: optional: qed: include: False rotate_basis: False isotope_shifts: include: False K_is: 1 C_is: 0.01 code_exec: ci+all-order run_ao_codes: False The optional.qed block allows users to specify inclusion of QED corrections (this is currently not supported). The optional.isotope_shifts block allows users to specify isotope shift calculations by switching the include value to True and specifying keys K_is and C_is . The optional.code_exec field allows users to specify automatic execution of basis set codes depending on value. The optional.run_ao_codes field allows users to specify whether they would like to run the all-order set of codes after construction of the basis set.","title":"basis.py"},{"location":"pci-py/#addpy","text":"The add.py script automatically generates the list of configurations given information about the atomic system of interest via a configuration file config.yml . Note that if BASS.INP is not present, the order of the conigurations will be by $nl$, and not how it is defined in the basis set construction. This script reads the add block: add: # Lists of even and odd parity reference configurations ref_configs: odd: [5s1 5p1] even: [5s2] basis_set: 22spdfg orbitals: core: 1s 2s 2p 3s 3p 3d 4s 4p active: [ 4-7p: 0 4, 4-7d: 0 4, 4-7f: 0 4, 5-7g: 0 4, ] excitations: single: True double: True triple: False The add.ref_configs block requires a list of reference configurations to excite electrons from to construct the list of configurations for the CI calculation. This list will not be constructed if left blank for a specified parity. The add.basis_set block requires specification of the basis set designated by nspdfg , where n specifies the principal quantum number, and spdfg specifies inclusion of s, p, d, f, and g orbitals. One can include higher partial waves by appending to the end of the list h , i , k , ... The add.orbitals block allows full customization of allowed orbital occupancies. For example, 1-2s: 2 2 defines the 1s and 2s orbitals to be closed, 2p: 6 6 defines the 2p orbital to be closed, 3-7p: 0 6 defines 3p to 7p orbitals to be completely open to allow up to 6 electrons on those orbitals. The add.excitations block defines the number of allowed excitations. This script also reads parameters for the CI execution from the conf block: conf: J: 0.0 JM: 0.0 J_selection: False num_energy_levels: 24 num_dvdsn_iterations: 100 include_lsj: True write_hij: False The conf.J field defines the total angular momentum of the energy levels. The conf.JM field defines the projection of the total angular momentum. The conf.J_selection field defines whether the user wants energy levels of a specific J value defined by J and JM. The conf.num_energy_levels field defines the number of energy levels to be calculated. The conf.num_dvdsn_iterations field defines the total number of Davidson iterations allowed. The conf.include_lsj field defines whether the user wants expectation values L^2 and S^2 to be calculated. The conf.write_hij field defines whether the user wants the Hamiltonian matrix to be written to file CONF.HIJ .","title":"add.py"},{"location":"pci-py/#dtmpy","text":"The dtm.py script automatically generates the density and/or transition matrix elements given output files from conf runs. This script reads the dtm block from config.yml : dtm: matrix_elements: E1 include_rpa: True The dtm.matrix_elements field defines the types of matrix elements to be calculated. This can be a single matrix element or an array of matrix elements such as [E1, M2] The dtm.include_rpa field defines whether the user would like to include RPA corrections.","title":"dtm.py"},{"location":"pci-py/#gen_portal_csvpy","text":"The gen_portal_csv.py script generates csv-formatted datafiles of atomic energy levels and matrix elements given output files from conf and dtm runs. This script reads the portal block from config.yml : portal: ignore_g: True min_uncertainty: 1.5 The portal.ignore_g field removes all atomic properties with >g orbitals in the configuration or >G terms. The porta.min_uncertainty field sets a minimum uncertainty in percentage for matrix value uncertainties. The script has predefined minimum uncertainties set for a few systems.","title":"gen_portal_csv.py"},{"location":"pci-py/#calc_lifetimespy","text":"The calc_lifetimes.py script generates csv-formatted datafiles of lifetimes and transition rates given output files from gen_portal_csv.py . More information about atomic data generation for the UD ATOM portal can be found here .","title":"calc_lifetimes.py"},{"location":"portal_codes/","text":"Portal codes In this section, we describe supplementary python scripts used to generate csv-formatted databases of energy levels and matrix elements for the University of Delaware's Portal for High-Precision Atomic Data and Computation . The required input files can be obtained from conf for energies and dtm for matrix elements. This process has been heavily automated using the pCI-py scripts . Method The method of generating the csv-formatted atomic data files is summarized in the following steps: Reading and reformatting input files First, we read all CONFFINAL.RES outputs from conf from each calculation done. The script will first check directories /ci+all-order/even/ , /ci+all-order/odd/ , /ci+second-order/even/ and /ci+second-order/odd/ for this file. If these directories are not detected, then it will check the /DATA_RAW/ directory, where users should place them. If using this directory, the all-order output files should be named CONFFINALeven.RES and CONFFINALodd.RES , and the second-order output files should be named CONFFINALevenMBPT.RES and CONFFINALoddMBPT.RES . If the all-order and second-order directories are detected, the script will place them in /DATA_RAW/ itself. Additionally, matrix element output files, such as E1.RES from dtm can be read. The script will check directories /ci+all-order/dtm/ and /ci+second-order/dtm/ first before /DATA_RAW/ . The all-order and second-order output files should be named, for example, E1.RES and E1MBPT.RES . Once all input files are read, if second-order/MBPT exist, then uncertainties are calculated and a new csv-formatted CONFFINAL.csv is written with uncertainties. Otherwise, they are all set to 0 in this file. Experimental data are then acquired by parsing the NIST Atomic Spectral Database for full list of energy levels and stored for comparison with the data we generated through pCI. Filtering and correcting misidentified configurations Sometimes the configurations and terms of energy levels outputted from the pCI codes might be be misidentified due to discrepancies in basis or numerical precision. At this stage, we attempt to correct misidentified configurations using the data from NIST. Here we create a correspondence that maps the parsed NIST identifications to the pCI theory data. This correspondence or mapping is written to the files /DATA_Output/Element_Even.txt and /DATA_Output/Element_Odd.txt . Outputting data for portal The final part of the portal codes reformats the mapping of NIST and pCI energy levels for use on the UD Atom portal. The output is a csv-formatted file of the energies of the system, with a preference for NIST data over pCI-calculated data, i.e. for each configuration, NIST energies and identifications are chosen over the theory data. If NIST data is not available, then the theory values are used. The final column of the csv file is_from_theory is set to True if theory values are used. The energy levels are stored in Element_Energies.csv and the matrix elements are stored in Element_Matrix_Elements_Theory.csv . Additionally, the script calc_lifetimes.py can be used to generate csv-formatted files of transition rates and lifetimes. These are outputted as Element_Lifetimes_Error_Check.csv and Element_Transition_Rates_Error_Check.csv .","title":"Portal codes"},{"location":"portal_codes/#portal-codes","text":"In this section, we describe supplementary python scripts used to generate csv-formatted databases of energy levels and matrix elements for the University of Delaware's Portal for High-Precision Atomic Data and Computation . The required input files can be obtained from conf for energies and dtm for matrix elements. This process has been heavily automated using the pCI-py scripts .","title":"Portal codes"},{"location":"portal_codes/#method","text":"The method of generating the csv-formatted atomic data files is summarized in the following steps: Reading and reformatting input files First, we read all CONFFINAL.RES outputs from conf from each calculation done. The script will first check directories /ci+all-order/even/ , /ci+all-order/odd/ , /ci+second-order/even/ and /ci+second-order/odd/ for this file. If these directories are not detected, then it will check the /DATA_RAW/ directory, where users should place them. If using this directory, the all-order output files should be named CONFFINALeven.RES and CONFFINALodd.RES , and the second-order output files should be named CONFFINALevenMBPT.RES and CONFFINALoddMBPT.RES . If the all-order and second-order directories are detected, the script will place them in /DATA_RAW/ itself. Additionally, matrix element output files, such as E1.RES from dtm can be read. The script will check directories /ci+all-order/dtm/ and /ci+second-order/dtm/ first before /DATA_RAW/ . The all-order and second-order output files should be named, for example, E1.RES and E1MBPT.RES . Once all input files are read, if second-order/MBPT exist, then uncertainties are calculated and a new csv-formatted CONFFINAL.csv is written with uncertainties. Otherwise, they are all set to 0 in this file. Experimental data are then acquired by parsing the NIST Atomic Spectral Database for full list of energy levels and stored for comparison with the data we generated through pCI. Filtering and correcting misidentified configurations Sometimes the configurations and terms of energy levels outputted from the pCI codes might be be misidentified due to discrepancies in basis or numerical precision. At this stage, we attempt to correct misidentified configurations using the data from NIST. Here we create a correspondence that maps the parsed NIST identifications to the pCI theory data. This correspondence or mapping is written to the files /DATA_Output/Element_Even.txt and /DATA_Output/Element_Odd.txt . Outputting data for portal The final part of the portal codes reformats the mapping of NIST and pCI energy levels for use on the UD Atom portal. The output is a csv-formatted file of the energies of the system, with a preference for NIST data over pCI-calculated data, i.e. for each configuration, NIST energies and identifications are chosen over the theory data. If NIST data is not available, then the theory values are used. The final column of the csv file is_from_theory is set to True if theory values are used. The energy levels are stored in Element_Energies.csv and the matrix elements are stored in Element_Matrix_Elements_Theory.csv . Additionally, the script calc_lifetimes.py can be used to generate csv-formatted files of transition rates and lifetimes. These are outputted as Element_Lifetimes_Error_Check.csv and Element_Transition_Rates_Error_Check.csv .","title":"Method"},{"location":"publications/","text":"Publications 2023 State-Insensitive Trapping of Alkaline-Earth Atoms in a Nanofiber-Based Optical Dipole Trap , K. Ton, G. Kestler, D. Filin, C. Cheung, P. Schneeweiss, T. Hoinkes, J. Volz, M. S. Safronova, A. Rauschenbeutel, J. T. Barreiro, PRX Quantum 4, 040308 (2023) . Optical Telecommunications-Band Clock based on Neutral Titanium Atoms , Scott Eustice, Dmytro Filin, Jackson Schrott, Sergey Porsev, Charles Cheung, Diego Novoa, Dan M. Stamper-Kurn, Marianna S. Safronova, Phys. Rev. A 107, L051102 (2023) . 2022 Prospects of a thousand-ion Sn 2+ Coulomb-crystal clock with sub-10 \u221219 inaccuracy , David R. Leibrandt, Sergey G. Porsev, Charles Cheung, Marianna S. Safronova, arXiv:2205.15484 (2022) . New Measurement Resolves Key Astrophysical Fe XVII Oscillator Strength Problem , Steffen K\u00fchn, Charles Cheung, Natalia S. Oreshkina, Ren\u00e9 Steinbr\u00fcgge, Moto Togawa, Sonja Bernitt, Lukas Berger, Jens Buck, Moritz Hoesch, J\u00f6rn Seltmann, Florian Trinter, Christoph H. Keitel, Mikhail G. Kozlov, Sergey G. Porsev, Ming Feng Gu, F. Scott Porter, Thomas Pfeifer, Maurice A. Leutenegger, Zolt\u00e1n Harman, Marianna S. Safronova, Jos\u00e9 R. Crespo L\u00f3pez-Urrutia, and Chintan Shah, Phys. Rev. Lett. 129, 245001 (2022) . Physics Viewpoint: Getting a Clearer View of Iron Emission Lines . Calculation of energies and hyperfine-structure constants of 233 U + and 233 U , S. G. Porsev, C. Cheung, and M. S. Safronova, Phys. Rev. A 106, 042810 (2022) . Laser Spectroscopy of the y 7 P o J states of Cr I , E. B. Norrgard, D. S. Barker, S. P. Eckel, S. G. Porsev, C. Cheung, M. G. Kozlov, I. I. Tupitsyn, and M. S. Safronova, Phys. Rev. A 105, 032812 (2022) . 2021 Low-lying energy levels of 229 Th 35+ and the electronic bridge process , S. G. Porsev, C. Cheung and M. S. Safronova, Quantum Sci. Technol. 6, 034014 (2021) . Scalable Codes for Precision Calculations of Properties of Complex Atomic Systems , C. Cheung, M. S. Safronova, S. G. Porsev, Symmetry 13 (4), 621 (2021) . Observation of an electric quadrupole transition in a negative ion: Experiment and Theory , C. W. Walter, S. E. Spielman, R. Ponce, N. D. Gibson, J. N. Yukich, C. Cheung, and M. S. Safronova, Phys. Rev. Lett. 126, 083001 (2021) . Predicting quasibound states of negative ions: La - as a test case , M. S. Safronova, C. Cheung, M. G. Kozlov, S. E. Spielman, N. D. Gibson, and C. W. Walter, Phys. Rev. 103, 022819 (2021) . 2020 High-resolution photo-excitation measurements exacerbate the long-standing Fe XVII emission problem , Steffen K\u00fchn et al., Phys. Rev. Lett. 124, 225001 (2020) . Physics Synopsis: Resolving Discrepancies in X-Ray Astronomy . Optical clocks based on the Cf 15+ and Cf 17+ ions , S. G. Porsev, U. I. Safronova, M. S. Safronova, P. O. Schmidt, A. I. Bondarev, M. G. Kozlov, I. I. Tupitsyn, Phys. Rev. A 102, 012802 (2020) . Accurate Prediction of Clock Transitions in a Highly Charged Ion with Complex Electronic Structure , C. Cheung, M. S. Safronova, S. G. Porsev, M. G. Kozlov, I. I. Tupitsyn, and A. I. Bondarev, Phys. Rev. Lett. 124, 163001 (2020) . 2015 CI-MBPT: A package of programs for relativistic atomic calculations based on a method combining configuration interaction and many-body perturbation theory , M. G. Kozlov, S. G. Porsev, M. S. Safronova, and I. I. Tupitsyn, Comput. Phys. Commun. 195, 199 (2015) .","title":"Publications"},{"location":"publications/#publications","text":"","title":"Publications"},{"location":"publications/#2023","text":"State-Insensitive Trapping of Alkaline-Earth Atoms in a Nanofiber-Based Optical Dipole Trap , K. Ton, G. Kestler, D. Filin, C. Cheung, P. Schneeweiss, T. Hoinkes, J. Volz, M. S. Safronova, A. Rauschenbeutel, J. T. Barreiro, PRX Quantum 4, 040308 (2023) . Optical Telecommunications-Band Clock based on Neutral Titanium Atoms , Scott Eustice, Dmytro Filin, Jackson Schrott, Sergey Porsev, Charles Cheung, Diego Novoa, Dan M. Stamper-Kurn, Marianna S. Safronova, Phys. Rev. A 107, L051102 (2023) .","title":"2023"},{"location":"publications/#2022","text":"Prospects of a thousand-ion Sn 2+ Coulomb-crystal clock with sub-10 \u221219 inaccuracy , David R. Leibrandt, Sergey G. Porsev, Charles Cheung, Marianna S. Safronova, arXiv:2205.15484 (2022) . New Measurement Resolves Key Astrophysical Fe XVII Oscillator Strength Problem , Steffen K\u00fchn, Charles Cheung, Natalia S. Oreshkina, Ren\u00e9 Steinbr\u00fcgge, Moto Togawa, Sonja Bernitt, Lukas Berger, Jens Buck, Moritz Hoesch, J\u00f6rn Seltmann, Florian Trinter, Christoph H. Keitel, Mikhail G. Kozlov, Sergey G. Porsev, Ming Feng Gu, F. Scott Porter, Thomas Pfeifer, Maurice A. Leutenegger, Zolt\u00e1n Harman, Marianna S. Safronova, Jos\u00e9 R. Crespo L\u00f3pez-Urrutia, and Chintan Shah, Phys. Rev. Lett. 129, 245001 (2022) . Physics Viewpoint: Getting a Clearer View of Iron Emission Lines . Calculation of energies and hyperfine-structure constants of 233 U + and 233 U , S. G. Porsev, C. Cheung, and M. S. Safronova, Phys. Rev. A 106, 042810 (2022) . Laser Spectroscopy of the y 7 P o J states of Cr I , E. B. Norrgard, D. S. Barker, S. P. Eckel, S. G. Porsev, C. Cheung, M. G. Kozlov, I. I. Tupitsyn, and M. S. Safronova, Phys. Rev. A 105, 032812 (2022) .","title":"2022"},{"location":"publications/#2021","text":"Low-lying energy levels of 229 Th 35+ and the electronic bridge process , S. G. Porsev, C. Cheung and M. S. Safronova, Quantum Sci. Technol. 6, 034014 (2021) . Scalable Codes for Precision Calculations of Properties of Complex Atomic Systems , C. Cheung, M. S. Safronova, S. G. Porsev, Symmetry 13 (4), 621 (2021) . Observation of an electric quadrupole transition in a negative ion: Experiment and Theory , C. W. Walter, S. E. Spielman, R. Ponce, N. D. Gibson, J. N. Yukich, C. Cheung, and M. S. Safronova, Phys. Rev. Lett. 126, 083001 (2021) . Predicting quasibound states of negative ions: La - as a test case , M. S. Safronova, C. Cheung, M. G. Kozlov, S. E. Spielman, N. D. Gibson, and C. W. Walter, Phys. Rev. 103, 022819 (2021) .","title":"2021"},{"location":"publications/#2020","text":"High-resolution photo-excitation measurements exacerbate the long-standing Fe XVII emission problem , Steffen K\u00fchn et al., Phys. Rev. Lett. 124, 225001 (2020) . Physics Synopsis: Resolving Discrepancies in X-Ray Astronomy . Optical clocks based on the Cf 15+ and Cf 17+ ions , S. G. Porsev, U. I. Safronova, M. S. Safronova, P. O. Schmidt, A. I. Bondarev, M. G. Kozlov, I. I. Tupitsyn, Phys. Rev. A 102, 012802 (2020) . Accurate Prediction of Clock Transitions in a Highly Charged Ion with Complex Electronic Structure , C. Cheung, M. S. Safronova, S. G. Porsev, M. G. Kozlov, I. I. Tupitsyn, and A. I. Bondarev, Phys. Rev. Lett. 124, 163001 (2020) .","title":"2020"},{"location":"publications/#2015","text":"CI-MBPT: A package of programs for relativistic atomic calculations based on a method combining configuration interaction and many-body perturbation theory , M. G. Kozlov, S. G. Porsev, M. S. Safronova, and I. I. Tupitsyn, Comput. Phys. Commun. 195, 199 (2015) .","title":"2015"},{"location":"qed/","text":"How to include QED corrections The following instructions assume familiarity with the main programs of the pCI package . Steps to run a QED calculation In this section, we will introduce calculations including QED corrections. Construct basis set by running hfd and bass to obtain HFD.DAT Run sgc to form an empty SGC.CON file cp HFD.DAT HFD-noQED.DAT - save a copy of HFD.DAT without QED Create a file qedpot.inp with number corresponding to the variant of the QED potential and the name of the HFD.DAT file. Click here to see a description of qedpot.inp . 1 # kvar=1-5, Variant of QED potential HFD.DAT # name of file holding basis set Create a file qed.in selecting options. Click here to see a description of qed.in . 1 # 1 for general diagonalization, 2 for first-order 1 # 1 for no QED, 2 for QED 2 # 0 for Coulomb, 1 for Gaunt, 2 for Full Breit Loop until convergence: Run qedpot_conf to construct selected QED potential Run qed_rot to rotate orbitals to diagonalize Hamiltonian with QED corrections cp SGC.CON SGC-noQED.CON - save a copy of SGC.CON without QED Run qedpot_conf Rename SGCqed.CON to SGC.CON Run conf You can also use the following batch.qed script for steps 3-9, making sure to change inputs relevant to your job. #! /bin/bash -fe kvar=1 # variant of QED potential bin='./' qedpot=$bin'qedpot_conf' qedrot=$bin'qed_rot' iter=25 # max number of iterations ##################################### cat >qedpot.inp <<EndofFile $kvar HFD.DAT EndofFile ##################################### cat >qed.in <<EndofFile 1 # diagonalization 1 # 1 for noQED, 2 for QED 2 # 0 for Coulomb, 1 for Gaunt, 2 for Full Breit EndofFile ##################################### n=1 while [ $n -lt $iter ]; do echo 'Iteration '$n $qedpot >qp.res $qedrot <qed.in >qr.res grep 'changed' \"QED_ROT.RES\" if grep -q reached \"QED_ROT.RES\"; then echo 'Converged in '$n' iterations' break fi let n=n+1 done cp SGC.CON SCG-noQED.CON ./qedpot_conf >qp.res cp SGCqed.CON SGC.CON #####################################","title":"Running QED"},{"location":"qed/#how-to-include-qed-corrections","text":"The following instructions assume familiarity with the main programs of the pCI package .","title":"How to include QED corrections"},{"location":"qed/#steps-to-run-a-qed-calculation","text":"In this section, we will introduce calculations including QED corrections. Construct basis set by running hfd and bass to obtain HFD.DAT Run sgc to form an empty SGC.CON file cp HFD.DAT HFD-noQED.DAT - save a copy of HFD.DAT without QED Create a file qedpot.inp with number corresponding to the variant of the QED potential and the name of the HFD.DAT file. Click here to see a description of qedpot.inp . 1 # kvar=1-5, Variant of QED potential HFD.DAT # name of file holding basis set Create a file qed.in selecting options. Click here to see a description of qed.in . 1 # 1 for general diagonalization, 2 for first-order 1 # 1 for no QED, 2 for QED 2 # 0 for Coulomb, 1 for Gaunt, 2 for Full Breit Loop until convergence: Run qedpot_conf to construct selected QED potential Run qed_rot to rotate orbitals to diagonalize Hamiltonian with QED corrections cp SGC.CON SGC-noQED.CON - save a copy of SGC.CON without QED Run qedpot_conf Rename SGCqed.CON to SGC.CON Run conf You can also use the following batch.qed script for steps 3-9, making sure to change inputs relevant to your job. #! /bin/bash -fe kvar=1 # variant of QED potential bin='./' qedpot=$bin'qedpot_conf' qedrot=$bin'qed_rot' iter=25 # max number of iterations ##################################### cat >qedpot.inp <<EndofFile $kvar HFD.DAT EndofFile ##################################### cat >qed.in <<EndofFile 1 # diagonalization 1 # 1 for noQED, 2 for QED 2 # 0 for Coulomb, 1 for Gaunt, 2 for Full Breit EndofFile ##################################### n=1 while [ $n -lt $iter ]; do echo 'Iteration '$n $qedpot >qp.res $qedrot <qed.in >qr.res grep 'changed' \"QED_ROT.RES\" if grep -q reached \"QED_ROT.RES\"; then echo 'Converged in '$n' iterations' break fi let n=n+1 done cp SGC.CON SCG-noQED.CON ./qedpot_conf >qp.res cp SGCqed.CON SGC.CON #####################################","title":"Steps to run a QED calculation"},{"location":"radial_grid/","text":"Changing the radial grid One feature of the pCI code package stems from the original CI-MBPT code package. One-electron orbitals outside the nucleus are defined on a radial grid. Inside the nucleus, they are described in a form of the Taylor expansion over r/R , where R is the nuclear radius. In the codes used to construct the basis set, the parameter R2 sets the size of the radial grid in a.u. The default version of the codes come with a grid of xxx nodes. In order to rebuild the code with a different size grid, several parameters must be changed in the basis set codes. For example, the following parameters sets the codes for a 1500-pt grid: bdhf set NGP=1500 : bdhf.f wfun.f wint.f yfun1.f zfun.f zint.f bspl set NHF=1500 bspl.f set NGP=1500 wfun.f wint.f yfun1.f yint1.f bas_wj , bas_x , hfd , bass , rpa , rpa_dtm set IPx6=1500 bas_x.par set IP6=1470 hfd.par allcore-rle-ci , valsd-rle-cis , sdvw-rle-cis , second-cis set NHF=1500 all.par second-cis.par archiv.par set NGP=1500 archiv.par spl.in and bas_wj.in set number of points to 0.1 and 1500","title":"Changing the radial grid"},{"location":"radial_grid/#changing-the-radial-grid","text":"One feature of the pCI code package stems from the original CI-MBPT code package. One-electron orbitals outside the nucleus are defined on a radial grid. Inside the nucleus, they are described in a form of the Taylor expansion over r/R , where R is the nuclear radius. In the codes used to construct the basis set, the parameter R2 sets the size of the radial grid in a.u. The default version of the codes come with a grid of xxx nodes. In order to rebuild the code with a different size grid, several parameters must be changed in the basis set codes. For example, the following parameters sets the codes for a 1500-pt grid: bdhf set NGP=1500 : bdhf.f wfun.f wint.f yfun1.f zfun.f zint.f bspl set NHF=1500 bspl.f set NGP=1500 wfun.f wint.f yfun1.f yint1.f bas_wj , bas_x , hfd , bass , rpa , rpa_dtm set IPx6=1500 bas_x.par set IP6=1470 hfd.par allcore-rle-ci , valsd-rle-cis , sdvw-rle-cis , second-cis set NHF=1500 all.par second-cis.par archiv.par set NGP=1500 archiv.par spl.in and bas_wj.in set number of points to 0.1 and 1500","title":"Changing the radial grid"},{"location":"supplementary/","text":"Supplementary programs In this section, we describe supplementary programs used to generate basis sets or optimized configuration lists. . bdhfA - Dirac-Hartree-Fock code bdhfA can be used in place of bdhf to make a B-spline basis set. This program reads in an input file xx.dat with a much simpler input than for bdhf , and will write bdhf.in , which can be renamed to bas_wj.in for subsequent use in all-order codes. Click here to see a description of a minimum input file for Cs . 1 1 Cs 1s 2s 2p 3s 3p 3d 4s 4p 4d 5s 5p ========== 7 70 9 220.0 Click here to see a description of a complete input file for Cs . 1 1 Cs 137 0.0001 1s 2s 2p 3s 3p 3d 4s 4p 4d 5s 5p 6 9 s1/2 6 9 p1/2 5d3/2 5d5/2 ========== 7 70 9 220.0 Click here to see a description of a complete input file for Yb . 1 1 Yb 176 0.0001 1s 2s 2p 3s 3p 3d 4s 4p 4d 5s 5p 4f 6s1/2 5d3/2 5d5/2 ========== 7 50 9 60.0 The input files have the following format: 1 # 0 for old input, 1 for new input 1 # convergence parameter: 1 - most cases, 5 - in case of convergence problems Cs # name of element - e.g. Cs, Yb, etc. nuclear charge Z is set using this name # Isotope can also be entered. If no isotope is entered, most abundant or # most stable isotope will be used. # First grid point can be entered if finer grid is used inside the nuclei. # The default is set to 0.0005, which will produce a few points inside the nucleus. # Use 0.0001, for example, for more points (~25). # Examples (either will work): # Cs # Cs 137 # Cs 0.0001 1s 2s ... 5p # List of filled core shells. This code has to use fully filled subshells for core potential. # Valence electron lines can be omitted if the code is used for bspl. # To include valence electrons, input nmin nmax l j, or nlj: # 6 8 s1/2 # for 6, 7, 8 s1/2 # 6 s1/2 # for just 6s1/2 ========== # required line for formatting 7 70 9 220.0 # highest partial wave, number of splines, order of splines, and cavity radius in a.u. thorA - dynamic polarizability code thorA is used to make a batch script along with all input files for a range of wavelengths to aid in the submission of dynamic polarizability calculations. Click here to see a description of the input file . C # C for Caviness, D for Darwin 2 # inout on the second line for ine_dyn_E28 input 3P2. # state names for file names 800 980 2 # initial wavelength, final wavelength, and step in nm con_cut - truncating configuration lists con_cut is used to truncate a configuration list to only hold all configurations with weights above a user-specified cutoff. This program outputs the file CONF_CUT.RES , which can be used as a new CONF.INP file. The configurations in CONF_CUT.RES are also listed in order of descending weights, so the most important configurations are at the top of the list. You can copy the top configurations to your ADD.INP file and reconstruct a new configuration list with the top configurations as basic configurations, or use merge_ci to put the top configurations in another CONF.INP file. merge_ci - merging configuration lists merge_ci takes in two CONF.INP files named C_A.INP and CONF.INP and outputs the file C_M.INP . This output file can be renamed CONF.INP again to be used in conf . In summary: Run con_cut , inputting a cutoff threshold for the weight, to obtain CON_CUT.RES . Rename CON_CUT.RES to C_A.INP . Replace the basic configurations in ADD.INP with the top configurations from C_A.INP and run add . Run merge_ci , combining the configurations in C_A.INP and CONF.INP to obtain C_M.INP . Rename CONF.INP to C_B.INP , and C_M.INP to CONF.INP . Run conf . conf_pt - valence perturbation theory When running CI calculations with a large number of configurations relative to the number of work resources, it is often times necessary to determine the most important configurations in the CI space, and truncate the list of configurations to make successive calculations feasible. The conf_pt program begins the same way conf begins, reading in several input parameters and the list of configurations from the file CONF.INP , the basis set from the files HFD.DAT and CONF.DAT , and the radial integrals from CONF.INT and CONF.GNT . In addition, it also reads the CI eigenvectors from the file CONF.XIJ . qed_pot_conf - The pCI package includes 5 variants ( kvar ) of QED potentials: QEDMOD Flambaum local potential + QEDMOD non-local correction Flambaum local potential QEDPOT Semi-empirical approach sort - converts parallel matrix element files to serial format sort.py is a python program that sorts the matrix elements of the operator J^2 in order of ascending index (as done in the serial version of the conf program). This program takes in the parallel file CONF.JJJ or CONF.HIJ and returns a serial version of the inputted file. However, there is one change made to the file. There is an additional integer at the start of the file that stores the total number of matrix elements.","title":"Supplementary programs"},{"location":"supplementary/#supplementary-programs","text":"In this section, we describe supplementary programs used to generate basis sets or optimized configuration lists. .","title":"Supplementary programs"},{"location":"supplementary/#bdhfa-dirac-hartree-fock-code","text":"bdhfA can be used in place of bdhf to make a B-spline basis set. This program reads in an input file xx.dat with a much simpler input than for bdhf , and will write bdhf.in , which can be renamed to bas_wj.in for subsequent use in all-order codes. Click here to see a description of a minimum input file for Cs . 1 1 Cs 1s 2s 2p 3s 3p 3d 4s 4p 4d 5s 5p ========== 7 70 9 220.0 Click here to see a description of a complete input file for Cs . 1 1 Cs 137 0.0001 1s 2s 2p 3s 3p 3d 4s 4p 4d 5s 5p 6 9 s1/2 6 9 p1/2 5d3/2 5d5/2 ========== 7 70 9 220.0 Click here to see a description of a complete input file for Yb . 1 1 Yb 176 0.0001 1s 2s 2p 3s 3p 3d 4s 4p 4d 5s 5p 4f 6s1/2 5d3/2 5d5/2 ========== 7 50 9 60.0 The input files have the following format: 1 # 0 for old input, 1 for new input 1 # convergence parameter: 1 - most cases, 5 - in case of convergence problems Cs # name of element - e.g. Cs, Yb, etc. nuclear charge Z is set using this name # Isotope can also be entered. If no isotope is entered, most abundant or # most stable isotope will be used. # First grid point can be entered if finer grid is used inside the nuclei. # The default is set to 0.0005, which will produce a few points inside the nucleus. # Use 0.0001, for example, for more points (~25). # Examples (either will work): # Cs # Cs 137 # Cs 0.0001 1s 2s ... 5p # List of filled core shells. This code has to use fully filled subshells for core potential. # Valence electron lines can be omitted if the code is used for bspl. # To include valence electrons, input nmin nmax l j, or nlj: # 6 8 s1/2 # for 6, 7, 8 s1/2 # 6 s1/2 # for just 6s1/2 ========== # required line for formatting 7 70 9 220.0 # highest partial wave, number of splines, order of splines, and cavity radius in a.u.","title":"bdhfA - Dirac-Hartree-Fock code"},{"location":"supplementary/#thora-dynamic-polarizability-code","text":"thorA is used to make a batch script along with all input files for a range of wavelengths to aid in the submission of dynamic polarizability calculations. Click here to see a description of the input file . C # C for Caviness, D for Darwin 2 # inout on the second line for ine_dyn_E28 input 3P2. # state names for file names 800 980 2 # initial wavelength, final wavelength, and step in nm","title":"thorA - dynamic polarizability code"},{"location":"supplementary/#con_cut-truncating-configuration-lists","text":"con_cut is used to truncate a configuration list to only hold all configurations with weights above a user-specified cutoff. This program outputs the file CONF_CUT.RES , which can be used as a new CONF.INP file. The configurations in CONF_CUT.RES are also listed in order of descending weights, so the most important configurations are at the top of the list. You can copy the top configurations to your ADD.INP file and reconstruct a new configuration list with the top configurations as basic configurations, or use merge_ci to put the top configurations in another CONF.INP file.","title":"con_cut - truncating configuration lists"},{"location":"supplementary/#merge_ci-merging-configuration-lists","text":"merge_ci takes in two CONF.INP files named C_A.INP and CONF.INP and outputs the file C_M.INP . This output file can be renamed CONF.INP again to be used in conf . In summary: Run con_cut , inputting a cutoff threshold for the weight, to obtain CON_CUT.RES . Rename CON_CUT.RES to C_A.INP . Replace the basic configurations in ADD.INP with the top configurations from C_A.INP and run add . Run merge_ci , combining the configurations in C_A.INP and CONF.INP to obtain C_M.INP . Rename CONF.INP to C_B.INP , and C_M.INP to CONF.INP . Run conf .","title":"merge_ci - merging configuration lists"},{"location":"supplementary/#conf_pt-valence-perturbation-theory","text":"When running CI calculations with a large number of configurations relative to the number of work resources, it is often times necessary to determine the most important configurations in the CI space, and truncate the list of configurations to make successive calculations feasible. The conf_pt program begins the same way conf begins, reading in several input parameters and the list of configurations from the file CONF.INP , the basis set from the files HFD.DAT and CONF.DAT , and the radial integrals from CONF.INT and CONF.GNT . In addition, it also reads the CI eigenvectors from the file CONF.XIJ .","title":"conf_pt - valence perturbation theory"},{"location":"supplementary/#qed_pot_conf-","text":"The pCI package includes 5 variants ( kvar ) of QED potentials: QEDMOD Flambaum local potential + QEDMOD non-local correction Flambaum local potential QEDPOT Semi-empirical approach","title":"qed_pot_conf -"},{"location":"supplementary/#sort-converts-parallel-matrix-element-files-to-serial-format","text":"sort.py is a python program that sorts the matrix elements of the operator J^2 in order of ascending index (as done in the serial version of the conf program). This program takes in the parallel file CONF.JJJ or CONF.HIJ and returns a serial version of the inputted file. However, there is one change made to the file. There is an additional integer at the start of the file that stores the total number of matrix elements.","title":"sort - converts parallel matrix element files to serial format"},{"location":"theory/","text":"Theory For any many-electron system, we can divide all electrons into core and valence electrons. In this way, we can separate the electron-electron correlation problem into one describing the valence-valence correlations under the frozen-core approximation, and another describing the core-core and core-valence correlations. In the initial approximation, we start from the solution of the restricted Dirac-Hartree-Fock (HFD) equations in the central field approximation to construct one-electron orbitals for the core and valence electrons. Virtual orbitals can be constructed from B-splines or by other means to account for correlations. The valence-valence correlation problem is solved using the CI method, while core-core and core-valence correlations are included using either MBPT or the all-order method. In either case, we form an effective Hamiltonian in the valence CI space, then diagonalize the effective Hamiltonian using the CI method to find energies and wave functions for the low-lying states. Configuration Interaction The CI method is a standard ab initio method for calculating atomic properties of many-electron systems. In the valence space, the CI wave function is constructed as a linear combination of all distinct states of a specified angular momentum J and parity \\Psi_J=\\sum_ic_i\\Phi_i, where the set \\left\\{\\Phi_i\\right\\} are Slater determinants generated by exciting electrons from some reference configurations obtained to higher orbitals. Varying the coefficients $c_i$ results in a generalized eigenvalue problem \\sum_j\\langle\\Phi_i|H|\\Phi_j\\rangle c_j = Ec_i, which can be written in matrix form and diagonalized to find the energies and wave functions of the low-lying states. The energy matrix of the CI method can be obtained as a projection of the exact Hamiltonian H onto the CI subspace H^\\text{CI} 1 H^\\text{CI}=E_\\text{core}+\\sum_{i>N_\\text{core}}h_i^\\text{CI}+\\sum_{j>i>N_\\text{core}}V_{ij}, where E_\\text{core} is the energy of the frozen core, N_\\text{core} is the number of core electrons, h_i^\\text{CI} accounts for the kinetic energy of the valence electrons and their interaction with the central field, and V_{ij} accounts for the valence-valence correlations. Having obtained from CI the many-electron states |J M\\rangle and |J' M'\\rangle with the total angular momenta J,J' and their projections M,M' , one can form density transition matrix in terms of the one-electron states |nljm\\rangle 2 \\hat{\\rho}=\\rho_{nljm,n^\\prime l^\\prime j^\\prime m^\\prime}|nljm\\rangle\\langle n^\\prime l^\\prime j^\\prime m^\\prime|, where \\rho_{nljm,n^\\prime l^\\prime j^\\prime m^\\prime}=\\langle J^\\prime M^\\prime|a_{n^\\prime l^\\prime j^\\prime m^\\prime}^\\dagger a_{nljm}|JM\\rangle. Here un-primed indices refer to the initial state and primed indices refer to the final state. The many-electron matrix element can then be written as \\langle J^\\prime M^\\prime|T_q^L|JM\\rangle=\\text{Tr}\\,\\rho_{nljm,n^\\prime l^\\prime j^\\prime m^\\prime}\\langle n^\\prime l^\\prime j^\\prime m^\\prime|T_q^L|nljm\\rangle, where the trace sums over all quantum numbers (nljm) and (n^\\prime l^\\prime j^\\prime m^\\prime) , and T_q^L is the spherical component of the tensor operator of rank L . Using the Wigner-Eckart theorem, one can reduce the many-electron matrix element to \\langle J^\\prime \\Vert T^L \\Vert J\\rangle = \\text{Tr}\\,\\rho_{nlj,n^\\prime l^\\prime j^\\prime}^L \\langle n^\\prime l^\\prime j^\\prime\\Vert T^L \\Vert nlj\\rangle, where \\rho_{nlj,n^\\prime l^\\prime j^\\prime}^L = (-1)^{J^\\prime -M^\\prime}\\left( \\begin{array}{ccc} J^\\prime & L & J \\\\ -M^\\prime & q & M \\end{array}\\right)^{-1} \\sum_{mm^\\prime} (-1)^{j^\\prime-m^\\prime}\\left( \\begin{array}{ccc} j^\\prime & L & j \\\\ -m^\\prime & q & m \\end{array}\\right) \\rho_{nljm,n^\\prime l^\\prime j^\\prime m^\\prime}. We have developed new parallel programs based on these methods: conf realizes the CI method, which forms the CI Hamiltonian and uses Davidson's algorithm of diagonalization 3 to find low-lying energies and wave functions; dtm calculates reduced matrix elements of one-electron operators by forming the reduced density transition matrices. Valence Perturbation Theory As the number of configurations contributing to the CI wave function grows exponentially with the number of valence electrons, efficient selection of the most important configurations from a set of configurations becomes the main challenge of accurate computations. To significantly reduce the number of configurations, we further developed a method suggested in Ref. 4 to predict important configurations based on a set of configurations with known weights. This method can be used to optimize the CI space by identifying the most important configurations from a list of CI configurations using perturbation theory (PT). All second-order corrections are taken into account and added to the energy calculated from CI to obtain the total energy, E^\\text{CI}=E_0+E_1 , while first-order corrections to the wave functions are stored for use in subsequent CI calculations. This process of using CI on a small subspace, calculating corrections via PT, and reordering the list of configurations in descending weights can be repeated several times to form the most optimal CI subspace. Once the energy differences between subsequent CI calculations are relatively small, it can be assumed that convergence has been met. We've developed a new parallel program conf_pt that realizes the CI+PT method. The parallel version enables computations of extremely large problems, with tests running up to 400 million determinants. References V. A. Dzuba, V. V. Flambaum, and M. G. Kozlov. Combination of the many-body perturbation theory with the configuration-interaction method. Phys. Rev. A , 54(5):3948\u20133959, November 1996. \u21a9 M. G. Kozlov, S. G. Porsev, M. S. Safronova, and I. I. Tupitsyn. CI-MBPT: A package of programs for relativistic atomic calculations based on a method combining configuration interaction and many-body perturbation theory. Computer Physics Communications , 195:199\u2013213, 2015. \u21a9 Ernest R. Davidson. The iterative calculation of a few of the lowest eigenvalues and corresponding eigenvectors of large real-symmetric matrices. Journal of Computational Physics , 17(1):87\u201394, January 1975. \u21a9 Yu. G. Rakhlina, M. G. Kozlov, and S. G. Porsev. The energy of electron affinity to a zirconium atom. Optics and Spectroscopy , 90(6):817\u2013821, June 2001. \u21a9","title":"Theory"},{"location":"theory/#theory","text":"For any many-electron system, we can divide all electrons into core and valence electrons. In this way, we can separate the electron-electron correlation problem into one describing the valence-valence correlations under the frozen-core approximation, and another describing the core-core and core-valence correlations. In the initial approximation, we start from the solution of the restricted Dirac-Hartree-Fock (HFD) equations in the central field approximation to construct one-electron orbitals for the core and valence electrons. Virtual orbitals can be constructed from B-splines or by other means to account for correlations. The valence-valence correlation problem is solved using the CI method, while core-core and core-valence correlations are included using either MBPT or the all-order method. In either case, we form an effective Hamiltonian in the valence CI space, then diagonalize the effective Hamiltonian using the CI method to find energies and wave functions for the low-lying states.","title":"Theory"},{"location":"theory/#configuration-interaction","text":"The CI method is a standard ab initio method for calculating atomic properties of many-electron systems. In the valence space, the CI wave function is constructed as a linear combination of all distinct states of a specified angular momentum J and parity \\Psi_J=\\sum_ic_i\\Phi_i, where the set \\left\\{\\Phi_i\\right\\} are Slater determinants generated by exciting electrons from some reference configurations obtained to higher orbitals. Varying the coefficients $c_i$ results in a generalized eigenvalue problem \\sum_j\\langle\\Phi_i|H|\\Phi_j\\rangle c_j = Ec_i, which can be written in matrix form and diagonalized to find the energies and wave functions of the low-lying states. The energy matrix of the CI method can be obtained as a projection of the exact Hamiltonian H onto the CI subspace H^\\text{CI} 1 H^\\text{CI}=E_\\text{core}+\\sum_{i>N_\\text{core}}h_i^\\text{CI}+\\sum_{j>i>N_\\text{core}}V_{ij}, where E_\\text{core} is the energy of the frozen core, N_\\text{core} is the number of core electrons, h_i^\\text{CI} accounts for the kinetic energy of the valence electrons and their interaction with the central field, and V_{ij} accounts for the valence-valence correlations. Having obtained from CI the many-electron states |J M\\rangle and |J' M'\\rangle with the total angular momenta J,J' and their projections M,M' , one can form density transition matrix in terms of the one-electron states |nljm\\rangle 2 \\hat{\\rho}=\\rho_{nljm,n^\\prime l^\\prime j^\\prime m^\\prime}|nljm\\rangle\\langle n^\\prime l^\\prime j^\\prime m^\\prime|, where \\rho_{nljm,n^\\prime l^\\prime j^\\prime m^\\prime}=\\langle J^\\prime M^\\prime|a_{n^\\prime l^\\prime j^\\prime m^\\prime}^\\dagger a_{nljm}|JM\\rangle. Here un-primed indices refer to the initial state and primed indices refer to the final state. The many-electron matrix element can then be written as \\langle J^\\prime M^\\prime|T_q^L|JM\\rangle=\\text{Tr}\\,\\rho_{nljm,n^\\prime l^\\prime j^\\prime m^\\prime}\\langle n^\\prime l^\\prime j^\\prime m^\\prime|T_q^L|nljm\\rangle, where the trace sums over all quantum numbers (nljm) and (n^\\prime l^\\prime j^\\prime m^\\prime) , and T_q^L is the spherical component of the tensor operator of rank L . Using the Wigner-Eckart theorem, one can reduce the many-electron matrix element to \\langle J^\\prime \\Vert T^L \\Vert J\\rangle = \\text{Tr}\\,\\rho_{nlj,n^\\prime l^\\prime j^\\prime}^L \\langle n^\\prime l^\\prime j^\\prime\\Vert T^L \\Vert nlj\\rangle, where \\rho_{nlj,n^\\prime l^\\prime j^\\prime}^L = (-1)^{J^\\prime -M^\\prime}\\left( \\begin{array}{ccc} J^\\prime & L & J \\\\ -M^\\prime & q & M \\end{array}\\right)^{-1} \\sum_{mm^\\prime} (-1)^{j^\\prime-m^\\prime}\\left( \\begin{array}{ccc} j^\\prime & L & j \\\\ -m^\\prime & q & m \\end{array}\\right) \\rho_{nljm,n^\\prime l^\\prime j^\\prime m^\\prime}. We have developed new parallel programs based on these methods: conf realizes the CI method, which forms the CI Hamiltonian and uses Davidson's algorithm of diagonalization 3 to find low-lying energies and wave functions; dtm calculates reduced matrix elements of one-electron operators by forming the reduced density transition matrices.","title":"Configuration Interaction"},{"location":"theory/#valence-perturbation-theory","text":"As the number of configurations contributing to the CI wave function grows exponentially with the number of valence electrons, efficient selection of the most important configurations from a set of configurations becomes the main challenge of accurate computations. To significantly reduce the number of configurations, we further developed a method suggested in Ref. 4 to predict important configurations based on a set of configurations with known weights. This method can be used to optimize the CI space by identifying the most important configurations from a list of CI configurations using perturbation theory (PT). All second-order corrections are taken into account and added to the energy calculated from CI to obtain the total energy, E^\\text{CI}=E_0+E_1 , while first-order corrections to the wave functions are stored for use in subsequent CI calculations. This process of using CI on a small subspace, calculating corrections via PT, and reordering the list of configurations in descending weights can be repeated several times to form the most optimal CI subspace. Once the energy differences between subsequent CI calculations are relatively small, it can be assumed that convergence has been met. We've developed a new parallel program conf_pt that realizes the CI+PT method. The parallel version enables computations of extremely large problems, with tests running up to 400 million determinants.","title":"Valence Perturbation Theory"},{"location":"theory/#references","text":"V. A. Dzuba, V. V. Flambaum, and M. G. Kozlov. Combination of the many-body perturbation theory with the configuration-interaction method. Phys. Rev. A , 54(5):3948\u20133959, November 1996. \u21a9 M. G. Kozlov, S. G. Porsev, M. S. Safronova, and I. I. Tupitsyn. CI-MBPT: A package of programs for relativistic atomic calculations based on a method combining configuration interaction and many-body perturbation theory. Computer Physics Communications , 195:199\u2013213, 2015. \u21a9 Ernest R. Davidson. The iterative calculation of a few of the lowest eigenvalues and corresponding eigenvectors of large real-symmetric matrices. Journal of Computational Physics , 17(1):87\u201394, January 1975. \u21a9 Yu. G. Rakhlina, M. G. Kozlov, and S. G. Porsev. The energy of electron affinity to a zirconium atom. Optics and Spectroscopy , 90(6):817\u2013821, June 2001. \u21a9","title":"References"},{"location":"ud_instructions/","text":"Working at UD The University of Delaware currently houses and maintains the Caviness community cluster and the NSF funded DARWIN cluster. Cluster Processor Caviness 2x18-core 2.10 GHz Intel E5-2695 v4 (\"Broadwell\") DARWIN 2x32-core AMD EPYC 7002 Series Processors More information about the UD clusters can be found here . Compiling at UD The computers at UD utilize the VALET system for installing software. We load the cmake and openmpi modules into your environment using the command vpkg_require . $ vpkg_require intel Adding package `intel/2018u4` to your environment To see what packages have been added to your environment, you can use the vpkg_history command. $ vpkg_history [standard] intel/2018u4 To remove the changes produced by vpkg_require , you can use the vpkg_rollback command. $ vpkg_rollback The parallel codes are built using the CMakeLists.txt file. A Debug build can be done: $ ls pCI $ cd pCI $ mkdir build-debug $ cd build-debug $ vpkg_require cmake openmpi/4.1.0:intel-2020 $ FC=mpifort cmake -DCMAKE_BUILD_TYPE=Debug -DCMAKE_INSTALL_PREFIX=$(pwd)/../20200402-debug .. : $ make $ make install An optimized build demands a little more: $ cd .. $ mkdir build-opt $ cd build-opt $ FC=mpifort cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=$(pwd)/../20200317-opt -DCMAKE_Fortran_FLAGS_RELEASE=\"-g -O3 -mcmodel=large -xHost -m64\" .. : $ make $ make install Note Each community cluster has their own versions of cmake and openmpi. To see what versions and variants of a package are available on the cluster, use the vpkg_versions command. Running Jobs at UD The UD clusters utilize the Slurm workload manager (job scheduling system) to manage and control the resources available to computational tasks. Users can either run their jobs interactively or submit in batch. For an interacive job, the user must type the commands they wish to execute in real time. For a batch job, those sequence of commands are saved to a file, known as a job script, which is submitted to Slurm. Using batch job scripts have several advantages such as reusability and increased job throughput. More information about running jobs can be found here . Job scripts It is strongly recommended to use a job script file patterned after the prototypes in /opt/templates/ . There are README.md files in each subdirectory to explain the use of the templates. The following job scripts are valid for usage only in the Caviness and DARWIN community clusters. Serial job script An serial job script template with full descriptions of capabilities is given here. Click here to see a description of serial.qs . #!/bin/bash -l # # Sections of this script that can/should be edited are delimited by a # [EDIT] tag. All Slurm job options are denoted by a line that starts # with \"#SBATCH \" followed by flags that would otherwise be passed on # the command line. Slurm job options can easily be disabled in a # script by inserting a space in the prefix, e.g. \"# SLURM \" and # reenabled by deleting that space. # # This is a batch job template for a program using a single processor # core/thread (a serial job). # #SBATCH --ntasks=1 # # [EDIT] All jobs have memory limits imposed. The default is 1 GB per # CPU allocated to the job. The default can be overridden either # with a per-node value (--mem) or a per-CPU value (--mem-per-cpu) # with unitless values in MB and the suffixes K|M|G|T denoting # kibi, mebi, gibi, and tebibyte units. Delete the space between # the \"#\" and the word SBATCH to enable one of them: # # SBATCH --mem=8G # SBATCH --mem-per-cpu=1024M # # [EDIT] Each node in the cluster has local scratch disk of some sort # that is always mounted as /tmp. Per-job and per-step temporary # directories are automatically created and destroyed by the # auto_tmpdir plugin in the /tmp filesystem. To ensure a minimum # amount of free space on /tmp when your job is scheduled, the # --tmp option can be used; it has the same behavior unit-wise as # --mem and --mem-per-cpu. Delete the space between the \"#\" and the # word SBATCH to enable: # # SBATCH --tmp=24G # # [EDIT] It can be helpful to provide a descriptive (terse) name for # the job (be sure to use quotes if there's whitespace in the # name): # #SBATCH --job-name=serial_job # # [EDIT] The partition determines which nodes can be used and with what # maximum runtime limits, etc. Partition limits can be displayed # with the \"sinfo --summarize\" command. # # SBATCH --partition=standard # # To run with priority-access to resources owned by your workgroup, # use the \"_workgroup_\" partition: # # SBATCH --partition=_workgroup_ # # [EDIT] The maximum runtime for the job; a single integer is interpreted # as a number of minutes, otherwise use the format # # d-hh:mm:ss # # Jobs default to the default runtime limit of the chosen partition # if this option is omitted. # #SBATCH --time=0-02:00:00 # # You can also provide a minimum acceptable runtime so the scheduler # may be able to run your job sooner. If you do not provide a # value, it will be set to match the maximum runtime limit (discussed # above). # # SBATCH --time-min=0-01:00:00 # # [EDIT] By default SLURM sends the job's stdout to the file \"slurm- .out\" # and the job's stderr to the file \"slurm- .err\" in the working # directory. Override by deleting the space between the \"#\" and the # word SBATCH on the following lines; see the man page for sbatch for # special tokens that can be used in the filenames: # # SBATCH --output=%x-%j.out # SBATCH --error=%x-%j.out # # [EDIT] Slurm can send emails to you when a job transitions through various # states: NONE, BEGIN, END, FAIL, REQUEUE, ALL, TIME_LIMIT, # TIME_LIMIT_50, TIME_LIMIT_80, TIME_LIMIT_90, ARRAY_TASKS. One or more # of these flags (separated by commas) are permissible for the # --mail-type flag. You MUST set your mail address using --mail-user # for messages to get off the cluster. # # SBATCH --mail-user='my_address@udel.edu' # SBATCH --mail-type=END,FAIL,TIME_LIMIT_90 # # [EDIT] By default we DO NOT want to send the job submission environment # to the compute node when the job runs. # #SBATCH --export=NONE # # # [EDIT] Define a Bash function and set this variable to its # name if you want to have the function called when the # job terminates (time limit reached or job preempted). # # PLEASE NOTE: when using a signal-handling Bash # function, any long-running commands should be prefixed # with UD_EXEC, e.g. # # UD_EXEC mpirun vasp # # If you do not use UD_EXEC, then the signals will not # get handled by the job shell! # #job_exit_handler() { # # Copy all our output files back to the original job directory: # cp * \"$SLURM_SUBMIT_DIR\" # # # Don't call again on EXIT signal, please: # trap - EXIT # exit 0 #} #export UD_JOB_EXIT_FN=job_exit_handler # # [EDIT] By default, the function defined above is registered # to respond to the SIGTERM signal that Slurm sends # when jobs reach their runtime limit or are # preempted. You can override with your own list of # signals using this variable -- as in this example, # which registers for both SIGTERM and the EXIT # pseudo-signal that Bash sends when the script ends. # In effect, no matter whether the job is terminated # or completes, the UD_JOB_EXIT_FN will be called. # #export UD_JOB_EXIT_FN_SIGNALS=\"SIGTERM EXIT\" # # If you have VALET packages to load into the job environment, # uncomment and edit the following line: # #vpkg_require intel/2019 # # Do general job environment setup: # . /opt/shared/slurm/templates/libexec/common.sh # # [EDIT] Add your script statements hereafter, or execute a script or program # using the srun command. # srun date Once the job script has been set up, you can submit the job using the sbatch command: sbatch serial.qs Parallel job script An openmpi job script template with full descriptions of capabilities is given here. Click here to see a description of openmpi.qs . #!/bin/bash -l # # Sections of this script that can/should be edited are delimited by a # [EDIT] tag. All Slurm job options are denoted by a line that starts # with \"#SBATCH \" followed by flags that would otherwise be passed on # the command line. Slurm job options can easily be disabled in a # script by inserting a space in the prefix, e.g. \"# SLURM \" and # reenabled by deleting that space. # # This is a batch job template for a program using multiple processor # cores/threads on one or more nodes. This particular variant should # be used with Open MPI or another MPI library that is tightly- # integrated with Slurm. # # [EDIT] There are several ways to communicate the number and layout # of worker processes. Under GridEngine, the only option was # to request a number of slots and GridEngine would spread the # slots across an arbitrary number of nodes (not necessarily # with a common number of worker per node, either). This method # is still permissible under Slurm by providing ONLY the # --ntasks option: # # #SBATCH --ntasks= # # To limit the number of nodes used to satisfy the distribution # of workers, the --nodes option can be used in addition # to --ntasks: # # #SBATCH --nodes= # #SBATCH --ntasks= # # in which case, workers will be allocated to # nodes in round-robin fashion. # # For a uniform distribution of workers the --tasks-per-node # option should be used with the --nodes option: # # #SBATCH --nodes= # #SBATCH --tasks-per-node= # # The --ntasks option can be omitted in this case and will be # implicitly equal to * . # # Given the above information, set the options you want to use # and add a space between the \"#\" and the word SBATCH for the ones # you don't want to use. # #SBATCH --nodes= #SBATCH --ntasks= #SBATCH --tasks-per-node= # # [EDIT] Normally, each MPI worker will not be multithreaded; if each # worker allows thread parallelism, then alter this value to # reflect how many threads each worker process will spawn. # #SBATCH --cpus-per-task=1 # # [EDIT] All jobs have memory limits imposed. The default is 1 GB per # CPU allocated to the job. The default can be overridden either # with a per-node value (--mem) or a per-CPU value (--mem-per-cpu) # with unitless values in MB and the suffixes K|M|G|T denoting # kibi, mebi, gibi, and tebibyte units. Delete the space between # the \"#\" and the word SBATCH to enable one of them: # # SBATCH --mem=8G # SBATCH --mem-per-cpu=1024M # # [EDIT] Each node in the cluster has local scratch disk of some sort # that is always mounted as /tmp. Per-job and per-step temporary # directories are automatically created and destroyed by the # auto_tmpdir plugin in the /tmp filesystem. To ensure a minimum # amount of free space on /tmp when your job is scheduled, the # --tmp option can be used; it has the same behavior unit-wise as # --mem and --mem-per-cpu. Delete the space between the \"#\" and the # word SBATCH to enable: # # SBATCH --tmp=24G # # [EDIT] It can be helpful to provide a descriptive (terse) name for # the job (be sure to use quotes if there's whitespace in the # name): # #SBATCH --job-name=openmpi_job # # [EDIT] The partition determines which nodes can be used and with what # maximum runtime limits, etc. Partition limits can be displayed # with the \"sinfo --summarize\" command. # # SBATCH --partition=standard # # To run with priority-access to resources owned by your workgroup, # use the \"_workgroup_\" partition: # # SBATCH --partition=_workgroup_ # # [EDIT] The maximum runtime for the job; a single integer is interpreted # as a number of minutes, otherwise use the format # # d-hh:mm:ss # # Jobs default to the default runtime limit of the chosen partition # if this option is omitted. # #SBATCH --time=0-02:00:00 # # You can also provide a minimum acceptable runtime so the scheduler # may be able to run your job sooner. If you do not provide a # value, it will be set to match the maximum runtime limit (discussed # above). # # SBATCH --time-min=0-01:00:00 # # [EDIT] By default SLURM sends the job's stdout to the file \"slurm- .out\" # and the job's stderr to the file \"slurm- .err\" in the working # directory. Override by deleting the space between the \"#\" and the # word SBATCH on the following lines; see the man page for sbatch for # special tokens that can be used in the filenames: # # SBATCH --output=%x-%j.out # SBATCH --error=%x-%j.out # # [EDIT] Slurm can send emails to you when a job transitions through various # states: NONE, BEGIN, END, FAIL, REQUEUE, ALL, TIME_LIMIT, # TIME_LIMIT_50, TIME_LIMIT_80, TIME_LIMIT_90, ARRAY_TASKS. One or more # of these flags (separated by commas) are permissible for the # --mail-type flag. You MUST set your mail address using --mail-user # for messages to get off the cluster. # # SBATCH --mail-user='my_address@udel.edu' # SBATCH --mail-type=END,FAIL,TIME_LIMIT_90 # # [EDIT] By default we DO NOT want to send the job submission environment # to the compute node when the job runs. # #SBATCH --export=NONE # # # [EDIT] Do any pre-processing, staging, environment setup with VALET # or explicit changes to PATH, LD_LIBRARY_PATH, etc. # vpkg_require openmpi/default # # [EDIT] If you're not interested in how the job environment gets setup, # uncomment the following. # #UD_QUIET_JOB_SETUP=YES # # [EDIT] Slurm has a specific MPI-launch mechanism in srun that can speed-up # the startup of jobs with large node/worker counts. Uncomment this # line if you want to use that in lieu of mpirun. # #UD_USE_SRUN_LAUNCHER=YES # # [EDIT] By default each MPI worker process will be bound to a core/thread # for better efficiency. Uncomment this to NOT affect such binding. # #UD_DISABLE_CPU_AFFINITY=YES # # [EDIT] MPI ranks are distributed ( : . ,..) # # CORE sequentially to all allocated cores on each allocated node in # the sequence they occur in SLURM_NODELIST (this is the default) # # -N2 -n4 => n000(0:0.0,1:0.1,2:0.2,3:0.3); n001(4:0.0,5:0.1,6:0.2,7:0.3) # # NODE round-robin across the nodes allocated to the job in the sequence # they occur in SLURM_NODELIST # # -N2 -n4 => n000(0:0.0,2:0.1,4:0.2,6:0.3); n001(1:0.0,3:0.1,5:0.2,7:0.3) # # SOCKET round-robin across the allocated sockets on each allocated node # in the sequence they occur in SLURM_NODELIST # # -N2 -n4 => n000(0:0.0,2:0.1,4:1.0,6:1.1); n001(1:0.0,3:0.1,5:1.0,7:1.1) # # PLEASE NOTE: socket mode requires use of the --exclusive flag # to ensure uniform allocation of cores across sockets! # #UD_MPI_RANK_DISTRIB_BY=CORE # # [EDIT] By default all MPI byte transfers are limited to NOT use any # TCP interfaces on the system. Setting this variable will force # the job to NOT use any Infiniband interfaces. # #UD_DISABLE_IB_INTERFACES=YES # # [EDIT] Should Open MPI display LOTS of debugging information as the job # executes? Uncomment to enable. # #UD_SHOW_MPI_DEBUGGING=YES # # [EDIT] Define a Bash function and set this variable to its # name if you want to have the function called when the # job terminates (time limit reached or job preempted). # # PLEASE NOTE: when using a signal-handling Bash # function, any long-running commands should be prefixed # with UD_EXEC, e.g. # # UD_EXEC mpirun vasp # # If you do not use UD_EXEC, then the signals will not # get handled by the job shell! # #job_exit_handler() { # # Copy all our output files back to the original job directory: # cp * \"$SLURM_SUBMIT_DIR\" # # # Don't call again on EXIT signal, please: # trap - EXIT # exit 0 #} #export UD_JOB_EXIT_FN=job_exit_handler # # [EDIT] By default, the function defined above is registered # to respond to the SIGTERM signal that Slurm sends # when jobs reach their runtime limit or are # preempted. You can override with your own list of # signals using this variable -- as in this example, # which registers for both SIGTERM and the EXIT # pseudo-signal that Bash sends when the script ends. # In effect, no matter whether the job is terminated # or completes, the UD_JOB_EXIT_FN will be called. # #export UD_JOB_EXIT_FN_SIGNALS=\"SIGTERM EXIT\" # # Do standard Open MPI environment setup (networks, etc.) # . /opt/shared/slurm/templates/libexec/openmpi.sh # # [EDIT] Execute your MPI program # ${UD_MPIRUN} ./my_mpi_program arg1 \"arg2 has spaces\" arg3 mpi_rc=$? # # [EDIT] Do any cleanup work here... # # # Be sure to return the mpirun's result code: # exit $mpi_rc Once the job script has been set up, you can submit the job using the sbatch command: sbatch openmpi.qs Managing Jobs at UD Once the job has been submitted, you can monitor the status of your job using the squeue command: squeue -u <username> squeue -p <partition_name> You can also continuously monitor your job by using the watch command: watch squeue -u <username> watch squeue -p <partition_name> (Caviness only) To see information about the current utilization of guaranteed resources for the workgroup, you can run the squota command: squota To cancel your job, you can run the scancel command: scancel <job-id> To see information about the partitions and nodes, you can run the sinfo command: sinfo sinfo -p <partition-name> To see information about your queued jobs, you can run the scontrol command: scontrol show job <job-id> To see information about a completed job, you can run the sacct command: sacct -j <job-id> More information about managing jobs can be found here for Caviness and DARWIN . Additional Information and Support For additional information and support for running jobs on the UD clusters, please visit the respective cluster documentation pages: Caviness: http://docs.hpc.udel.edu/abstract/caviness/caviness DARWIN: http://docs.hpc.udel.edu/abstract/darwin/darwin","title":"Working at UD"},{"location":"ud_instructions/#working-at-ud","text":"The University of Delaware currently houses and maintains the Caviness community cluster and the NSF funded DARWIN cluster. Cluster Processor Caviness 2x18-core 2.10 GHz Intel E5-2695 v4 (\"Broadwell\") DARWIN 2x32-core AMD EPYC 7002 Series Processors More information about the UD clusters can be found here .","title":"Working at UD"},{"location":"ud_instructions/#compiling-at-ud","text":"The computers at UD utilize the VALET system for installing software. We load the cmake and openmpi modules into your environment using the command vpkg_require . $ vpkg_require intel Adding package `intel/2018u4` to your environment To see what packages have been added to your environment, you can use the vpkg_history command. $ vpkg_history [standard] intel/2018u4 To remove the changes produced by vpkg_require , you can use the vpkg_rollback command. $ vpkg_rollback The parallel codes are built using the CMakeLists.txt file. A Debug build can be done: $ ls pCI $ cd pCI $ mkdir build-debug $ cd build-debug $ vpkg_require cmake openmpi/4.1.0:intel-2020 $ FC=mpifort cmake -DCMAKE_BUILD_TYPE=Debug -DCMAKE_INSTALL_PREFIX=$(pwd)/../20200402-debug .. : $ make $ make install An optimized build demands a little more: $ cd .. $ mkdir build-opt $ cd build-opt $ FC=mpifort cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=$(pwd)/../20200317-opt -DCMAKE_Fortran_FLAGS_RELEASE=\"-g -O3 -mcmodel=large -xHost -m64\" .. : $ make $ make install Note Each community cluster has their own versions of cmake and openmpi. To see what versions and variants of a package are available on the cluster, use the vpkg_versions command.","title":"Compiling at UD"},{"location":"ud_instructions/#running-jobs-at-ud","text":"The UD clusters utilize the Slurm workload manager (job scheduling system) to manage and control the resources available to computational tasks. Users can either run their jobs interactively or submit in batch. For an interacive job, the user must type the commands they wish to execute in real time. For a batch job, those sequence of commands are saved to a file, known as a job script, which is submitted to Slurm. Using batch job scripts have several advantages such as reusability and increased job throughput. More information about running jobs can be found here .","title":"Running Jobs at UD"},{"location":"ud_instructions/#job-scripts","text":"It is strongly recommended to use a job script file patterned after the prototypes in /opt/templates/ . There are README.md files in each subdirectory to explain the use of the templates. The following job scripts are valid for usage only in the Caviness and DARWIN community clusters.","title":"Job scripts"},{"location":"ud_instructions/#serial-job-script","text":"An serial job script template with full descriptions of capabilities is given here. Click here to see a description of serial.qs . #!/bin/bash -l # # Sections of this script that can/should be edited are delimited by a # [EDIT] tag. All Slurm job options are denoted by a line that starts # with \"#SBATCH \" followed by flags that would otherwise be passed on # the command line. Slurm job options can easily be disabled in a # script by inserting a space in the prefix, e.g. \"# SLURM \" and # reenabled by deleting that space. # # This is a batch job template for a program using a single processor # core/thread (a serial job). # #SBATCH --ntasks=1 # # [EDIT] All jobs have memory limits imposed. The default is 1 GB per # CPU allocated to the job. The default can be overridden either # with a per-node value (--mem) or a per-CPU value (--mem-per-cpu) # with unitless values in MB and the suffixes K|M|G|T denoting # kibi, mebi, gibi, and tebibyte units. Delete the space between # the \"#\" and the word SBATCH to enable one of them: # # SBATCH --mem=8G # SBATCH --mem-per-cpu=1024M # # [EDIT] Each node in the cluster has local scratch disk of some sort # that is always mounted as /tmp. Per-job and per-step temporary # directories are automatically created and destroyed by the # auto_tmpdir plugin in the /tmp filesystem. To ensure a minimum # amount of free space on /tmp when your job is scheduled, the # --tmp option can be used; it has the same behavior unit-wise as # --mem and --mem-per-cpu. Delete the space between the \"#\" and the # word SBATCH to enable: # # SBATCH --tmp=24G # # [EDIT] It can be helpful to provide a descriptive (terse) name for # the job (be sure to use quotes if there's whitespace in the # name): # #SBATCH --job-name=serial_job # # [EDIT] The partition determines which nodes can be used and with what # maximum runtime limits, etc. Partition limits can be displayed # with the \"sinfo --summarize\" command. # # SBATCH --partition=standard # # To run with priority-access to resources owned by your workgroup, # use the \"_workgroup_\" partition: # # SBATCH --partition=_workgroup_ # # [EDIT] The maximum runtime for the job; a single integer is interpreted # as a number of minutes, otherwise use the format # # d-hh:mm:ss # # Jobs default to the default runtime limit of the chosen partition # if this option is omitted. # #SBATCH --time=0-02:00:00 # # You can also provide a minimum acceptable runtime so the scheduler # may be able to run your job sooner. If you do not provide a # value, it will be set to match the maximum runtime limit (discussed # above). # # SBATCH --time-min=0-01:00:00 # # [EDIT] By default SLURM sends the job's stdout to the file \"slurm- .out\" # and the job's stderr to the file \"slurm- .err\" in the working # directory. Override by deleting the space between the \"#\" and the # word SBATCH on the following lines; see the man page for sbatch for # special tokens that can be used in the filenames: # # SBATCH --output=%x-%j.out # SBATCH --error=%x-%j.out # # [EDIT] Slurm can send emails to you when a job transitions through various # states: NONE, BEGIN, END, FAIL, REQUEUE, ALL, TIME_LIMIT, # TIME_LIMIT_50, TIME_LIMIT_80, TIME_LIMIT_90, ARRAY_TASKS. One or more # of these flags (separated by commas) are permissible for the # --mail-type flag. You MUST set your mail address using --mail-user # for messages to get off the cluster. # # SBATCH --mail-user='my_address@udel.edu' # SBATCH --mail-type=END,FAIL,TIME_LIMIT_90 # # [EDIT] By default we DO NOT want to send the job submission environment # to the compute node when the job runs. # #SBATCH --export=NONE # # # [EDIT] Define a Bash function and set this variable to its # name if you want to have the function called when the # job terminates (time limit reached or job preempted). # # PLEASE NOTE: when using a signal-handling Bash # function, any long-running commands should be prefixed # with UD_EXEC, e.g. # # UD_EXEC mpirun vasp # # If you do not use UD_EXEC, then the signals will not # get handled by the job shell! # #job_exit_handler() { # # Copy all our output files back to the original job directory: # cp * \"$SLURM_SUBMIT_DIR\" # # # Don't call again on EXIT signal, please: # trap - EXIT # exit 0 #} #export UD_JOB_EXIT_FN=job_exit_handler # # [EDIT] By default, the function defined above is registered # to respond to the SIGTERM signal that Slurm sends # when jobs reach their runtime limit or are # preempted. You can override with your own list of # signals using this variable -- as in this example, # which registers for both SIGTERM and the EXIT # pseudo-signal that Bash sends when the script ends. # In effect, no matter whether the job is terminated # or completes, the UD_JOB_EXIT_FN will be called. # #export UD_JOB_EXIT_FN_SIGNALS=\"SIGTERM EXIT\" # # If you have VALET packages to load into the job environment, # uncomment and edit the following line: # #vpkg_require intel/2019 # # Do general job environment setup: # . /opt/shared/slurm/templates/libexec/common.sh # # [EDIT] Add your script statements hereafter, or execute a script or program # using the srun command. # srun date Once the job script has been set up, you can submit the job using the sbatch command: sbatch serial.qs","title":"Serial job script"},{"location":"ud_instructions/#parallel-job-script","text":"An openmpi job script template with full descriptions of capabilities is given here. Click here to see a description of openmpi.qs . #!/bin/bash -l # # Sections of this script that can/should be edited are delimited by a # [EDIT] tag. All Slurm job options are denoted by a line that starts # with \"#SBATCH \" followed by flags that would otherwise be passed on # the command line. Slurm job options can easily be disabled in a # script by inserting a space in the prefix, e.g. \"# SLURM \" and # reenabled by deleting that space. # # This is a batch job template for a program using multiple processor # cores/threads on one or more nodes. This particular variant should # be used with Open MPI or another MPI library that is tightly- # integrated with Slurm. # # [EDIT] There are several ways to communicate the number and layout # of worker processes. Under GridEngine, the only option was # to request a number of slots and GridEngine would spread the # slots across an arbitrary number of nodes (not necessarily # with a common number of worker per node, either). This method # is still permissible under Slurm by providing ONLY the # --ntasks option: # # #SBATCH --ntasks= # # To limit the number of nodes used to satisfy the distribution # of workers, the --nodes option can be used in addition # to --ntasks: # # #SBATCH --nodes= # #SBATCH --ntasks= # # in which case, workers will be allocated to # nodes in round-robin fashion. # # For a uniform distribution of workers the --tasks-per-node # option should be used with the --nodes option: # # #SBATCH --nodes= # #SBATCH --tasks-per-node= # # The --ntasks option can be omitted in this case and will be # implicitly equal to * . # # Given the above information, set the options you want to use # and add a space between the \"#\" and the word SBATCH for the ones # you don't want to use. # #SBATCH --nodes= #SBATCH --ntasks= #SBATCH --tasks-per-node= # # [EDIT] Normally, each MPI worker will not be multithreaded; if each # worker allows thread parallelism, then alter this value to # reflect how many threads each worker process will spawn. # #SBATCH --cpus-per-task=1 # # [EDIT] All jobs have memory limits imposed. The default is 1 GB per # CPU allocated to the job. The default can be overridden either # with a per-node value (--mem) or a per-CPU value (--mem-per-cpu) # with unitless values in MB and the suffixes K|M|G|T denoting # kibi, mebi, gibi, and tebibyte units. Delete the space between # the \"#\" and the word SBATCH to enable one of them: # # SBATCH --mem=8G # SBATCH --mem-per-cpu=1024M # # [EDIT] Each node in the cluster has local scratch disk of some sort # that is always mounted as /tmp. Per-job and per-step temporary # directories are automatically created and destroyed by the # auto_tmpdir plugin in the /tmp filesystem. To ensure a minimum # amount of free space on /tmp when your job is scheduled, the # --tmp option can be used; it has the same behavior unit-wise as # --mem and --mem-per-cpu. Delete the space between the \"#\" and the # word SBATCH to enable: # # SBATCH --tmp=24G # # [EDIT] It can be helpful to provide a descriptive (terse) name for # the job (be sure to use quotes if there's whitespace in the # name): # #SBATCH --job-name=openmpi_job # # [EDIT] The partition determines which nodes can be used and with what # maximum runtime limits, etc. Partition limits can be displayed # with the \"sinfo --summarize\" command. # # SBATCH --partition=standard # # To run with priority-access to resources owned by your workgroup, # use the \"_workgroup_\" partition: # # SBATCH --partition=_workgroup_ # # [EDIT] The maximum runtime for the job; a single integer is interpreted # as a number of minutes, otherwise use the format # # d-hh:mm:ss # # Jobs default to the default runtime limit of the chosen partition # if this option is omitted. # #SBATCH --time=0-02:00:00 # # You can also provide a minimum acceptable runtime so the scheduler # may be able to run your job sooner. If you do not provide a # value, it will be set to match the maximum runtime limit (discussed # above). # # SBATCH --time-min=0-01:00:00 # # [EDIT] By default SLURM sends the job's stdout to the file \"slurm- .out\" # and the job's stderr to the file \"slurm- .err\" in the working # directory. Override by deleting the space between the \"#\" and the # word SBATCH on the following lines; see the man page for sbatch for # special tokens that can be used in the filenames: # # SBATCH --output=%x-%j.out # SBATCH --error=%x-%j.out # # [EDIT] Slurm can send emails to you when a job transitions through various # states: NONE, BEGIN, END, FAIL, REQUEUE, ALL, TIME_LIMIT, # TIME_LIMIT_50, TIME_LIMIT_80, TIME_LIMIT_90, ARRAY_TASKS. One or more # of these flags (separated by commas) are permissible for the # --mail-type flag. You MUST set your mail address using --mail-user # for messages to get off the cluster. # # SBATCH --mail-user='my_address@udel.edu' # SBATCH --mail-type=END,FAIL,TIME_LIMIT_90 # # [EDIT] By default we DO NOT want to send the job submission environment # to the compute node when the job runs. # #SBATCH --export=NONE # # # [EDIT] Do any pre-processing, staging, environment setup with VALET # or explicit changes to PATH, LD_LIBRARY_PATH, etc. # vpkg_require openmpi/default # # [EDIT] If you're not interested in how the job environment gets setup, # uncomment the following. # #UD_QUIET_JOB_SETUP=YES # # [EDIT] Slurm has a specific MPI-launch mechanism in srun that can speed-up # the startup of jobs with large node/worker counts. Uncomment this # line if you want to use that in lieu of mpirun. # #UD_USE_SRUN_LAUNCHER=YES # # [EDIT] By default each MPI worker process will be bound to a core/thread # for better efficiency. Uncomment this to NOT affect such binding. # #UD_DISABLE_CPU_AFFINITY=YES # # [EDIT] MPI ranks are distributed ( : . ,..) # # CORE sequentially to all allocated cores on each allocated node in # the sequence they occur in SLURM_NODELIST (this is the default) # # -N2 -n4 => n000(0:0.0,1:0.1,2:0.2,3:0.3); n001(4:0.0,5:0.1,6:0.2,7:0.3) # # NODE round-robin across the nodes allocated to the job in the sequence # they occur in SLURM_NODELIST # # -N2 -n4 => n000(0:0.0,2:0.1,4:0.2,6:0.3); n001(1:0.0,3:0.1,5:0.2,7:0.3) # # SOCKET round-robin across the allocated sockets on each allocated node # in the sequence they occur in SLURM_NODELIST # # -N2 -n4 => n000(0:0.0,2:0.1,4:1.0,6:1.1); n001(1:0.0,3:0.1,5:1.0,7:1.1) # # PLEASE NOTE: socket mode requires use of the --exclusive flag # to ensure uniform allocation of cores across sockets! # #UD_MPI_RANK_DISTRIB_BY=CORE # # [EDIT] By default all MPI byte transfers are limited to NOT use any # TCP interfaces on the system. Setting this variable will force # the job to NOT use any Infiniband interfaces. # #UD_DISABLE_IB_INTERFACES=YES # # [EDIT] Should Open MPI display LOTS of debugging information as the job # executes? Uncomment to enable. # #UD_SHOW_MPI_DEBUGGING=YES # # [EDIT] Define a Bash function and set this variable to its # name if you want to have the function called when the # job terminates (time limit reached or job preempted). # # PLEASE NOTE: when using a signal-handling Bash # function, any long-running commands should be prefixed # with UD_EXEC, e.g. # # UD_EXEC mpirun vasp # # If you do not use UD_EXEC, then the signals will not # get handled by the job shell! # #job_exit_handler() { # # Copy all our output files back to the original job directory: # cp * \"$SLURM_SUBMIT_DIR\" # # # Don't call again on EXIT signal, please: # trap - EXIT # exit 0 #} #export UD_JOB_EXIT_FN=job_exit_handler # # [EDIT] By default, the function defined above is registered # to respond to the SIGTERM signal that Slurm sends # when jobs reach their runtime limit or are # preempted. You can override with your own list of # signals using this variable -- as in this example, # which registers for both SIGTERM and the EXIT # pseudo-signal that Bash sends when the script ends. # In effect, no matter whether the job is terminated # or completes, the UD_JOB_EXIT_FN will be called. # #export UD_JOB_EXIT_FN_SIGNALS=\"SIGTERM EXIT\" # # Do standard Open MPI environment setup (networks, etc.) # . /opt/shared/slurm/templates/libexec/openmpi.sh # # [EDIT] Execute your MPI program # ${UD_MPIRUN} ./my_mpi_program arg1 \"arg2 has spaces\" arg3 mpi_rc=$? # # [EDIT] Do any cleanup work here... # # # Be sure to return the mpirun's result code: # exit $mpi_rc Once the job script has been set up, you can submit the job using the sbatch command: sbatch openmpi.qs","title":"Parallel job script"},{"location":"ud_instructions/#managing-jobs-at-ud","text":"Once the job has been submitted, you can monitor the status of your job using the squeue command: squeue -u <username> squeue -p <partition_name> You can also continuously monitor your job by using the watch command: watch squeue -u <username> watch squeue -p <partition_name> (Caviness only) To see information about the current utilization of guaranteed resources for the workgroup, you can run the squota command: squota To cancel your job, you can run the scancel command: scancel <job-id> To see information about the partitions and nodes, you can run the sinfo command: sinfo sinfo -p <partition-name> To see information about your queued jobs, you can run the scontrol command: scontrol show job <job-id> To see information about a completed job, you can run the sacct command: sacct -j <job-id> More information about managing jobs can be found here for Caviness and DARWIN .","title":"Managing Jobs at UD"},{"location":"ud_instructions/#additional-information-and-support","text":"For additional information and support for running jobs on the UD clusters, please visit the respective cluster documentation pages: Caviness: http://docs.hpc.udel.edu/abstract/caviness/caviness DARWIN: http://docs.hpc.udel.edu/abstract/darwin/darwin","title":"Additional Information and Support"},{"location":"upscaling/","text":"Upscaling basis sets When running larger and larger CI calculations, you may run into problems due to the lack of available computational resources. In this section, we will discuss a method of generating effective configuration spaces by building on top of a subspace of important configurations, i.e. upscaling the basis set. Applications of this method includes expanding a basis set to include configurations with higher principle quantum numbers, higher partial waves, extra configurations and more. Theory We begin with with an initial configuration space H_0 from which we were able to obtain the corresponding low-lying eigenvalues E_0 and eigenvectors X_0 . Our goal is then to obtain the low-lying eigenvalues E and eigenvectors X of a larger configuration space H=H_0+H_1 , where H_1 is a configuration space generated from an expansion of the initial configuration space H_0 . Here, the CI calculation for H is not possible due to prohibitive computational resources, but we can very well approximate H by stripping H_0 of unimportant configurations, thus reducing the size of the overall configuration space. The goal then is to construct an effective configuration space \\begin{aligned} H^*=H_0^*+H_1 \\\\ \\approx H_0+H_1 \\end{aligned}, where H_0^* is a subspace of H_0 with important configurations. It is important to check that the energies do not shift significantly when stripping H_0 of unimportant configurations by running the CI calculation for the H_0^* configuration space, e.g. make sure \\Delta E_0=E_0^*-E_0 \\approx 0 . We can then run the CI procedure to obtain the energies E^* and eigenvectors X^* in the effective configuration space H^* . Assuming \\Delta E_0 \\approx 0 as mentioned above, we can approximate the energies of the full larger configuration space H as E\\approx E^*-\\Delta E_0 . Method We can now apply the upscaling method described above using the pCI code package. This method can be repeated to expand the basis set as many times as necessary. Run 1 We start from a base run corresponding to H_0 , where we were successful in obtaining the desired energies and eigenvectors via conf . From this run, we can use the con_cut program to obtain CON_CUT.RES , which is a CONF.INP file corresponding to a smaller configuration space corresponding to H_0^* , with the important configurations. Note that when running con_cut , you will be given the option to choose a cutoff value based on the weights of configurations. This subspace should have differences of energies within 5 cm ^{-1} from the full configuration space, e.g. E_0^*-E_0 \\leq 5 cm ^{-1} . Note this 5 cm ^{-1} is an arbitrary value to ensure an optimal configuration space, where nearly all important configurations are still included. Run 2 Next, we will need to generate the configuration subspace H_1 corresponding to the desired expansion. This is done by running concmp , which takes in two CONF.INP files named C_A.INP and C_B.INP . Here, C_A.INP should correspond to the configuration space of the initial full run H_0 and C_B.INP should correspond to the configuration space of the full desired expansion H . Inputting arguments 0 0 1 , concmp compares the two configuration lists and outputs a new configuration list C_B_new.INP , which includes all configurations that are in C_B.INP , but not in C_A.INP . We can then use the merge_ci program to merge CON_CUT.RES , corresponding to H_0^* , and C_B_new.INP , corresponding to H_1 , to obtain C_M.INP , which corresponds to H^*=H^*_0+H_1 . Note that that program merge_ci requires two specific input files, so one must rename CON_CUT.RES to C_A.INP and C_B_new.INP to CONF.INP . Note that this specific choice of renaming CON_CUT.RES to C_A.INP allows the initial matrix for diagonalization to be constructed in the H_0^* subspace. We can then rename the merged C_M.INP to CONF.INP and run conf to obtain the desired energies and eigenvectors. The final energies from the expanded basis set can then be calculated by subtracting the energies resulting from conf using CON_CUT.RES . Example Here is an example of upscaling a basis set from 17g to 18g: Run conf for full 17g Run con_cut on 17g to obtain 17g_cut configurations in CON_CUT.RES find a log_cutoff threshold value where the energies obtained from the resulting configuration list is near-identical to those of the full 17g run Run concmp to obtain (18g-17g) configurations, i.e. configurations only in 18g 17g in C_A.INP , 18g in C_B.INP run concmp with input 0 0 1 to obtain output C_B_new.INP Run merge_ci to combine 17g_cut with (18g-17g) configurations rename C_B_new.INP to CONF.INP rename CON_CUT.RES from 17g_cut to C_A.INP Run conf to obtain energies for 17g_cut + (18g-17g) Obtain energy difference of 18g-17g by subtracting out 17g_cut energies","title":"Upscaling basis sets"},{"location":"upscaling/#upscaling-basis-sets","text":"When running larger and larger CI calculations, you may run into problems due to the lack of available computational resources. In this section, we will discuss a method of generating effective configuration spaces by building on top of a subspace of important configurations, i.e. upscaling the basis set. Applications of this method includes expanding a basis set to include configurations with higher principle quantum numbers, higher partial waves, extra configurations and more.","title":"Upscaling basis sets"},{"location":"upscaling/#theory","text":"We begin with with an initial configuration space H_0 from which we were able to obtain the corresponding low-lying eigenvalues E_0 and eigenvectors X_0 . Our goal is then to obtain the low-lying eigenvalues E and eigenvectors X of a larger configuration space H=H_0+H_1 , where H_1 is a configuration space generated from an expansion of the initial configuration space H_0 . Here, the CI calculation for H is not possible due to prohibitive computational resources, but we can very well approximate H by stripping H_0 of unimportant configurations, thus reducing the size of the overall configuration space. The goal then is to construct an effective configuration space \\begin{aligned} H^*=H_0^*+H_1 \\\\ \\approx H_0+H_1 \\end{aligned}, where H_0^* is a subspace of H_0 with important configurations. It is important to check that the energies do not shift significantly when stripping H_0 of unimportant configurations by running the CI calculation for the H_0^* configuration space, e.g. make sure \\Delta E_0=E_0^*-E_0 \\approx 0 . We can then run the CI procedure to obtain the energies E^* and eigenvectors X^* in the effective configuration space H^* . Assuming \\Delta E_0 \\approx 0 as mentioned above, we can approximate the energies of the full larger configuration space H as E\\approx E^*-\\Delta E_0 .","title":"Theory"},{"location":"upscaling/#method","text":"We can now apply the upscaling method described above using the pCI code package. This method can be repeated to expand the basis set as many times as necessary.","title":"Method"},{"location":"upscaling/#run-1","text":"We start from a base run corresponding to H_0 , where we were successful in obtaining the desired energies and eigenvectors via conf . From this run, we can use the con_cut program to obtain CON_CUT.RES , which is a CONF.INP file corresponding to a smaller configuration space corresponding to H_0^* , with the important configurations. Note that when running con_cut , you will be given the option to choose a cutoff value based on the weights of configurations. This subspace should have differences of energies within 5 cm ^{-1} from the full configuration space, e.g. E_0^*-E_0 \\leq 5 cm ^{-1} . Note this 5 cm ^{-1} is an arbitrary value to ensure an optimal configuration space, where nearly all important configurations are still included.","title":"Run 1"},{"location":"upscaling/#run-2","text":"Next, we will need to generate the configuration subspace H_1 corresponding to the desired expansion. This is done by running concmp , which takes in two CONF.INP files named C_A.INP and C_B.INP . Here, C_A.INP should correspond to the configuration space of the initial full run H_0 and C_B.INP should correspond to the configuration space of the full desired expansion H . Inputting arguments 0 0 1 , concmp compares the two configuration lists and outputs a new configuration list C_B_new.INP , which includes all configurations that are in C_B.INP , but not in C_A.INP . We can then use the merge_ci program to merge CON_CUT.RES , corresponding to H_0^* , and C_B_new.INP , corresponding to H_1 , to obtain C_M.INP , which corresponds to H^*=H^*_0+H_1 . Note that that program merge_ci requires two specific input files, so one must rename CON_CUT.RES to C_A.INP and C_B_new.INP to CONF.INP . Note that this specific choice of renaming CON_CUT.RES to C_A.INP allows the initial matrix for diagonalization to be constructed in the H_0^* subspace. We can then rename the merged C_M.INP to CONF.INP and run conf to obtain the desired energies and eigenvectors. The final energies from the expanded basis set can then be calculated by subtracting the energies resulting from conf using CON_CUT.RES .","title":"Run 2"},{"location":"upscaling/#example","text":"Here is an example of upscaling a basis set from 17g to 18g: Run conf for full 17g Run con_cut on 17g to obtain 17g_cut configurations in CON_CUT.RES find a log_cutoff threshold value where the energies obtained from the resulting configuration list is near-identical to those of the full 17g run Run concmp to obtain (18g-17g) configurations, i.e. configurations only in 18g 17g in C_A.INP , 18g in C_B.INP run concmp with input 0 0 1 to obtain output C_B_new.INP Run merge_ci to combine 17g_cut with (18g-17g) configurations rename C_B_new.INP to CONF.INP rename CON_CUT.RES from 17g_cut to C_A.INP Run conf to obtain energies for 17g_cut + (18g-17g) Obtain energy difference of 18g-17g by subtracting out 17g_cut energies","title":"Example"}]}