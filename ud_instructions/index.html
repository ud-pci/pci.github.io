<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Working at UD - pci</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
  <link href="../css/extra.css" rel="stylesheet" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Working at UD";
    var mkdocs_page_input_path = "ud_instructions.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> pci</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <p class="caption"><span class="caption-text">Basics</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">Introduction</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../theory/">Theory</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Using pCI</span></p>
                <ul class="current">
                    <li class="toctree-l1"><a class="reference internal" href="../installation/">Installation</a>
                    </li>
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Working at UD</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#compiling-at-ud">Compiling at UD</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#running-jobs-at-ud">Running Jobs at UD</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#job-scripts">Job scripts</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#serial-job-script">Serial job script</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#parallel-job-script">Parallel job script</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#managing-jobs-at-ud">Managing Jobs at UD</a>
    </li>
    </ul>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">pCI Technical Details</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../main/">pCI code package</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../all-order/">All-order/MBPT code package</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../auxiliary/">Auxiliary programs</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../changelog/">Changelog</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Examples</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../basis_x/">Building a basis set for CI+X</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../qed/">Running QED</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../basis_neutral/">Example 1 - basis set for neutral atoms</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../basis_complex/">Example 2 - another method of creating basis</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">About</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../about/">About the Team</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../contact/">Contact Us</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">pci</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>Using pCI &raquo;</li>
        
      
    
    <li>Working at UD</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>

          <div role="main">
            <div class="section">
              
                <h1 id="working-at-ud">Working at UD</h1>
<p>The University of Delaware currently houses and maintains three community clusters. </p>
<table>
<thead>
<tr>
<th align="center">Cluster</th>
<th align="center">Processor</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="http://docs.hpc.udel.edu/abstract/farber/farber" target="_blank">Farber</a></td>
<td align="center">10-core 2.5 GHz Intel E5-2670 v2 ("Ivy Bridge")</td>
</tr>
<tr>
<td align="center">&nbsp; &nbsp; &nbsp; <a href="http://docs.hpc.udel.edu/abstract/caviness/caviness" target="_blank">Caviness</a> &nbsp; &nbsp; &nbsp;</td>
<td align="center">&nbsp; &nbsp; &nbsp; 18-core 2.10 GHz Intel E5-2695 v4 ("Broadwell") &nbsp; &nbsp; &nbsp;</td>
</tr>
<tr>
<td align="center"><a href="http://docs.hpc.udel.edu/abstract/darwin/earlyaccess" target="_blank">DARWIN</a></td>
<td align="center">32-core AMD EPYC 7002 Series Processors</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p>More information about the community clusters can be found <em><a href="http://docs.hpc.udel.edu/" target="_blank">here</a></em>.  </p>
<h2 id="compiling-at-ud">Compiling at UD</h2>
<p>The computers at UD utilize the VALET system for installing software. We load the cmake and openmpi modules into your environment using the command <code>vpkg_require</code>. </p>
<pre><code class="language-text">$ vpkg_require intel
Adding package `intel/2018u4` to your environment
</code></pre>
<p>To see what packages have been added to your environment, you can use the <code>vpkg_history</code> command.</p>
<pre><code class="language-text">$ vpkg_history
[standard]
  intel/2018u4
</code></pre>
<p>To remove the changes produced by <code>vpkg_require</code>, you can use the <code>vpkg_rollback</code> command.</p>
<pre><code class="language-text">$ vpkg_rollback
</code></pre>
<p>The parallel codes are built using the <code>CMakeLists.txt</code> file. </p>
<p>A Debug build can be done:</p>
<pre><code class="language-text">$ ls
pCI
$ cd pCI
$ mkdir build-debug
$ cd build-debug
$ vpkg_require cmake openmpi/4.1.0:intel-2020
$ FC=mpifort cmake -DCMAKE_BUILD_TYPE=Debug -DCMAKE_INSTALL_PREFIX=$(pwd)/../20200402-debug ..
   :
$ make
$ make install
</code></pre>
<p>An optimized build demands a little more:</p>
<pre><code class="language-text">$ cd ..
$ mkdir build-opt
$ cd build-opt
$ FC=mpifort cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=$(pwd)/../20200317-opt -DCMAKE_Fortran_FLAGS_RELEASE=&quot;-g -O3 -mcmodel=large -xHost -m64&quot; ..
   :
$ make
$ make install
</code></pre>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Each community cluster has their own versions of cmake and openmpi. To see what versions and variants of a package are available on the cluster, use the <code>vpkg_versions</code> command.</p>
</div>
<h2 id="running-jobs-at-ud">Running Jobs at UD</h2>
<h3 id="job-scripts">Job scripts</h3>
<p>It is strongly recommended to use a job script file patterned after the prototypes in <code>/opt/templates/</code>. There are <code>README.md</code> files in each subdirectory to explain the use of the templates. The following job scripts are valid for usage only in the Caviness and DARWIN community clusters. </p>
<h4 id="serial-job-script">Serial job script</h4>
<p>An serial job script template with full descriptions of capabilities is given here.  </p>
<details><summary>Click here to see a description of <a href="../src/serial.qs" download>serial.qs</a>.</summary>
    <pre><code>#!/bin/bash -l
#
# Sections of this script that can/should be edited are delimited by a
# [EDIT] tag.  All Slurm job options are denoted by a line that starts
# with "#SBATCH " followed by flags that would otherwise be passed on
# the command line.  Slurm job options can easily be disabled in a
# script by inserting a space in the prefix, e.g. "# SLURM " and
# reenabled by deleting that space.
#
# This is a batch job template for a program using a single processor
# core/thread (a serial job).
#
#SBATCH --ntasks=1
#
# [EDIT] All jobs have memory limits imposed.  The default is 1 GB per
#        CPU allocated to the job.  The default can be overridden either
#        with a per-node value (--mem) or a per-CPU value (--mem-per-cpu)
#        with unitless values in MB and the suffixes K|M|G|T denoting
#        kibi, mebi, gibi, and tebibyte units.  Delete the space between
#        the "#" and the word SBATCH to enable one of them:
#
# SBATCH --mem=8G
# SBATCH --mem-per-cpu=1024M
#
# [EDIT] Each node in the cluster has local scratch disk of some sort
#        that is always mounted as /tmp.  Per-job and per-step temporary
#        directories are automatically created and destroyed by the
#        auto_tmpdir plugin in the /tmp filesystem.  To ensure a minimum
#        amount of free space on /tmp when your job is scheduled, the
#        --tmp option can be used; it has the same behavior unit-wise as
#        --mem and --mem-per-cpu.  Delete the space between the "#" and the
#        word SBATCH to enable:
#
# SBATCH --tmp=24G
#
# [EDIT] It can be helpful to provide a descriptive (terse) name for
#        the job (be sure to use quotes if there's whitespace in the
#        name):
#
#SBATCH --job-name=serial_job
#
# [EDIT] The partition determines which nodes can be used and with what
#        maximum runtime limits, etc.  Partition limits can be displayed
#        with the "sinfo --summarize" command.
#
# SBATCH --partition=standard
#
#        To run with priority-access to resources owned by your workgroup,
#        use the "_workgroup_" partition:
#
# SBATCH --partition=_workgroup_
#
# [EDIT] The maximum runtime for the job; a single integer is interpreted
#        as a number of minutes, otherwise use the format
#
#          d-hh:mm:ss
#
#        Jobs default to the default runtime limit of the chosen partition
#        if this option is omitted.
#
#SBATCH --time=0-02:00:00
#
#        You can also provide a minimum acceptable runtime so the scheduler
#        may be able to run your job sooner.  If you do not provide a
#        value, it will be set to match the maximum runtime limit (discussed
#        above).
#
# SBATCH --time-min=0-01:00:00
#
# [EDIT] By default SLURM sends the job's stdout to the file "slurm-<jobid>.out"
#        and the job's stderr to the file "slurm-<jobid>.err" in the working
#        directory.  Override by deleting the space between the "#" and the
#        word SBATCH on the following lines; see the man page for sbatch for
#        special tokens that can be used in the filenames:
#
# SBATCH --output=%x-%j.out
# SBATCH --error=%x-%j.out
#
# [EDIT] Slurm can send emails to you when a job transitions through various
#        states: NONE, BEGIN, END, FAIL, REQUEUE, ALL, TIME_LIMIT,
#        TIME_LIMIT_50, TIME_LIMIT_80, TIME_LIMIT_90, ARRAY_TASKS.  One or more
#        of these flags (separated by commas) are permissible for the
#        --mail-type flag.  You MUST set your mail address using --mail-user
#        for messages to get off the cluster.
#
# SBATCH --mail-user='my_address@udel.edu'
# SBATCH --mail-type=END,FAIL,TIME_LIMIT_90
#
# [EDIT] By default we DO NOT want to send the job submission environment
#        to the compute node when the job runs.
#
#SBATCH --export=NONE
#

#
# [EDIT] Define a Bash function and set this variable to its
#        name if you want to have the function called when the
#        job terminates (time limit reached or job preempted).
#
#        PLEASE NOTE:  when using a signal-handling Bash
#        function, any long-running commands should be prefixed
#        with UD_EXEC, e.g.
#
#                 UD_EXEC mpirun vasp
#
#        If you do not use UD_EXEC, then the signals will not
#        get handled by the job shell!
#
#job_exit_handler() {
#  # Copy all our output files back to the original job directory:
#  cp * "$SLURM_SUBMIT_DIR"
#
#  # Don't call again on EXIT signal, please:
#  trap - EXIT
#  exit 0
#}
#export UD_JOB_EXIT_FN=job_exit_handler

#
# [EDIT] By default, the function defined above is registered
#        to respond to the SIGTERM signal that Slurm sends
#        when jobs reach their runtime limit or are
#        preempted.  You can override with your own list of
#        signals using this variable -- as in this example,
#        which registers for both SIGTERM and the EXIT
#        pseudo-signal that Bash sends when the script ends.
#        In effect, no matter whether the job is terminated
#        or completes, the UD_JOB_EXIT_FN will be called.
#
#export UD_JOB_EXIT_FN_SIGNALS="SIGTERM EXIT"

#
# If you have VALET packages to load into the job environment,
# uncomment and edit the following line:
#
#vpkg_require intel/2019

#
# Do general job environment setup:
#
. /opt/shared/slurm/templates/libexec/common.sh

#
# [EDIT] Add your script statements hereafter, or execute a script or program
#        using the srun command.
#
srun date</code></pre>
</details>

<p>Once the job script has been set up, you can submit the job using the <code>sbatch</code> command:</p>
<pre><code class="language-text">sbatch serial.qs
</code></pre>
<h4 id="parallel-job-script">Parallel job script</h4>
<p>An openmpi job script template with full descriptions of capabilities is given here. </p>
<details><summary>Click here to see a description of <a href="../src/openmpi.qs" download>openmpi.qs</a>.</summary>
    <pre><code>#!/bin/bash -l
#
# Sections of this script that can/should be edited are delimited by a
# [EDIT] tag.  All Slurm job options are denoted by a line that starts
# with "#SBATCH " followed by flags that would otherwise be passed on
# the command line.  Slurm job options can easily be disabled in a
# script by inserting a space in the prefix, e.g. "# SLURM " and
# reenabled by deleting that space.
#
# This is a batch job template for a program using multiple processor
# cores/threads on one or more nodes.  This particular variant should
# be used with Open MPI or another MPI library that is tightly-
# integrated with Slurm.
#
# [EDIT] There are several ways to communicate the number and layout
#        of worker processes.  Under GridEngine, the only option was
#        to request a number of slots and GridEngine would spread the
#        slots across an arbitrary number of nodes (not necessarily
#        with a common number of worker per node, either).  This method
#        is still permissible under Slurm by providing ONLY the
#        --ntasks option:
#
#             #SBATCH --ntasks=<nproc>
#
#        To limit the number of nodes used to satisfy the distribution
#        of <nproc> workers, the --nodes option can be used in addition
#        to --ntasks:
#
#             #SBATCH --nodes=<nhosts>
#             #SBATCH --ntasks=<nproc>
#
#        in which case, <nproc> workers will be allocated to <nhosts>
#        nodes in round-robin fashion.
#
#        For a uniform distribution of workers the --tasks-per-node
#        option should be used with the --nodes option:
#
#             #SBATCH --nodes=<nhosts>
#             #SBATCH --tasks-per-node=<nproc-per-node>
#
#        The --ntasks option can be omitted in this case and will be
#        implicitly equal to <nhosts> * <nproc-per-node>.
#
#        Given the above information, set the options you want to use
#        and add a space between the "#" and the word SBATCH for the ones
#        you don't want to use.
#
#SBATCH --nodes=<nhosts>
#SBATCH --ntasks=<nproc>
#SBATCH --tasks-per-node=<nproc-per-node>
#
# [EDIT] Normally, each MPI worker will not be multithreaded; if each
#        worker allows thread parallelism, then alter this value to
#        reflect how many threads each worker process will spawn.
#
#SBATCH --cpus-per-task=1
#
# [EDIT] All jobs have memory limits imposed.  The default is 1 GB per
#        CPU allocated to the job.  The default can be overridden either
#        with a per-node value (--mem) or a per-CPU value (--mem-per-cpu)
#        with unitless values in MB and the suffixes K|M|G|T denoting
#        kibi, mebi, gibi, and tebibyte units.  Delete the space between
#        the "#" and the word SBATCH to enable one of them:
#
# SBATCH --mem=8G
# SBATCH --mem-per-cpu=1024M
#
# [EDIT] Each node in the cluster has local scratch disk of some sort
#        that is always mounted as /tmp.  Per-job and per-step temporary
#        directories are automatically created and destroyed by the
#        auto_tmpdir plugin in the /tmp filesystem.  To ensure a minimum
#        amount of free space on /tmp when your job is scheduled, the
#        --tmp option can be used; it has the same behavior unit-wise as
#        --mem and --mem-per-cpu.  Delete the space between the "#" and the
#        word SBATCH to enable:
#
# SBATCH --tmp=24G
#
# [EDIT] It can be helpful to provide a descriptive (terse) name for
#        the job (be sure to use quotes if there's whitespace in the
#        name):
#
#SBATCH --job-name=openmpi_job
#
# [EDIT] The partition determines which nodes can be used and with what
#        maximum runtime limits, etc.  Partition limits can be displayed
#        with the "sinfo --summarize" command.
#
# SBATCH --partition=standard
#
#        To run with priority-access to resources owned by your workgroup,
#        use the "_workgroup_" partition:
#
# SBATCH --partition=_workgroup_
#
# [EDIT] The maximum runtime for the job; a single integer is interpreted
#        as a number of minutes, otherwise use the format
#
#          d-hh:mm:ss
#
#        Jobs default to the default runtime limit of the chosen partition
#        if this option is omitted.
#
#SBATCH --time=0-02:00:00
#
#        You can also provide a minimum acceptable runtime so the scheduler
#        may be able to run your job sooner.  If you do not provide a
#        value, it will be set to match the maximum runtime limit (discussed
#        above).
#
# SBATCH --time-min=0-01:00:00
#
# [EDIT] By default SLURM sends the job's stdout to the file "slurm-<jobid>.out"
#        and the job's stderr to the file "slurm-<jobid>.err" in the working
#        directory.  Override by deleting the space between the "#" and the
#        word SBATCH on the following lines; see the man page for sbatch for
#        special tokens that can be used in the filenames:
#
# SBATCH --output=%x-%j.out
# SBATCH --error=%x-%j.out
#
# [EDIT] Slurm can send emails to you when a job transitions through various
#        states: NONE, BEGIN, END, FAIL, REQUEUE, ALL, TIME_LIMIT,
#        TIME_LIMIT_50, TIME_LIMIT_80, TIME_LIMIT_90, ARRAY_TASKS.  One or more
#        of these flags (separated by commas) are permissible for the
#        --mail-type flag.  You MUST set your mail address using --mail-user
#        for messages to get off the cluster.
#
# SBATCH --mail-user='my_address@udel.edu'
# SBATCH --mail-type=END,FAIL,TIME_LIMIT_90
#
# [EDIT] By default we DO NOT want to send the job submission environment
#        to the compute node when the job runs.
#
#SBATCH --export=NONE
#

#
# [EDIT] Do any pre-processing, staging, environment setup with VALET
#        or explicit changes to PATH, LD_LIBRARY_PATH, etc.
#
vpkg_require openmpi/default

#
# [EDIT] If you're not interested in how the job environment gets setup,
#        uncomment the following.
#
#UD_QUIET_JOB_SETUP=YES

#
# [EDIT] Slurm has a specific MPI-launch mechanism in srun that can speed-up
#        the startup of jobs with large node/worker counts.  Uncomment this
#        line if you want to use that in lieu of mpirun.
#
#UD_USE_SRUN_LAUNCHER=YES

#
# [EDIT] By default each MPI worker process will be bound to a core/thread
#        for better efficiency.  Uncomment this to NOT affect such binding.
#
#UD_DISABLE_CPU_AFFINITY=YES

#
# [EDIT] MPI ranks are distributed <nodename>(<rank>:<socket>.<core>,..)
#
#    CORE    sequentially to all allocated cores on each allocated node in
#            the sequence they occur in SLURM_NODELIST (this is the default)
#
#              -N2 -n4 => n000(0:0.0,1:0.1,2:0.2,3:0.3); n001(4:0.0,5:0.1,6:0.2,7:0.3)
#
#    NODE    round-robin across the nodes allocated to the job in the sequence
#            they occur in SLURM_NODELIST
#
#              -N2 -n4 => n000(0:0.0,2:0.1,4:0.2,6:0.3); n001(1:0.0,3:0.1,5:0.2,7:0.3)
#
#    SOCKET  round-robin across the allocated sockets on each allocated node
#            in the sequence they occur in SLURM_NODELIST
#
#              -N2 -n4 => n000(0:0.0,2:0.1,4:1.0,6:1.1); n001(1:0.0,3:0.1,5:1.0,7:1.1)
#
#            PLEASE NOTE:  socket mode requires use of the --exclusive flag
#            to ensure uniform allocation of cores across sockets!
#
#UD_MPI_RANK_DISTRIB_BY=CORE

#
# [EDIT] By default all MPI byte transfers are limited to NOT use any
#        TCP interfaces on the system.  Setting this variable will force
#        the job to NOT use any Infiniband interfaces.
#
#UD_DISABLE_IB_INTERFACES=YES

#
# [EDIT] Should Open MPI display LOTS of debugging information as the job
#        executes?  Uncomment to enable.
#
#UD_SHOW_MPI_DEBUGGING=YES

#
# [EDIT] Define a Bash function and set this variable to its
#        name if you want to have the function called when the
#        job terminates (time limit reached or job preempted).
#
#        PLEASE NOTE:  when using a signal-handling Bash
#        function, any long-running commands should be prefixed
#        with UD_EXEC, e.g.
#
#                 UD_EXEC mpirun vasp
#
#        If you do not use UD_EXEC, then the signals will not
#        get handled by the job shell!
#
#job_exit_handler() {
#  # Copy all our output files back to the original job directory:
#  cp * "$SLURM_SUBMIT_DIR"
#
#  # Don't call again on EXIT signal, please:
#  trap - EXIT
#  exit 0
#}
#export UD_JOB_EXIT_FN=job_exit_handler

#
# [EDIT] By default, the function defined above is registered
#        to respond to the SIGTERM signal that Slurm sends
#        when jobs reach their runtime limit or are
#        preempted.  You can override with your own list of
#        signals using this variable -- as in this example,
#        which registers for both SIGTERM and the EXIT
#        pseudo-signal that Bash sends when the script ends.
#        In effect, no matter whether the job is terminated
#        or completes, the UD_JOB_EXIT_FN will be called.
#
#export UD_JOB_EXIT_FN_SIGNALS="SIGTERM EXIT"

#
# Do standard Open MPI environment setup (networks, etc.)
#
. /opt/shared/slurm/templates/libexec/openmpi.sh

#
# [EDIT] Execute your MPI program
#
${UD_MPIRUN} ./my_mpi_program arg1 "arg2 has spaces" arg3
mpi_rc=$?

#
# [EDIT] Do any cleanup work here...
#

#
# Be sure to return the mpirun's result code:
#
exit $mpi_rc</code></pre>
</details>

<p>Once the job script has been set up, you can submit the job using the <code>sbatch</code> command:</p>
<pre><code class="language-text">sbatch openmpi.qs
</code></pre>
<h2 id="managing-jobs-at-ud">Managing Jobs at UD</h2>
<p>Once the job has been submitted, you can monitor the status of your job using the <code>squeue</code> command:</p>
<pre><code class="language-text">squeue -u &lt;username&gt;
squeue -p &lt;partition_name&gt;
</code></pre>
<p>You can also continuously monitor your job by using the <code>watch</code> command:</p>
<pre><code class="language-text">watch squeue -u &lt;username&gt;
watch squeue -p &lt;partition_name&gt;
</code></pre>
<p>To see the amount of resources available for your job, you can run the <code>squota</code> command:</p>
<pre><code class="language-text">squota
</code></pre>
<p>To cancel your job, you can run the <code>scancel</code> command:</p>
<pre><code class="language-text">scancel &lt;job-id&gt;
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../main/" class="btn btn-neutral float-right" title="pCI code package">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../installation/" class="btn btn-neutral" title="Installation"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>&copy; Copyright 2021, pCI Development Team.</p>
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../installation/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../main/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
